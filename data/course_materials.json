[
  [
    [
      {
        "lesson_name": "Задачи и модели машинного обучения",
        "type": "text",
        "step_id": 635922,
        "lesson_id": 83186,
        "content": "Дорогие слушатели курса! \n\n Преподаватель курса Артём Шевляков будет отвечать на ваши вопросы   с      7 марта по 31 мая 2019 г .  \n\n Курс запущен в тестовом режиме, и поэтому возможны опечатки в заданиях. Просим слушателей писать о неточностях в комментариях. \n\n   Будьте внимательны: в численных ответах на задачи целая часть числа отделяется от дробной с помощью запятой (а не точки)!     \n\n Надеемся, что этот курс будет полезным для вас!",
        "rating": 1,
        "url": "https://stepik.org/lesson/83186/step/1?unit=59822"
      },
      {
        "lesson_name": "Задачи и модели машинного обучения",
        "type": "text",
        "step_id": 626857,
        "lesson_id": 83186,
        "content": "В этом разделе мы ответим на вопросы: \n\n Что такое машинное обучение (МО)? \n\n Зачем его нужно изучать? \n\n Какие знания вам для этого потребуются? \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/zzRzm",
        "rating": 1,
        "url": "https://stepik.org/lesson/83186/step/2?unit=59822"
      },
      {
        "lesson_name": "Задачи и модели машинного обучения",
        "type": "video",
        "step_id": 595690,
        "lesson_id": 83186,
        "content": "уважаемые слушатели я вас приветствую на первой лекции нашего курса посвященного машинному обучению и анализу данных ну что ж давайте поговорим о моделях машинного обучения всех моделях которые эти самые данные анализируют я думаю на первой лекции неуместно задавать всякие сложные заумные формулы которые как то определяют моделей машинного обучения давайте познакомимся с моделями машинного обучения на конкретных примерах итак первая большая группа моделей машинного обучения это различные рекомендательные системы действительно когда вы заходите на какой то сайт где продаются какие то товары либо есть фильмы музыка и так далее и вы начинаете пользоваться этими предлагаемыми товарами то есть смотреть фильмы и слушать музыку то очень часто на этих сайтах можно обнаружить где то сбоку или снизу рекомендации то есть те новые товары которые вам рекомендованы в том же ключе эта работа рекомендательной системы рекомендательная система на основании истории ваших просмотров а также сравнивая ваше поведение с поведением других пользователей системы она умеет понимать какое именно контент вас интерес помимо рекомендательных систем моделей машинного обучения очень часто используются при автоматической работе с текстом например когда вы набираете текст на ком нибудь вашем устройстве на телефоне например очень часто можно заметить что алгорит он начинает рекомендовать те слова которые как он считает нужным должны появиться в этот момент например на этом слайде вы видите если набрать на каком то устройстве слова они рассеялись и машинное то алгоритмом впринципе определить более менее то слово которое должно появиться следующем конечно он может ошибиться например вместо машинного обучения порекомендовать вот машинное отделение или машинное масло но в данной конкретной ситуации алгоритмы мы можем считать проработал более менее качественно он определил тройку наиболее вероятных слов которые нужно будет ставить его то наши слова которые мы ожидаем оно только попал помимо работы с текстом и рекомендательных систем существуют всевозможные программы обработки изображений которые в свою очередь основаны на моделях машинного обучения как работают такие программы ну допустим у вас есть некое изображение например фотография или какая картина нарисованная вами и вы хотите е обработать том или ином стиле например если у вас картина он вы хотите чтобы эта картина преобразовалось полотно стиле допустим сальвадора дали нет ничего сложного есть моделей машинного обучения которые позволяют осуществлять такую обработку изображений естественной есть и другие не менее важные задачи машинного обучения например очень важна проблема предсказания адекватной стоимости жилья действительно когда я своей жизни брал жилищный кредит там естественно пришлось вызвать оценщика который бы оценил объект недвижимости который я собираю приобретать как пишется отчет по оценке недвижимости с точки зрения математики все очень просто оценщика берет ваш объект а также смотрят по открытым базам данных те объекты которые расположены вблизи вашего и немножко отличаются точнее не сильно отличаются по характеристикам после этого когда оценки сформировал вот набор других объектов недвижимости похоже на вашей он грубо говоря начинает усреднять цену а ну естественно водяники поправочные леса там связан с с возрастом дома или с материалом сцені естественно такую операцию можно и нужно получать модели машинного обучения и такие модели машинного обучения уже есть такая важная задача это кредитный скоринг то есть предсказания нужно тому или иному заёмщику давать кредит очень важная проблема для банков следующая важная задача это прогнозирование спроса на товары здесь речь идет не только об чистом прогнозирование спроса то есть вот сколько допустим буханок хлеба вас купят будущей неделе но здесь можно рассматривать более сложные задачи например у вас есть данные о всех покупках которые совершили покупателей супермаркете и вы хотите посмотреть а как часто покупатели если они или хлеб купит ещё и молоко это так называемая задача поиска ассоциативных правил когда из покупке одного товара следует покупка и второго товара когда вы вычислите все таки ассоциативные прав было то есть группы товаров которые покупаются вместе то вы сможете как владелец магазина например проводить ту или иную политику маркетинга в отношении этих товаров следующее направление машинного обучения это медицинская диагностика действительно многие медицинские анализы являются по сути дела такими маленькими моделями машинного обучения они действительно анализирует вот компоненты вашей крови допустим и делают вывод болеть и высотой или иной болезни или нет следующая важная проблема это ранжирования поисковой выдачи делов том что мы все пользуемся поисковиками яндекс или гугл или другими и вот после того как вы вставите поисков билли какой то сразу возникает вот это самое поисковая выдача и вот на самом деле это очень важная проблема каком порядке расположить в страницы в этой самой поисковой выдачи какие документы будут более релевантной чем другие но и следующая важная проблема в анализе данных это поиск аномалии то есть поиск тех данных которые он очень сильно не похожи на своих соседей это тоже очень важная проблема естественно машинное обучение и анализ данных легче всего изучать как говорится не снуля имеет тот или иной запас знаний конечно наш курс он достаточно элементарной но тем не менее знания следующих математических дисциплин очень очень сильно приветствуется например многие модели машинного обучения основаны на математической статистики другая группа моделей машинного обучения она основана на методах оптимизации но и и иногда моделях машинного обучения используются идея из геометрии и линейной алгебры естественно знание этих дисциплин оно необязательно для нашего курса он достаточно элементарно и все математические формулы которые у нас будут встречаться естественно я буду очень подробно объяснять какие самые важные задачи для машинного обучения можно сформулировать так сказать на формальном языке чем занимаются люди в этой области первая задача восстановления данных эта задача действительно кто когда у вас есть таблица с данными но дело в том что идеальных таблицы с данными не бывает всегда в таблице будут существовать ячейки которых значения пропущено или оно заведомо некорректно вторая большая и важная проблема анализа данных это поиск выбросов как задачи можно сформулировать на абстрактном языке у вас есть множество объектов и нужно внутри него внутри этого самого множество найти все аномалии то есть те объекты которые не похожи нам нужно большинство других объектов своих соседей очень близко от задачи поиска выбросов примыкает задача поиска новизны и ее формулировка очень сильно похоже на задачи поиска выбросов у вас есть множество объектов но теперь выберите какой то объект который этому множеству не принадлежит объекту а не из множества м испрашивается объекта он похож на объекты из нам или нет следующая важная задача анализа данных эта задача кластеризации у вас есть множество объектов и их нужно разбить на несколько групп то есть классиков следующая задача машинного обучения эта задача предсказания у вас есть множество объектов м есть некий признак или как он называется целевым и для всех объектов из множества м инициатива признака или кому известно а теперь представьте к вам приходит новый объект а не из множества м и у него значение признака или оно пропущено и вот нужно как то значение признака или предсказать в этом заключ эта задача предсказания",
        "rating": 1,
        "url": "https://stepik.org/lesson/83186/step/3?unit=59822"
      },
      {
        "lesson_name": "Задачи и модели машинного обучения",
        "type": "video",
        "step_id": 595689,
        "lesson_id": 83186,
        "content": "свою лекцию я начал достаточно оптимистично то есть я говорил чем занимается машины обучения как это важно какие актуальные задачи она решает но тем не менее у машинного обучения есть и проблемы но первая проблема она заключается в том что машинное обучение начала развиваться не так уж и давно и поэтому возникает очень большая проблема именно компетентности заказчиков и исполнителей конечно же очень часто этим двум группам то есть заказчиком и исполнителем очень сложно друг друга понять заказчики например склонны требовать от исполнителей от программистов то что модели машинного обучения сделать просто не в состоянии и вот естественно наш курсу можно рассматривать как вот некий способ примирить заказчиков и исполнителей теории машинного обучения вторая проблема кто будет нести ответственность за ошибки моделей машинного обучения допустим произошел вот такой вот неприятный случай вы сдавали медицинский анализ а результаты этого анализа который были получены с помощью моделей машинного обучения предсказали допустим что вы больны хотя на самом деле вы здоровы или что самое худшее анализ предсказал что вы здоровы хотя на самом деле у вас была болезнь что в этом случае делать кто так сказать ответить за допущенную ошибку естественно привлекать к ответственности врача который расписался на результатах анализа бессмысленно потому что сами результаты анализа выдала модели машинного обучения привлекать к ответственности программиста который эту модель это как то писала вычислял вообще говоря это естественно но может и не логично потому что модели машинного обучения она не сама по себе такая плохая оказалось дело в том что модели машинного обучения они как бы строится исходя из истории то есть из истории тех случаев которые были ты подданный так сказать им во время обучения в данном случае модель машины модели машинного обучения связанная с медицинским анализам она скорее всего строилась на истории болезни других людей про которых было известно больные они или нет и возникает вопрос а кто действительно виноват в том что модель неправильно обучилась почему вот эта выборка истории болезни на которых она обучалась получилось вот такой вот может неадекватный действительно это очень сложный вопрос и он скорее всего не имеет я думаю пока точного решу ранее юридическом смысле поэтому нужно иметь конечно ввиду кто действительно будет нести ответственность за ошибки моделей машинного обучения следующая проблема допустим моделей машинного обучения выдала правильный ответ как вы думаете совпадение это или нет действительно чему то научилась и теперь стала давать адекватные ответы или это просто совпадение здесь на ум приходит такая схема мошенничества когда то был такой способ рассылки электронных писем вам на электронную почту присылают письмо в котором говорится что мы предсказываем что на следующей неделе курс доллара повысится если мы так сказать угадали то вы можете доверить нам свои сбережения для дальнейшей так сказать их игре на бирже проходит неделя курс доллара действий она повышается и вам приходит второе письмо в котором написано да действительно мы же были правы курс доллара повысился а теперь мы предсказываем что следующей неделе курс доллара упадет неделя проходит и действительно курс доллара падает но это действительно вот те кто присылает эти письма обладают моделей машинного обучения которые эти самые колебания валюты предсказывает на самом деле такая схема была явно мошенническое делов том что эти люди которые рассылали письма они сделали следующее взяли две большие группы людей одной группе прислали письмо в котором говорили что курс доллара повысился другой группе говорили что курс доллара упадет естественно курс доллара на следующей неделе допустим действительно повышался тогда они отбрасывали людей которым дали неверный прогноз и дальше работали стой группы которые вот предсказание случайным образом совпало эта группа делилась на две части одной части говорили что на следующей неделе курс доллара снова повысится этим говорили что он наоборот упадет естественно происходило события крым но одно из двух да и после этого нет дальше работать только с одной группой людей поскольку электронную почту можно было разослать на большое количество адресов сформировать вот такие вот группы большие можно было достаточно легко и допустим конце концов после десяти писем оставалась группа людей у которых все предсказания были правильны эти люди естественно верили что существует такая модель которая предсказывает курс доллара и отдавали этим мошенникам все свои средства якобы для игры на бирже итак что нужно знать когда вы работаете с модели машинного обучения естественно нужно знать основные идеи методов машинного обучения что они могут делать то что они делают не состоянии второе что нужно знать нужно какие этапы работы поддаются автоматизации а какие по прежнему зависит от человеческого фактора но из третьих естественно нужно знать методы обмана заказчика и исполнителя потому что все таки машинное обучение на нем основа шоу бизнеса нужно действительно понимать что можно вы может человек выполнить из обещанного что он из обещанного выполните состоянии эти три тезиса я буду отдыхать на них заострять внимание постоянно во всех коллекциях своего курса итак какие выводы можно сделать из нашей лекции во первых мы познакомились некоторые модели машинного обучения мы поняли какие знания нужны для того чтобы разбираться в работе модели машинного обучения хотя многие вопросы об эффективности и точности модели машинного обучения до сих пор остаются открытыми",
        "rating": 1,
        "url": "https://stepik.org/lesson/83186/step/4?unit=59822"
      }
    ],
    [
      {
        "lesson_name": "Представление данных для машинного обучения. Признаки объектов",
        "type": "video",
        "step_id": 595692,
        "lesson_id": 83187,
        "content": "она этой лекции мы поговорим потом каком виде нужно представить данные чтобы подать их на вход моделей машинного обучения действительно каком виде представляются обычно данные по которым строится эти самые модели машинного женя наиболее классический способ это представление данных в виде таблиц итак что мы видим на этой таблице мы видим что естественно таблица состоит из строк и столбцов что такое строка в таблице это объект то есть та сущность за которыми вы наблюдали и информацию которой вы как то накопили итак строки это объекты а столбцы это признаки таким образом у каждого объекта есть значение нескольких признаков итак у нас будет допустим н объектов это означает что объем выборки у нас равен и м признаков у каждого объекта есть свои признаки а какие признаки бывают делов том что если вы заметили на прошлой таблице признаки вообще я могут очень сильно отличаться друг от друга по своей природе какие типы признаков бывают признак может быть количественным или числовым это тот признак который действительно выражается по своему физическому смыслу виде некоторого вич общественного действительного числа признак может быть порядком порядковый признак он не имеет числовое природы он всего лишь задает порядок на вашем множестве объектов и наконец признак может быть номинальным или категориальной этот признак тоже не имеет числовое природы и как правило он обладает очень маленьким числом различных значений какие типы признаков есть в нашей таблице например рост и вес это действительно числовые признаки потому что по своей так сказать природе они действительно выражаются вот таким вот числом пол это бинарный признаков бинарный признак это тот номинальный признаку которого два возможных значения так вот пол классическом случае конечно это признак бинарные её ственно пол не может иметь числовое природы то что мы в этой таблице обозначает мужчину с помощью единичек женщин с помощью нуля мы естественно хотим показать что допустим мужчина больше как число чем женщина или то что если сложить две женщин не будет снова женщина да а если сложить женщину и мужчину то будет мужчина то есть на один будет единиц естественно мы этого не хотим показать вданном случае число ноль один это всего лишь метки грубо говоря классов класса мужчины и класса женщина при знак пол не имеет числовое природа это не число это грубо говоря некие метки у человека есть другой признак например группа крови группа крови хотя она выражается вообще говоря исторические число да первая вторая третья четвертая группа крови группа крови тем не менее не имеет числовое природы потому что с группами крови вы не можете там их складывать умножать и так далее это всего лишь некий немецкий поэтому группа крови это номинальный признак категориальный то есть но уже не бинарный кстати а вот место на олимпиаде которая есть нашей таблицы это порядковый признак конечно как и случае с номинальными признаками место на олимпиаде мы выражаем числом но эти числа они не совсем как бы числа вместо олимпиаде никак нельзя складывать там нажать и так далее вот это не число это вот тоже некие метки или грубо говоря ранге это те ранги которым обладает наши объекты таблицах и на этих рынках есть вот порядке так мы знаем что первое место это лучше чем второе а теперь представьте что у вас есть объекты более сложной природы например картинки или допустим файлы или тексты как их представить в виде строк таблице но самый простейший способ как представить рисунок виде строки таблицы он переведен на этом и на следующем слайде допустим для простоты что у вас все рисунки имеют одинаковый размер чтобы не возиться с большими числами мы допустим что каждый из рисунков он состоит из девяти пикселов и все эти пикселы монохромный то есть либо белый либо черный черно белый и каждый из девяти этих признаков будет отвечать за цвет того или иного пиксела но посмотрите вот получается вот такая вот строчка первая строка в таблице соответствует рисунку который приведен ниже действительно мы видим что у этого рисунка пиксела с номерами один две три четыре шесть а не белые а остальные пиксела не черный а теперь посмотрите на признаки которые есть у первого рисунка действительно эти признаки бинарные и они равны единице только для тех пикселов которые черный а для белых пикселов они равны нулю естественно таким же образом можно закодировать и остальные картинки такого же размера и которые тоже очень монохромная главный недостаток такого представления том что когда вы приходите от рисунках строке вы теряете информацию о смежности пикселов то есть вы как вы забываете о том какие пиксела были расположены рядом друг с другом и теперь каждый пиксел вас кодируется вот каким то своим новым бинарным признаком вот информацию о том какие пикселы были близки друг другу она потерялась и вода красоты заключается проблема вот такого представления рисунков естест цена табличное представление объектов налагает и некоторые ограничения самое главное ограничение заключается в том что когда вы пытаетесь сформировать одну общую таблицу для всех объектов то все объекты должны иметь одно и тоже число признаков это требование не всегда легко выполнить делов том что сплошь и рядом встречаются такая ситуация у вас есть от один объект который имеет более допустим сложную структуру или более длительную историю чем объект а вам соответствии с обычным представлениям нужно и объект и объектов одну таблицу подается данные виде таблиц каждый объект из таблицы он описывается с помощью некоторого набора признаков и естественно признаки бывают разного типа",
        "rating": 1,
        "url": "https://stepik.org/lesson/83187/step/1?unit=59823"
      }
    ],
    [
      {
        "lesson_name": "Основные понятия математической статистики",
        "type": "video",
        "step_id": 595696,
        "lesson_id": 83188,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83188/step/1?unit=59824"
      },
      {
        "lesson_name": "Основные понятия математической статистики",
        "type": "video",
        "step_id": 595697,
        "lesson_id": 83188,
        "content": "но теперь надо вернуться к понятию симметричный выборки итак если медианные средние значения близки друг другу то выборку называют симметричный а теперь давайте это сформулировать с помощью строгого математического языка на практике на практике симметрично выборка считают выборов который наблюдается указанные на складе неравенства в этом неравенстве слева стоит модуль разности между средним и медианой и этот модуль должен быть меньше чем стройный отклонение деленное на корень квадратный из объемом выборки если такое неравенство выполняется то выборка называется симметричной а теперь следующее величина которая касается правда ни одного признака опары признаков делов том что нам нужна величина которой это показывает как бы степень зависимости признаков как значение одного признака влияют на значение другого признака и естественно эта величина должна иметь смысл даже для признаков с разными единицами измерения то есть допустим первый признак выражается килограмма второе метров но тем не менее мы должны изобрести ту форму которая будет позволять вычислить степень влияния килограммов на вот метры которые есть нашей таблицы статистики естественно такая величина давным давно изобретено и она называла акцентом корреляции давайте разберемся чем ее смысл у нас есть вот такая табличка тренировочная вне есть несколько признаков давайте будем смотреть какие признаки наиболее сильно влияют на признак п один давайте рассмотрим новый пароль признаки п один и п две как вы думаете легко ли зная значение один вычислить значение для п две на самом деле вообще они очевидно когда давайте тогда пока оставим в стороне возьмём признак п три можно ли легко не зная значение признака один вычислить значение признака п три оказывается легко для этого нужно просто взять и значение признака п один просто домножить на стол и мы видим что после такой операции будет наблюдаться равенство всех вот парах наших вещей теперь давайте возьмём признак пять четыре легко не зная признак по один предсказать значение по четыре оказывается тоже легко для этого признака один ему нужно прибавить десять и после такой операции у вас возникнет вот признак пять четыре но а теперь давайте признаком пять тут немножко похитрее на самом деле признак пять тоже очень легко предсказать зная значение признака панин дело в том что по пять это не что иное как четыре минус один в таблице мы заметили что существует очень сильная зависимость между признаком п один и признаками три четыре пять а как эту зависимость изобразить геометрически а для этого нужно пары признаков пара значение признаков представить виде точек на плоскости представьте себе что вы по горизонтальной оси откладывайте значения первого признака по вертикальной оси откладывается значение второго признака на пересечении вы ставите точку то есть наблюдаемое значение пару значения знаков которые есть ваши таблицы когда вы нанесете вот все наблюдаемые вашей таблице значения для вот этих вот пара признаков у вас возникли вот такая вот некая россыпь точек давайте через эти точки попытаемся провести прямую если эти точки очень легко ложатся напрямую частности если они лежат точности на одной прямой то это означает что зависимость между призы очень большая а если эти точки расположены хаотично и как примую не проводили будут оставаться точки которые лежат далеко от этой прямой то это означает что между признаками очевидной зависимости нед таким образом коэффициент корреляции если выражаться умными словами показывает степень того как ваши данные ложатся на прямую по какой порно кастинг авиации считается файла очень простая итак у вас есть первый признак п второй признаку их значения и культы во первых вы должны вычислить попарное произведения по культуре просуммировать их два числа вычесть произведение объемом выборки на средние значения признаков пайку и все это хозяйство поделить на объем выборки минус один и на отклонение признаков по признаку и вот то что получится в итоге это и будет коэффициент корреляции то есть должно получиться число из отрезка от минус единицы до единицы какие свойства есть коэффициент корреляции если коэффициент корреляции равен нулю или очень близко положено около нуля причем как то так и другую сторону да но мне очень близка к нулю то это означает что очевидно зависимости между признаками нет если оценка реакции больше нуля то как правило большим значением одного признака соответствующее значение второго признака это означает что зависимости между двумя признаками прямая ну например коэффициент корреляции будет больше нуля для следующих парк признаков первый признак это число жителей в городе второй признак это число дорожно транспортных происшествий в этом городе действительно вот чем больше город тем больше чем количество дорожно транспортных происшествий третье свойство процента корреляции если он наоборот меньше нуля то здесь возникает обратная зависимость между значениями признака большему значению одного признака соответствует меньше значение второго признака например если первый ребенок это количество выкуриваемых сигарет в день то второй признак это допустим сколько лет прожил вот этот курильщик так вот здесь существует естественно обратное чем больше сигареты курит тем меньше в среднем конечно же с точки зрения статистики ты проживешь а что ещё нужно знать про котенка реляции чем ближе модуль коэффициента корреляции кондиции а как мы знаем он может быть близок минус единицей единицы но в любом случае если модуль корреляции близок единицы то это означает что существует очень сильная зависимость между указанными признаками но и частности если модуль процента корреляции вообще равен единице то это означает что между признаками пайку существует вообще линейная зависимость какие выводы следуют из нашей лекции первая мы научились с помощью умных форму считать средние медиану отклонение и коэффициент корреляции естест а мы выяснили что медианные средние это не одно и то же и в третьих научились оценивать степень зависимости между признаками это делается с помощью такой хитрый характеристики как коэффициент корреляции",
        "rating": 1,
        "url": "https://stepik.org/lesson/83188/step/2?unit=59824"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 467073,
        "lesson_id": 83239,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83239/step/1?unit=59875"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 467076,
        "lesson_id": 83239,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83239/step/2?unit=59875"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 467078,
        "lesson_id": 83239,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83239/step/3?unit=59875"
      },
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 467080,
        "lesson_id": 83239,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83239/step/4?unit=59875"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 467081,
        "lesson_id": 83239,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83239/step/5?unit=59875"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "choice",
        "step_id": 467087,
        "lesson_id": 154087,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/154087/step/1?unit=128454"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Постановка проблемы и простейшие способы ее решения",
        "type": "text",
        "step_id": 307120,
        "lesson_id": 83189,
        "content": "Идеальные данные бывают лишь в теории. На практике не существует наборов без пропусков или некорректных значений. Отвечать на вопрос “Кто в этом виноват?” бесполезно, поэтому в этом разделе мы затронем лишь вопрос “Что с этим делать?  \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/vZKZy",
        "rating": 1,
        "url": "https://stepik.org/lesson/83189/step/1?unit=59825"
      },
      {
        "lesson_name": "Постановка проблемы и простейшие способы ее решения",
        "type": "video",
        "step_id": 626894,
        "lesson_id": 83189,
        "content": "дело в том что очень редко встречается таблица с данными которые все ячейки заполнены корректными значениями гораздо чаще ваши вашей жизни встретиться таблицы которые части ячеек вообще пустая от а другая часть может быть запор но заведомо не корректными значениями так вот естественно возникает вопрос а что делать в этом случае как бороться с пустыми ячейками или священниками которые заполнены всякой белибердой заведомо некорректными значениями но на этом слайде показана таблица пример таблицы которые части человек вообще не заполнена часть ячеек содержащих заведомо голиматью например веса может быть равен шестьсот шестьдесят шесть килограмм а также место на олимпиаде не может быть отрицательным значением скорее всего эти некорректное значение возникли из за описки человека который таблица заполнял то есть наверно хотела написать вес равен шестьдесят шесть килограмм а место на олимпиаде минус четыре четыре ноль но сейчас мы уже поздно говорите что двигало этим человеком какая у него идея голове была нам нужно как то с помощью формальные процедуры значения восстановить кроме того в таблице вы видите пароль человек который значение нету вообще им следовательно эти ячейки тоже нужно как то либо заполнить либо сделать с ними что то ещё ну какие существуют простые я вначале скажу о простых способов борьбы мы с пропусками данных или некорректными значениями первые два способа они такие хирургические то есть выберите объект то есть страховка блицев которой ячейка пуста или не корректно и просто на просто вырезаете вскоре второй способ а он аналогичен первому если вашей таблице есть столбец в котором очень много пропусков столбце в этом то этот столбец можно полностью удалить из таблицы ну как видите эти им на способы они такие радикальные и их нужно применять естественно с осторожностью потому что если вашей таблице будет много пустых ячеек разбросанных парк ну примерно равномерно по всей таблице то он удаляя объекты строки или столбцы признаки вы можете получить совершенно очень маленькую таблицу из которой уже никакой интересной информации вы не замечаете поэтому конечно мне нужно стараться избегать удаление строк или столбцов таблицы и применять более изощренные методы восстановления данных например значение ячейки можно заменить на средние значения а также на медиану или моду из значений сталіца итак вы видите перед собой пустую или ячейку с некорректным значением пробегаете просто отцу которому принадлежит эта ячейка и заменяете знаете значение вот вашей ячейки на средние или я но и на моду станции на складе приведен пример замены пустой ячейки итак мы видим столбец значение некоторого признака который содержит шесть нормальных значений седьмое оно неизвестно пропущено так вот это значение можно восстановить либо с помощью среднего оно данном случае равно две целых шестьдесят семь сотых либо медианы медианы здесь равна трем либо мода она равна четырем замена пустой ячейки на средние или на медиану оно актуа именно для числовых признаков а как быть с номинальными или категориальные признаками вот рассмотрим следующую проблему у нас есть признак пол человека который содержится пропущенные значение возникает вопрос по какому принципу мы будем заменять попусту ячейку и так будем считать что вы немножко подумали озвученные более очевидные решения которые могут быть можно поступить так ну во первых можно пропущенные значения заменить на моду мода она определена и для номинальных признаков и поэтому в данном случае можно заменить на наиболее часто встречающиеся значение то есть на ноги для нашего примера есть второй способ можно немножко канализировать процедуру а именно мы видим что наш инструкция признаки есть три нуля две единицы то есть доля многие три пятых одной единицы две пятых так давайте сгенерируем случайной величины которая примет значение ноль серия три объединиться с вероятностью две пятых и вот в результате генерации такой случайной величины мы получим собственно и ответы либо ноль либо единица и это значение запишем пустующая наконец третий способ самый радикальный наш признак можно волевым решением объявить числовым признаком со всеми вытекающими отсюда последствиями если это числовой признак то тогда мы например пропущенные через можем заменить на средние значения но отсюда возникает проблема что могут появиться не целые значения итак какие выводы можно сделать из нашей лекции во первых нужно осознать важность и трудности проблемы пропущенных значений действительно таблицах с данными очень часто бывают пропущены ячейки которые нужно восстанавливать наиболее простой способ борьбы с этим злом это естественно удаление строк и столбцов содержащих пропуске однако существуют и более сложные методы восстановления да часть из которых была рассказана на этой лекции",
        "rating": 1,
        "url": "https://stepik.org/lesson/83189/step/2?unit=59825"
      },
      {
        "lesson_name": "Постановка проблемы и простейшие способы ее решения",
        "type": "video",
        "step_id": 626895,
        "lesson_id": 83189,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83189/step/3?unit=59825"
      }
    ],
    [
      {
        "lesson_name": "Замечание об использовании метрики",
        "type": "video",
        "step_id": 626901,
        "lesson_id": 83190,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83190/step/1?unit=59826"
      }
    ],
    [
      {
        "lesson_name": "Использование коэффициента корреляции для восстановления данных",
        "type": "video",
        "step_id": 626903,
        "lesson_id": 83191,
        "content": "а сейчас мы поговорим оба использование коэффициента корреляции задачей о восстановлении данных можно использовать коэффициент корреляции для решения нашей задачи что мы его застали учили определение что такое коэффициент реакции давайте подумаем как действительно котенка реляции можно использовать для восстановления данных давайте вспомним смысл коэффициента корреляции коэффициент корреляции выражает как сильно значение одного признака влияют на значение друг такого признака на самом деле коэффициент корреляции выражает не что иное как меру близости признаков то время как метрика выражает меру близости объектов друг другу коэффициент корреляции смысл заключается в установлении степени близости признаков то есть столбцов нашей таблицей друг другу давайте рассмотрим как коэффициент корреляции используется для восстановления пропущенных данных итак у нас есть таблице она содержит параметры топ моделей которые снимались в одном известном журнале давайте сделаем значение одной ячейки неизвестным и попытаемся восстановить его с помощью коэффициента корреляции итак первый столбец нашей таблицы содержит пропущенные значения давайте посчитаем корреляцию этого столбца с другими столбцами нашей таблицы естественно подсветкой центр корреляции будет осуществляться без использования строки с поврежденной ячейкой применяя формулу для процента корреляции которая была на предыдущих лекциях мы получаем следующее значение коэффициента корреляции первого признака со ставшим ся мы видим что корреляция одних признаков очень маленькая других достаточно высокая вчастности мы видим числа которые очень близки по модулю единицы например корреляция с признаками с равна очень достаточно большому числу ноль целых девяносто одна сотая теперь как нам поступить дальше дальше нужно найти умную формулу которая позвол будет значение коэффициентов корреляции использовать для восстановления данных причем чем выше по модулю коэффициент корреляции тем больший вклад будет иметь соответствующий признак на восстанавливаемое значение такая формула известно анапа или на складе она достаточно сложная конечно же нов чем смысл смысл действительно этой формулы том что чем выше корреляция признаков с неизвестной ячейкой сказать им то признакам плиты тем сильнее будет склад признака пытая итоговое значение которое мы восстанавливаем на этом складе пример решения задачи восстановления с помощью коэффициента корреляции итак мы посчитали коэффициенты корреляции первого признака со стальными я сбоку на нашем складе приведены средние значения наших признаков они нам тоже понадобится поскольку среднее значение используется в формуле которую мы будем применять а итоговое значение для пустой ячейки восстанавливается с помощью нашей формулы преподши кнопки всех известных значений это делается способом указанным на складе и в итоге мы получаем что установленное значение численно равно девяносто четыре целых двадцать две сотых естественно оценка реакции можно применять не только для вычисления меры близости кольцов но и кстати для строк то есть данную задачу можно было решить аналогичным способом но вычисляя коэффициент корреляции для строк вычислив корреляцию каждой строки из последний и по то же самое формуле можно было бы восстановить пропущенные значения естественно возникает вопрос а когда вы применяете своей задачей коэффициент корреляции нужны данные нормировать как это было в случае с метрикой оказывается нормировать необязательно делов том что тренировка на защиту саму формулу коэффициента корреляции обратите внимание на эту форму мы видим что счастливы считается среднее значение а знаменатель стоят отклонения то есть это выражение очень сильно похож уже на второй способ нормализации данных и поэтому когда вы работаете только центром корреляции не используя метрику данные можно не нормировать какие выводы можно сделать из нашей лекции коэффициент корреляции действительно можно применять для задачи восстановления данных мы его не зря учили на одной из первых лекций можно вычислить коэффициент корреляции между столбцами с его помощью восстанавливать данные",
        "rating": 1,
        "url": "https://stepik.org/lesson/83191/step/1?unit=59827"
      }
    ],
    [
      {
        "lesson_name": "Применение метрик и КК в рекомендательных системах",
        "type": "video",
        "step_id": 626904,
        "lesson_id": 83192,
        "content": "сейчас мы поговорим о применении метрикой коэффициента корреляции рекомендательных системах действительно сейчас мы знаем несколько алгоритмов которые умеют восстанавливать данные с помощью таких умных понятия отметка и коэффициент корреляции было бы обидно если бы эти алгоритмы других задачах не применялись бы давайте посмотрим какие ещё задачи машинного обучения можно решить с помощью алгоритмов которые мы изучили на лекциях посвященных восстановлению данных оказывается алгоритм восстановления данных можно использовать при проектировании рекомендательных систем а что такое рекомендательная система он рекомендательная система это такая штука которая выдает рекомендации пользователю допустим вы заходите на сайт фильмами и рекомендательная система вам где нибудь сбоку выдает список рекомендаций то есть те фильмы которые она считает по её мнению вам нравится на основании чего рекомендательная система принимает решение о выдаче вам того или иного фильма во первых она следит за вашей истории это во первых во вторых и что самое неожиданное при выдаче рекомендации вам рекомендательная система на самом деле анализирует и других пользователей системы рекомендательная система с точки зрения математики формально рекомендательная система обязательно внутри себя содержит таблицу строки этой таблицы соответствует пользователям а столбцы товаром а на пересечении строки и столбца стоит оценка которую пользователь поставил соответствующим товаром на складе вы видите таблицу которой указана четыре пользователя и пять товаров пищевых и непищевых на пересечении стоят оценки но очевидно по пятибалльной шкале которую человек поставил соответствующему товарному поскольку жизни бывает так что все абсолютно все товары получили оценки от всех всех всех пользователей конечно такого не бывает естественно в этой таблице возникают пропуски например мы видим что саша не поставил пока оценку такому товару как семечки получить оценку для семечек для пользователя саша и есть главная задача рекомендательной системы то есть рекомендательная система должна заранее понять товар семечки саша понравится или нит математической точки зрения рекомендательная система должна оценить значение которое должно быть вот этой вот ячейке а как это можно сделать она предыдущих лекциях мы рассказывали как можно восстанавливать данные с помощью алгоритмов основанных на метрики или наконец эти корреляции применяя один из такого из таких алгоритмов можно восстановить значение пропущенные этой ячейки получить число четыре целых двенадцать сотых то есть это число четыре целых двенадцать сотых является как бы оценкой которую рекомендательная система дает соответствующему товару и пользователей саша то есть рекомендательная система считает что если бы саша поставила бы оценку товару семечки то эта оценка была бы равна четыре целых двенадцать теперь что происходит дальше когда саша следующий раз заходят на сайт магазина допустим то система видит что оценка для товара семь точки равна примерно четырем если эта оценка достаточно высокая то масштабах оценок всего давай саша не будет на предыдущих сайтах мы рассмотрели рекомендательную систему которая посещалась не анонимными пользователями то есть система могла следить за поведением каждого пользователя и накапливать историю поведение давайте посмотрим как решить задачу выработки рекомендаций для системы которые все пользователи аноним то есть они заходят на сайт что то делать что то покупает но они не авторизуется то есть мы не знаем какой конкретный человек здесь тот или иной заказ как в этом случае определить расстояние между пользователями или товарами можно исхитриться так давайте сразу эту ситуацию рассмотрим на примере интернет магазина а у нас пользователей они анонимные дак они конечно же но они составляют заказ из нескольких товаров так вот меры близости товаров может служить следующее величина как часто эти товары попадают в один и тот же заказ то есть рекомендательный система должна внутри себя хранить это было примерно такую табличку качестве строк в таблице указаны товары офф качестве столбцов номера всех всех заказов которые прошли этой системе на пересечении стоит единица если соответствующий товар так соответствующий заказ попал ноль противном случае так вот а что можно сделать с этой таблицей сроками в этой таблице вы поймете какие товары наиболее близки друг другу и что можно сделать точки зрения маркетинга на основании полученной информации а когда вы вычислить какие товары наиболее близки друг другу то когда нова сайт придет новый пользователь будет просматривать товар допустим а то вы найдете товар наиболее близких это валуа и где то сбоку его и порекомендуйте так будет работать рекомендательная система вслучае когда десять пользователя анонимный какие выводы можно сделать на основании нашей лекции первый год рекомендательных системах нужно вычислять предпочтения пользователей это их основная задача второй год очень часто предпочтения пользователей вычисляется с помощью известных нам алгоритмов восстановление данных",
        "rating": 1,
        "url": "https://stepik.org/lesson/83192/step/1?unit=59828"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 307187,
        "lesson_id": 83241,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83241/step/1?unit=59876"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 307188,
        "lesson_id": 83241,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83241/step/2?unit=59876"
      },
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 307191,
        "lesson_id": 83241,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83241/step/3?unit=59876"
      },
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 307192,
        "lesson_id": 83241,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83241/step/4?unit=59876"
      },
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 307194,
        "lesson_id": 83241,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83241/step/5?unit=59876"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 536932,
        "lesson_id": 190150,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190150/step/1?unit=164655"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Постановка проблемы",
        "type": "text",
        "step_id": 626859,
        "lesson_id": 83193,
        "content": "Машинное обучение (МО) ничего не знает о толерантности: если в выборке есть объект с аномальными значениями признаков, то качество обработки такой выборки с помощью алгоритмов МО оставляет желать лучшего. В этом разделе мы изучим, как поручить машине поиск аномалий в выборке.",
        "rating": 1,
        "url": "https://stepik.org/lesson/83193/step/1?unit=59829"
      },
      {
        "lesson_name": "Постановка проблемы",
        "type": "video",
        "step_id": 573474,
        "lesson_id": 83193,
        "content": "добрый день на этой лекции мы начинаем изучать такую важную и сложную имеет реальную проблему как проблема поиска выбросов и аномалий данных во первых нам нужно разобраться что такое аномалии с чем едят и что нужно с ними делать если вы своей выборки обнаружили какие то аномальные значения или выбросы и хочу так по другому называют позвольте вам дать формальную постановку задачи поиска выбросов поиск выбросов состоит следующим у вас есть набор данных м и вам нужно найти внем все объекты с аномальными значениями признаков теперь мы занимались на предыдущих лекциях делал чем выброс или аномалия данных как правило соответствует реально существующему объекту но объекту с очень странными значениями признаков вто же время когда мы занимались проблемой обновление данных то мы имели дело как правило с пустыми с некорректными ячейками которые возникли в результате чьей либо халатности опечатки или чего то ещё вслучае выбросов объект как правило существует только у него действий нормальные значения признаков этот факт обречет принципиально разную стратегию работы с выбросами и работа с пропущенными данными как вы помните пропущенные данные мы пытались всячески восстанавливать заменять их на допу современные значения восстанавливать с помощью метрики коэффициента корреляции и других вещей их варианта риски восстанавливать нем пропущенные значения было бы действительно неразумным поэтому свой образ они поступают очень жестко как только вы выброс нашли свои выборки то его нужно безжалостно удалять позволить привести несколько простых примеров данных где встречаются выбросы представьте себе что вы начинаете измерять температуру в комнате случайных точках пространства как правило у вас будут получаться значение температура от восемнадцать градусов до двадцати двух но если вы вдруг случайно зарядите на радиатор отопления то там температура может взлететь до семидесяти градусов температура измеренная точки лс поверхности радиатора и есть выброс то есть это аномальное значение для наших данных большинство данных имеет стандартную температуры от восемнадцать до двадцати двух но тут возникает одно значение нормальной температуры это есть выброс для нашей выборки естественно на такое значение нельзя считать типичным для нашей комнаты и как правило такие странные значение нужно удалять и просил написать их средний балл за все их сессии давайте сразу скажу что я проводила анкетирование на одном факультете где учиться действительно очень сложно там много математических дисциплин программа очень сложная даются очень серьезно мы знания студентам и учиться там действительно очень сложно так вот как вы думаете кто моей выборки студентам оказался выбросом можно и большинство студентов учится там на одни тройки но среди них оказался ровно один человек который все сессии сдавал на отлично я применил этой выборки студентов алгоритм поиска выбросов и алгоритм вкачестве выбраться нашел бедного отличника ну что ж теперь давайте разберёмся зачем вообще нужно возиться с выбросами зачем их нужно находить и уничтожать исключать из выбор во первых если вы свои данные планируете подавать на вход какому то семь нужно маргариту например алгоритм предсказания то эталонные или типичные объекты типичных представителей выборке это тоже очень важно для многих задач анализа данных или третьих многие статистические характеристики например среднее значение они очень сильно подвержены наличии выбросов например если мы посчитаем среднюю зарплату всех пассажиров электрички то мы получим а ну более менее скромное значение но если среди пассажиров электрички случайно оказывается долларовый миллионер как там случайно оказался то средне я значение средняя зарплата она очень сильно возрастет и перестанет быть адекватной то есть она средняя зарплата после добавления миллионера перестанет адекватно описывать экономическую ситуацию в стране поэтому когда вы хотите посчитать среднюю зарплату типичных пассажиров электрички естественно нужно вначале убедиться что среди них нету миллионеров нам действительно там ездит я не знаю заранее может быть уверена прошу обратить внимание на следующий слайд этот слайд несколько математических школ философский есть три умных мыслей связанных с вопросами которые нужно переводить из с этими мыслями во первых нужно смириться первая мысль делов том что не существует формального математического строгого определения что такое выброс и поэтому каждый человек каждый алгоритм понятие выбросов трактует по своему аномалий и вот где провести границу между нормы и аномалии как я уже сказал создать алгоритм который ищет выбросы или аномалии наших данных возникает такая проблема если мы спроектируем слишком беспощадный алгоритм который уничтожает абсолютно все выбросы нашей выборке что этот алгоритм всему своей беспощадности будет удалять и часть нормальных объектов на другой стороны если мы будем проектировать наоборот алгоритм который очень бережно относятся к нормальным объектом то силу своей гуманно если этот алгоритм будет оставлять часть выбросов таким образом существует вот такая вот дилемма между полным уничтожением всех выбросов и бережным отношением к нормальным объектом с этим нужно как то смириться делов том что построить идеальный детектор выбросов это все равно что предложить идеальный медицинские анализы без ложноположительных и ложноотрицательных результатов на предыдущих сайтах я рассказал что такое выбросы почему нужно с ними бороться царь почему их нужно искать и удалять теперь давайте рассмотрим основные способы обнаружения выбросов первый способ это самая простая ситуация когда вы знаете смысл признака который лицо ваших данных и кроме того вы знаете какой диапазон значений этого признака является нормой например если вы изучаете людей и у этих людей есть признак рост всего своего жизненного опыта вы понимаете какие нормальные значения должны быть у этого признака люди с ростом более двух сантиметров естественно такие конечно бывают но они не типичны и впринципе таких людей можно из выборки удалять потому что у них нормально высокий рост на этом слайде приведены две большие группы алгоритмов которые занимаются поиском выбросом первой группе указаны алгоритмы которые основаны на анализе одного признака чем заключается их работа вы подаете им таблицу с данными они берут в этой таблице только один признак то есть только один столбец из этой таблицы находит в нем а нормальные значения и строки соответствующих аномальным значением объявляются за выброса это первая группа алгоритмов делают алгоритм из предыдущей группы какие выводы можно сделать из нашей лекции во первых объектов аномальными значениями признаков называется выбросами наличие выбросов выборки ухудшает качество моделей машина обучение и поэтому выбраться из выборки нужно удалять третий вывод к сожалению сформулировать оптимальные критерии поиска выбросов невозможно с этим нужно смириться и довериться результатом алгоритмов как но эти выбросы ищут за вас",
        "rating": 1,
        "url": "https://stepik.org/lesson/83193/step/2?unit=59829"
      }
    ],
    [
      {
        "lesson_name": "Методы, анализирующие признаки по отдельности",
        "type": "video",
        "step_id": 573476,
        "lesson_id": 83194,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83194/step/1?unit=59830"
      }
    ],
    [
      {
        "lesson_name": "Критерий Шовене (Chauvenet)",
        "type": "video",
        "step_id": 573477,
        "lesson_id": 83196,
        "content": "а сейчас мы поговорим критерий на который занимается поиском выбросов критерия является более продвинутым способом поиска выбросов данных более продвинутым по сравнению с простейшими алгоритмами изложены имена предыдущей лекции чем смысл критерия шины питания семена заключается в проверке неравенство оно видите достаточно сложно устроенная которая указана в центре нашего сайта а что нужно сделать чтобы это неравенство проверить ну естественно нужно буквы которые в этом неравенстве указанные заменить на число которые вычисляются по вашим данным что вам нужно для этого вычислить во первых вам нужно вычислить среднее значение по чертой вычислить отклонение которое стоит знаменателе также естественно знать объем выборки н как происходит проверка значение признака на аномальность выберите значение признака пытая подставляете в формулу у вас все известно и если это неравенство выполняется то это значение признака плиты объявляется выбросом осталось разобраться с тем что за магическая функция эффект присутствует левой части неравенства вид функции в достаточно сложные и может вызвать его некоторых случаях ступай страха поэтому на этом складе выражение для функции рф закрытый квадрат конце зоя функция называется дополнением к функции ошибок если нашим слушателям исполнилось восемнадцать лет то они могут посмотреть на следующий слайд где функция еще будет переведена уже без квадратиков вот перед вами действительно функция ссс представленном виде такого очень страшного интеграла но нам для своих задачах принципе вид функций сцене важен главное понять как это функция себя ведет на нашем складе приведен график функции рф мы видим что с возрастанием аргумента значение функции стремятся к нулю что это означает это означает что если после постановки всех чисел у вас внутри скобок левой части неравенства получится слишком большое число то значение признака проекта будет выбросом то есть оно очень сильно отстоит от среднего значения итак теперь давайте порешаем полностью мир который демонстрирует поиск выбросов помощью критерия у нас есть выбор как из четырнадцати объектов значение признака по этих объектов приведено на складе что мы делаем во первых вычисляем среднее значение и отклонения среднее значение равно примерно десяти отклонение равно восьми давайте проверять на аномальные значение этого признака естественно проверять цикле абсолютно все значения признаков это не оптимально давайте возьмём значение признака такие которые наиболее далеко лежат от среднего значения такими далекими значением признака п для нашей выборки являются числа двадцать пять целых семьдесят одна сотая двадцать девять целых восемьдесят семь сотых давайте чтобы не терять времени проверим не двадцать девять а двадцать пять а почему а потому что если двадцать пять уже будет аномалии то аномалии естественно будет и двадцать девять потому что она расположена ещё дальше чем среднее значение признака как как проверить на нормальность значение признака двадцать пять целых семьдесят одна сотая все числа нужно подставить формула критерия ширины а потом по таблице вычислить значение функции мфц для наших чисел значение функции рф приведено на экране и оно равно ноль целых четырнадцать тысячных а теперь нужно вычислить правую части неравенства ширины где присутствует лишь объем выборки мы вычисляем и оказывается что левая часть неравенства ширины меньше чем правая его часть это означает что проверяемое значение является выбросом таким образом двадцать пять целых семьдесят одна сотая это выброс и тем более двадцать девять целых восемьдесят семь сотых это тоже выбор а вот если проверять число двадцать ноль целых сорок шесть сотых то это число выбросов уже не будет чтобы убедиться в этом нужно подставить все известные вам значение формулы для критерия шины и мы увидим что неравенство меняет знак на противополо можно число двадцать ноль целых сорок шесть сотых уже не аномалия а процедуру связанную с поиском выбросом по критерию ширины можно повторять несколько раз у нас из предыдущей выборке были удалены два объекта теперь давайте снова применим ту же самую процедуру к оставшимся объектом почему это нужно делать потому что у нас поменяются значение среднего и отклонения и вполне возможно что объекты которые не были выбросами прошлое выборки могут стать выбросов новый выборки теперь у нас двенадцать объектов мы по ним снова вычисляем среднее значение и отклонения естественно получаются новые значения которые вы видите на складе теперь давайте проверим на аномалию значение которое наиболее далеко лежит от среднего значения таким значением является двадцать ноль целых сорок шесть сотых прошлый раз это число прошло проверку аномальность критерий не выявило никакой не аномалия а теперь давайте посмотрим что сейчас получится число все поменялись вычисляя обе части неравенства ширины мы понимаем что оно выполняется и двадцать ноль целых сорок шесть сотых становится уже выбросом новый выборке мы его должны удалить другое число десять целых тридцать восемь сотых по неравенству ширина выбора проверку проходят и оно выбросом не является поскольку у нас там выборка поменялось мы из нее удалили одно число то по хорошему процедуру нужно повторить ещё раз теперь у нас осталось одиннадцать объектов мы пойдем снова вычисляем среднее значение и отклонения и мы будем проверять на аномалию объект который дальше всего лежит от среднего давайте проверим десять целых тридцать восемь сотых подставляя все известные число неравенство ширина мы получаем что оно имеет знак больше таким образом десять целых тридцать восемь сотых проверку проходят и не является выбросом соверша аналогично можно проверить остальные десять значение признака на аномалии я опускаю эти вычисления действительно все они пройдут проверку то есть в этой выборке из одиннадцати объектов аномалии уже не обнаружил яйца таким образом алгоритм основан на крите решены может закончить свою работу и считать что среди оставшихся объектов аномалии нет какие выводы следуют из нашей лекции на нашей лекции мы изучили критерий ширина поиск выбросов как правило критерий семена применяют виде итерационный процедуры которая находит все аномалии за несколько шагов",
        "rating": 1,
        "url": "https://stepik.org/lesson/83196/step/1?unit=59832"
      }
    ],
    [
      {
        "lesson_name": "Поиск выбросов без использования среднего и отклонения",
        "type": "video",
        "step_id": 573479,
        "lesson_id": 198410,
        "content": "а сейчас мы поговорим об алгоритмах поиска выбросов которые вообще не используют понятие среднего значения и отклонения естественно вначале возникает вопрос а вообще такое возможно как можно искать аномалии не использую эти важнейшие характеристики выборки оказывается не все так однозначно использование среднего значения и отклонения на самом деле приводит к некоторым проблемам во первых значение среднего и отклонения сильно чувствительна к наличию выбросов я объясню да этот эффект на одной из предыдущих лекций когда сравнивал средний доход пассажиров электрички до и после появления в ней миллионера таким образом возникает замкнутый круг мы ищем выбраться с помощью среднего и отклонения которые свое очередь как раз и их значения и обусловлены наличием выбросов выборке что делать необходимо предлагать новые методы поиска выбросов которые используют значение среднего и отклонения а какие характеристики выборки нужно ввести вместо среднего и отклонения а давайте познакомимся с такими важными понятиями статистики как квартире итак первая квартире двадцать пять это такое число что ровно двадцать пять процентов выборки меньше него вторая квартира купить пятьдесят это такое число что ровно пятьдесят процентов выборке меньше него кто внимательно слушал мои лекции то понимает что вторая квартира ещё по другому называется как это на самом деле мидиана ответе квартире определяется аналогично она обозначается курсами стать это такое число что ровно семьдесят пять процентов выборке меньше него давайте рассмотрим вычисления квартире на примере какой то конкретный выбор итак нам дана выборка вы видите на складе давайте посчитаем для нее ку двадцать пять пятьдесят ноль семьдесят пять ну легче всего почитать купит десятку она же диана по тем правилам которые были на одной из первых лекций находя для этой выборки меня мы получаем число четыре целых пять десятых оказывается кул двадцать пять ку семьдесят пять для этой выборке равно единице и шести соответственно откуда берутся эти числа как найти пн первую и третью квартире а самом деле правило очень простое когда вы читаете медиану то выборка развивается естественным образом на две части первая часть это все элементы которые меньше медианы а вторая часть это все элементы которые больше медианы по определение медианы обе эти группы элементов содержит одинаковое количество элементов если вы посчитаете медиану уже для элементов из первой группы то вы получите первую квартиру ку двадцать пять а если вы посчитаете медиа оно для элемента второй группы то вы получите третью квартирку семьдесят пять что нужно знать о квартирах оказывается из их определения следует факты приведенные на складе пятьдесят процентов элементов выборки содержатся в интервале ку двадцать пять доку семьдесят пять так вот этот интервал можно взять за некий эталон и те элементы которые находятся слишком далеко от этого интервала объявить выбросами на этом основан алгоритм который ищет выбросы без использования среднего значения и отклонения этот алгоритм строит интервал по правилу указанному на складе чтобы построить этот интервал нужно вычислить как левую так и правую границу левая границ вычисляется следующим образом мы берем первую квартирку двадцать пять вычитаем из нее выражение полтора умноженная на разницу между семьдесят пять и двадцать пять а правая граница интервала считается во многом аналогичны берётся ку семьдесят пять и к ней прибавляется полтора умноженная на разность между курсами специфику двадцать пять и все элементы выборки который в этот интервал не попадут объявляются выбросами внизу на складе показано как мы для нашего примера строя этот интервал нашем примере ооо двадцать пять равно единице у семьдесят пять равно шести интервал построенный по формуле приведенной выше получается равным от минус шести с половиной до тринадцать с половиной таким образом число сто будет являться выбросом оно не попадает в этот интервал а чеснок минут шесть выбраться уже является не будет потому что в этот интервал попал даня и аномалии может приводить иногда как нежелательным последствиям поэтому есть методы которые позволяют искать выбраться и аномалий без использования среднего значения и отклонения",
        "rating": 1,
        "url": "https://stepik.org/lesson/198410/step/1?unit=172631"
      }
    ],
    [
      {
        "lesson_name": "Методы, анализирующие несколько признаков",
        "type": "video",
        "step_id": 573480,
        "lesson_id": 198411,
        "content": "на этой лекции мы поговорим о методах поиска выбросов который анализирует несколько признаков возникает естественно вопрос как эти методы работают и зачем их нужно было вообще придумать потому что на предыдущих лекциях мы демонстрировали успешную работу алгоритмов которые ищут выбросы используя лишь одним признаком данных оказывается алгоритмы которые анализируют лишь один признаков данных ищут выбросы на основании значений лишь одного признака ли чтобы недостатков например следующий слайд показывает один из недостатков таких алгоритмов давайте рассмотрим выборку которая приведена здесь мы видим что в этой выборке есть куча значения которые группируются около единицы и куча значи которые группируются около ста но почему то возникает лишь один элемент пятьдесят который лежит точности посерединке этих двух групп скорее всего пятьдесят это и есть некий выброс который появился наверно зультате ошибки измерении или опечатки я не знаю но как мы пятьдесят обнаружен что это выброс делов том что пятьдесят ноль лежит как раз около среднего значения этой выборки и все алгоритм которые были рассмотрены до этого не увидит что пятьдесят это аномальное значение поэтому нужно разрабатывать новые алгоритмы которые анализируют не один признака несколько только признаков является приведенные на этом складе факт оказывается аномалия характеризуется очень часто не значениями одного признака именно нестандартные комбинации значений нескольких признаков и вот как раз вот типичная комбин акция признаков и приводит к аномалиям давайте рассмотрим такой пример будем изучать людей у каждого человека мы будем измерять рост и вес и на основании измерения сформирован такую большую таблицу людей с известными весами это просто ми давайте посмотрим а рост сто пятьдесят ноль сантиметров это аномалия ну конечно этот раз мне не самый высокий но это не аномалия есть много людей у которых рост сто пятьдесят семь метр принципе в этом ничего плохого нет теперь давайте рассмотрим а вес равный старт килограмма конечно этот лес не такой уж и маленький но впринципе его можно считать нормальным потому что если этот веспрем лежит двухметровым у дяди который тому же тяжелой атлетики занимается то это для него совершенно нормально однако если мы возьмём комбинацию этих значений вес сто килограммов и рост сто пятьдесят ноль сантиметров то это уже скорее всего будет не стандартный объект аномалия как медицинском так и математическом значение этого слова как возникла эта аномалия а именно она возникла из за комбинации двух значений признаков при чем каждый по отдельности признак ничего себе аномального не содержал команде привела их комбинации и поэтому необходимо изучать алгоритмы которые анализируют именно комбинация признаков ищут среди них аномальные комбинации какие то алгоритмы возникает во первых возникает целая группа метрических алгоритмов метрический алгоритм это тело горит на которые встаём вычислении опираются на метрику то есть на функцию расстояние между объект как применить метрику задачи о поиске аномалии нужно рассуждать так его ближайших соседей должно быть очень большое так вот на основании эта идея можно предложить алгоритм поиска выбросов что мы делаем для каждого объекта мы ищем расстояние до всех других объектов нашей выборке вспомню некоторые метрики когда мы найдем такие расстояния то давайте среди всех других объектов найдем его ближайшего соседа то есть тот объект расстояние до которого оказалось наименьшим итак теперь мы у каждого объекта знаем ближайшего соседа такого выбора сами будут те объекты у которых расстояние до их ближайшего соседа очень очень большое этих методов можно применять различные идеи из геометрии например можно сделать следующие итак давайте представим наши объекты виде точек на плоскости их пространстве как это делается очень просто если у нас объекта характеризуется двумя приза коментов каждый объект можно представить в виде точки на плоскости примерно так же как это указано на складе то есть вы отмечаете на всех значений признаков на пересечении ставить точку когда вы на плоскость нанесете все все все свои объекты из выборки возникает вот некоторые множество точек так вот для этого множество точек можно например построить выпуклую оболочку то есть некий такой много угольник или для пространства большей размерности многогранник который эти точки как вот описывать теперь подумайте а какие объекты после построения выпуклой оболочки будут являться выбросами выбросами на самом деле будут объекты которые попали на границу выпуклой оболочки потому что они лежат как бы скрывают от общей кучи эти объекты можно удалить а поиск вопросов продолжить для оставшихся объектов построить новую выпуклую оболочку и так далее выброса оказывается можно искать с помощью решения задачи кластеризации напомним что такое кластеризация кластеризация объектов это когда вы просите чтобы алгоритм разбил ваши данные на несколько групп кластеров и причем каждая группа будет состоять из объектов похожих друг на друга так вот давай применим какое либо из алгоритмов кластеризации мы правда пока не изучали это дело последующих лекций давайте применим какой нибудь алгоритм кластеризации к вашим данным и давайте подумаем объекты каких групп будут объявлены выбросами как вы знаете я думаю самое логичное решение то такое если объект попадает слишком маленький классе то это скорее всего выброс потому что алгоритм кластеризации не находит объектов похожих на него скорее всего это объект аномалия крайнем случае если получается классе состоящий вообще из одного элемента то этот элемент действительно какой то уникальный не похожий на все остальные это скорее всего у вас помимо указанных выше алгоритмов задача поиска образов можно поменять и моделей предсказания например некоторые вариации известной модели предсказания как метод опорных векторов позволяет находить выбросы некоторые вариации решающих деревьев тоже позволяет искать выбросы много подробнее об этих моделях мы будем говорить на соответствующих лекциях когда будем изучать модели предсказания наконец выбросы можно искать вообще с помощью произвольной модели предсказания что для этого нужно сделать итак у вас есть таблица с данными на объекты это сто такие признаки это столбцы так давайте возьмем некоторые столбец признаком п и сделаем следующее запустим модель предсказания признак по другим признакам нашей таблице какие объекты после работал летом предсказания логичнее всего объявить выбросами ну скорее всего это будут те объекты для которых предсказанная и известное значение признака по разойдутся очень сильно это означает что объект он какой то действительно непредсказуемые то есть чтение признака по другим его признаком восстановить для него очень очень сложно скорее всего такой алгоритма чем то уникален но и в нашей терминологии является аномалии или выбросом какие выводы можно сделать на основании нашей лекции первый вывод у методов поиска выбросов анализирующий признаки по отдельности есть недостатки эти недостатки могут исправить методы который анализирует несколько признаков одновременно также выбросы можно искать с помощью алгоритмов кластеризации при показания",
        "rating": 1,
        "url": "https://stepik.org/lesson/198411/step/1?unit=172632"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 307257,
        "lesson_id": 83242,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83242/step/1?unit=59877"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 307259,
        "lesson_id": 83242,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83242/step/2?unit=59877"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536146,
        "lesson_id": 83242,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83242/step/3?unit=59877"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536150,
        "lesson_id": 83242,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83242/step/4?unit=59877"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536152,
        "lesson_id": 83242,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83242/step/5?unit=59877"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 536942,
        "lesson_id": 190151,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190151/step/1?unit=164656"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Постановка задачи кластеризации",
        "type": "text",
        "step_id": 626864,
        "lesson_id": 83197,
        "content": "У вас есть очень большая куча объектов. Нужно навести в ней хоть какой-то порядок. Для этого всю выборку лучше разбить на группы, состоящие из похожих друг на друга объектов. В этом разделе будет показано, как машина умеет сама разбивать данные на группы. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/GKxK9",
        "rating": 1,
        "url": "https://stepik.org/lesson/83197/step/1?unit=59833"
      },
      {
        "lesson_name": "Постановка задачи кластеризации",
        "type": "video",
        "step_id": 582458,
        "lesson_id": 83197,
        "content": "эта лекция начинает целый цикл лекций посвященных задача кластеризации анализе данных во первых давайте разберемся что такое кластеризация для чего нужна с чем её едят начнём с формальной постановки задачи давно множество объектов их можно разбить на несколько групп или кластеров и каждый кластер состоит из похожих друг на друга объект на складе вы можете видеть пример кластеризацию вас было множество объектов это квадратики маленькие и алгоритм классе лизации разбил ваши множество объектов на три кластера каждый кластер показано своим цветом здесь нам че че так сказать повезло результаты работы алгоритма совпадает с тем что от него ожидают человек должен сразу сказать и все должны с этим смириться результаты работал билетов кластеризации не всегда совпадает с интуитивными нашими представлениями от данных то есть алгоритм кластеризации может разбить данные на классе совершенно произвольным впорядке для чего нужна кластеризация для чего проводят пастеризацию объектов во первых классификация позволяет оценить степень сходства объектов это очень важно для таких сложных объектов в анализе данных как например тексты содержание страницы например или пользователи какой то социальной сети активизация позволяет установить между такими объектами степень сходства во вторых кластеризация позволяет упростить дальнейшую обработку данных у вас было большое множество объе это вы применяете кластеризацию множество объектов развивается на классе и теперь для каждого кластера по отдельности вы можете запускать дополнительно алгоритмы анализировать не все кучу объектов а каждый классе по отдельности третьих что можно сделать после кластеризации кластеризация позволяет сократить объем хранимых данных например канализация разбивает ваши данные на кластеры и каждом классе можно оставить не все объекты а только часть из них и таким образом сэкономить память вычислительной машины фмш четвертых кластеризации позволяет искать выбросы об этом я говорил на одной из своих предыдущих лекций кроме того класти рисовать можно не только сами объекты но и признаки это очень интересный алгоритм кота позволяет получить класть или признаков для чего это нужно об этом не скажу одно из последующих лекций все алгоритмы кластеризации можно условно разделить на две большие группы к первой группе относятся следующие алгоритмы это алгоритмы которые разбивают данные на заданное число кластеров то есть количество кластеров оно определяется до начала работы алгоритма это число кластеров как правило определяет человек определяется числом вы запускаете об этом классе зации этот алгоритм разбивает ваши данные на заранее известное количество кластеров существуют алгоритмы и второй группы алгоритма из этой группы разбивают данные на число классиков которое вначале неизвестно число классе здесь определяется самим алгоритмом последующих лекции будут рассказаны алгоритмы принадлежащей как первой так и второй группе к первой группе относятся например алгоритм каминс алгоритм ка средних ко второй группе относятся этом файлов естественно каждая группа алгоритм кластеризации не лишена недостатков чем заключаются недостатки первой группы алгоритмов где число кластеров определяет человек перед запуском алгоритмом отделов том что человек может не угадать нужно число кластеров например для данных приведенных на складе человек всему своего незнания или поспешности может указать ни три классе а две допустимые четыре и соответственно результат алгоритм кластеризации будет не совсем корректным не совсем интуитивно понятными не вполне импортируемым а чем заключаются недостатки алгоритма второго типа здесь число кластеров определяется самим алгоритмом так вот алгоритм может вам выбрать либо слишком много классиков либо слишком мало такая стилизация как правило бесполезно например для данных приведенных на складе алгоритм кластеризации может их разбить либо на один классе либо допустим слишком много на десять классников италии другая кластеризация вообще говоря бесполезным разбиение данных на один классе это вообще говоря не кластеризация мы ничего с данными получается не сделали а разбиение на слишком большое число классиков это тоже я не помню дополнительная процедура потому что такая стилизация никакой пользы не дает позвольте сделать важное замечание для экономии вашего времени я не буду повторять это замечание но каждый последующий лекции потом что на одной из предыдущих лекций мы занимались алгоритмами которые вычисляют метрику между объектами так вот если алгоритм кластеризации внутри себя используют вычисления метрики или расстоянии между объектами то данные перед запуском алгоритма нужно нормировать какие выводы можно сделать на основании нашей лекции во первых кластеризации это действительно важная задача анализа данных она применяется при решении многих задач алгоритмы кластеризации можно условно разбить на две большие группы и дальнейших лекциях будут рассказаны алгоритмы как из первой группы так из второй",
        "rating": 1,
        "url": "https://stepik.org/lesson/83197/step/2?unit=59833"
      }
    ],
    [
      {
        "lesson_name": "Кластеризация с помощью графов",
        "type": "video",
        "step_id": 582459,
        "lesson_id": 83198,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83198/step/1?unit=59834"
      }
    ],
    [
      {
        "lesson_name": "Алгоритм FOREL (формальный элемент)",
        "type": "video",
        "step_id": 582460,
        "lesson_id": 83199,
        "content": "а сейчас мы познакомимся с алгоритмом кластеризации форнелл слово файлов данном случае это аббревиатура обозначающая словосочетание формально элемент крыминформ ли этот алгоритм не имеет никакого отношения а на прошлой лекции мы изучали алгоритм кластеризации которые используют представление данных виде графа а можно ли заниматься кластеризации без использования графов оказывается можно и алгоритм файл является пример я такого алгоритма которые не используют понятие граф давайте познакомимся с основными принципами и идеями которые лежат в основе работы алгоритма файл во первых количество кластеров заранее не определяется алгоритмом файл сам вычисляет количество классиков которое сочтет нужным таким образом видом файл принадлежит ко второй группе алгоритмов кластеризации какая основная идея работал летом файл этот алгоритм ищет образно говоря точки возмущение ваших объект и эти области смущения он определяет качестве классиков давайте рассмотрим формальную схему работы алгоритма файл чтобы подается на вход а его понравится на вход число эр как представлена объект с которыми работает алгоритм файл объекты для алгоритмов она представляется точками пространстве м многомерном пространстве мм это число признаков объектов не надо бояться понятие многомерное пространство давайте поймем его по аналогии когда объектов два признака то есть там равно двум то каждый объект я об этом говорил на предыдущих лекциях можно представить в виде точки на плоскости если он говорит матриц не признак а то очевидно каждый объект можно представить менее точки уже пространстве нашем обычном трехмерном пространстве но если у объекта очень много признаков м то плоскости пространства уже хватает чтобы адекватно представить такие объекты ниточки и поэтому образно говоря эти объекты являются точками многомерном пространстве это означает что каждый объект представляется вектором длины м в этом некоторые качестве значения указаны значения его признаков итак чем заключается первый шаг алгоритма файл произвольную точку пространства мы добавляем новые формальные элемент отсюда собственное название алгоритма файл формально элемент на втором шаге происходит следующее находясь точки с мы проводим окружность или если мы находимся многомерном пространстве мы проводим сферу радиуса сэр променяет окружность радиуса эр из центров мы находим все объекты которые попали внутри этой окружности множество таких объектов давайте обозначим через ка теперь что мы делаем мы находим центр тя прости объектов из множества что такое центр тяжести как его считать это будет понятно следующих лекциях допустим мы центр тяжести этих объектов как то нашли что происходит дальше а точка ф переносятся центр тяжести и находясь уже в новой точке мы снова проводим окружность радиуса р и так далее мы выполняем те же самые действия таким образом шаг две шаг три мы выполняем цикле до каких пор мы должны выполнять это цикл а до тех что пока множество карт не стабилизируется пока у него не перестану добавляться новые элементы исключаться старые вот пока она не стабилизируется мы должны крутиться цикле состоящем из шага две шаг три ну что допустим можно пока то есть множество объектов которые попадают в окружность радиуса эр стабилизировался когда множество к объявляется новым пластиком и объекты у него попавшие исключаются из выбор после этого мы возвращаемся на шаг один если выбор капуста то убери там заканчивать свою работу если же там остаются какие то элементы то мы повторяем те же самые действия направленные на поиск уже нового классика этот сайт содержит некая демонстрация работы алгоритма файлы на первом шаге возникает случайно точка ф и проводится окружность радиуса р после этого мы смотрим объекты которые попали в эту окружность и должны найти центр масс этих объект мы видим что большинство объектов лежит слева от красной точки и лишь только один из них лежит справа это означает что центр тяжести такое группа объектов очевидно будет смещен относительно красные точки влево это действительно так когда мы вычисляем новый центр тяжести то красная точка смещается влево снова проводится окружность радиуса р и мы видим что изменилось множество объектов которые эту окружность попадают например самый правый объект первого шага уже окружность не попадает мы повторяем наш процесс снова вычисляем центр тяжести и снова переносим вы новый центр тяжести центр окружности и таким образом окружности ладно наверно смещается вот по нашим данным после четвертого шага на нашем рисунке огромность больше уже не смещается то есть у нее не попадают новые объекты их не исключается старые таким образом алгоритм файл заканчивает работу связанную с поиском одного кластера все объекты попавшие на последнем шаге нашу окружность об являются объектами одного кластера и в дальнейшем они исключаются из выборки аудитинформ будет искать новые кластеры уже для оставшихся точек ну что осталось разобраться с вопросом как быстро найти центр тяжести точек да на самом деле это очень простая операция по представьте себе что у вас объекты имеют только два признака пайку так вот центр тяжести это на самом неделе будет пара состоящая из средних значений этих признаков то есть это точку которая первая координата равна точности среднему значению по признаку п а вторая координата это среднее значение по признаку рядом складе показано например вычисления центра тяжести у вас есть три объекта два признака пайку новый центр класса это не что иное как вот это среднее значение попали среднее значение покупку естественно когда признаков больше чем две всё происходит аналогично только теперь у вас объекты представляются точками трехмерном пространстве как на этом складе соответственно центр тяжести будет тоже представляется виде точки трехмерного пространства то есть точки с тремя координатами а как считается эти как двенадцать показано на складе какие выводы можно сделать из нашей лекции на нашей лекции мы изучили принципы работы алгоритма файл этот алгоритм ищет области смущения данных и эти области выдает за кластеры и наконец самое главное свойство алгоритма файл который нужно знать количество кластеров заранее не определено",
        "rating": 1,
        "url": "https://stepik.org/lesson/83199/step/1?unit=59835"
      }
    ],
    [
      {
        "lesson_name": "Алгоритм k-means (k - средних)",
        "type": "video",
        "step_id": 582461,
        "lesson_id": 83200,
        "content": "а сейчас мы поговорим об алгоритме каминска средних а на прошлой лекции мы изучали алгоритм файл алгоритм файл разбивает данные на число кластеров которые не известно заранее он сам определяет сколько классе должно получиться давайте же изучим алгоритм котором число кластеров известно заранее то есть этот алгоритм будет принадлежать уже первой группе алгоритм кластеризации какие основные принципы работы алгоритма комедии во первых как я уже сказал количество кластеров алгоритмика минск заранее определено и результате работы алгоритма каменс происходит одновременный поиск центров всех классиков как выглядит работа алгоритма конец ну точнее одна из реалий зации работы алгоритма конец но попадается число как то есть число кластеров на которые нужно разбить данные данные как её случаев орел представляется виде точек многомерного пространства мм это число признаков что такое многомерное пространство я об этом достаточно подробно говорил при описание алгоритма файл а на первом шаге мы генерируем к случайных точек которые будут имеете смысл центров класть их на втором шаге происходит следующим итак мы случайно образом раз ставили центры наших классиков а теперь нужно объекты отнести к тому или иному классу объектов относятся класснючие центр ближе всего находится после этого когда часть алгоритмов отошла к первому класс сейчас ко второму часть третьему и так далее нужно центры кластеров пересчитать среди объектов отнесенных одному классика нужно найти центр масс и туда передвинуть центр классика и я вот все эти действия которые упомянуты ноги знаешь агентами нужно крутить цикле пока не стабилизируется положение центров классиков давайте рассмотрим на примере работ алгоритма каменец итак у нас была целая множество точек точки на плоскости как я уже сказал это не что знаю как представление объектов с двумя признаками то есть по одной оси откладывается значение одного признака по другой версии значение второго признака таким образом объект действительно представляется виде точки на плоскость итак вот во все точки которые вы видите на первом складе наши объекты случайным образом генерируется три центра кластера если присмотреться это три красных крестика на нашем рисунке после генерации центров классиков происходит раскраска объектов то есть каждый объект будет отнесен к тому или иному классу у нас образно выражаясь будет три цветных классика это синий фиолетовый и бирюзовый итак мы видим что все объекты которые находятся ближе всего к третьему красному крестику это бирюзовый классика объекты которые относятся ближе всего ко второму крестику который посередине там находится это будет фиолетовый классе остальные объекты осенней классике что нужно делать дальше вам нужно пересчитывать центры классиков то есть нужно среди звук точек одного цвета находить центр тяжести что мы видим например для берлизова классика мы видим что красный крестик соответствующий центру находится ну на периферии кластера центр тяжести березовых почек он может быть всякое миша мы пересчитываем центр классиков и видим что центр бирюзова кластера поднимается сильно наверх это видно на втором рисунке проведем перерасчет центров кластера для каждого из трех кластеров быть снова пытаемся перекрасить наши объекты мы видим что часть бирюзовых точек становится фиолетовой почему а потому что новое положение центра бирюзового классика для них находится дальше чем новое положение фиолетово центра фиолетового классе такой перерасчета центров кластеров и передвижения центров классе по плоскости нужно осуществлять до тех пор пока не стабилизируется их положение то есть когда для каждого объекта будет определен его цвет это цвет уже больше меняется какие недостатки существуют алгоритма конец оказывается результат кластеризации существенно зависит от выбора исходных центров классиков напоминаю они выбираются случайно и более того выбор наиболее оптимального положи у меня для центров классиков перед первой операции он естественно неизвестен на этих рисунках вы можете видеть как неудачный выбор центров кластеров перед первой операции алгоритмов привел к тому что результаты работы алгоритма кластеризации не совпадает с интуитивным представление человека о существование классов а для этого набора данных то есть мы видим что например два нижних классика которые для человека кажется различными они врезультате работал их наконец были объединены вадим красный классе а вот облако объектов верхнем левом углу которая для человека представляется одним классика было наоборот разбита на два класса на фиолетовой и наконечник это произошло из за того что внизу кстати а случайной генерации процентов кластеров перед началом работы алгоритма была выбрана но некотором смысле не очень оптимальное их расположения выводы на этой лекции мы изучили работу алгоритма каминс основным свойствам этого алгоритма является то что число кластеров известно заранее",
        "rating": 1,
        "url": "https://stepik.org/lesson/83200/step/1?unit=59836"
      }
    ],
    [
      {
        "lesson_name": "Выбор оптимального числа кластеров",
        "type": "video",
        "step_id": 582462,
        "lesson_id": 83201,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83201/step/1?unit=59837"
      }
    ],
    [
      {
        "lesson_name": "Кластеризация по столбцам",
        "type": "video",
        "step_id": 582463,
        "lesson_id": 83202,
        "content": "давайте рассмотрим такую интересную неожиданную вещь как кластеризация по столбцам во первых возникает самый главный вопрос зачем вообще это нужно делать зачем нужно заниматься кластеризации столбцов потому что изначально если вспомнить постановку задачи кластеризации оказалось объектов то есть объекты разбивались на группы то есть на классе а здесь нам предлагается почему то заняться кластеризации столбцов ну давайте пока отложим выяснения причин он наконец лекция а пока посмотрим как это просто делается итак у вас есть таблица объекты это стройки признаки это станции мы видим таблицу которая содержит информацию о студентах что можно сделать и это таблицу можно так сказать транспонировать то есть перевернуть или умными словами зеркально отразить относительная диагонали после такого отражения транспонирования первом столбце у вас уже будут не названия объектов название признаков объекты будут наоборот расположены по горизонтали самой первой строке после этого вы запускаете один из стандартных алгоритмов кластеризации но поскольку у вас теперь по строкам откладываются признаки апостол сам объекты то фактически будет происходить кластеризация по столбцам а теперь собственно давайте подумаем а зачем вообще это нужно делать это нужно делать по следующим причинам во первых кластеризация по столбцам позволяет найти близкие по значению как долго признаки если у вас возникают группа признаков то есть классе признаков которые очень друг на друга похожи то можно например избыточные признаки вообще удалить и в дальнейшем при работе с вашими данными оставить только по небольшому числу признаков из каждого классика такая операция удаление внешних признаков очень важна анализе данных поскольку избыточное слишком большое число признаков а оно мешает анализа данных и влияет очень именно на лучшую сторону на качество алгоритм допустим предсказания но более подробно о способах удаления избыточных признаков я буду говорить на лекции которая как раз и касается отбора признаков какие выводы следуют из нашей лекции на классе оказывается можно развивать не только объекта но и признак как правило такая операция кластеризация признаков проводится для уменьшения объемов данных или для улучшения качества моделей предсказания",
        "rating": 1,
        "url": "https://stepik.org/lesson/83202/step/1?unit=59838"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 310046,
        "lesson_id": 83243,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83243/step/1?unit=59878"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 310047,
        "lesson_id": 83243,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83243/step/2?unit=59878"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310062,
        "lesson_id": 83243,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83243/step/3?unit=59878"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 310066,
        "lesson_id": 83243,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83243/step/4?unit=59878"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536198,
        "lesson_id": 83243,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83243/step/5?unit=59878"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "choice",
        "step_id": 536991,
        "lesson_id": 190152,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190152/step/1?unit=164657"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Постановка задачи предсказания",
        "type": "text",
        "step_id": 626865,
        "lesson_id": 83203,
        "content": "Мы переходим к изучению задач предсказания. Сегодня мы будем предсказывать числовые признаки с помощью очень простой модели линейной регрессии. Хотя, может быть, и не такой простой... \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/6yzyK",
        "rating": 1,
        "url": "https://stepik.org/lesson/83203/step/1?unit=59839"
      },
      {
        "lesson_name": "Постановка задачи предсказания",
        "type": "video",
        "step_id": 588074,
        "lesson_id": 83203,
        "content": "сегодня мы начинаем заниматься изучением задач предсказания эта тема очень длинные фактически изучение задачи предсказание мы будем заниматься на всех всех всех лекциях буквально до конца нашего курса что такое задачи принципы здания как ваше формальная постановка формальная постановка задачи предсказания достаточно проста у вас есть множество объектов с некоторыми признаками среди всех признаков давайте выделим один такой важный признак игрик признаках ещё надо бывают целевым признакам так вот что известно у вас есть выборка объектов для которых значение признака игорь известно известно для всех объектов выборки что нужно сделать модель модель должна понять как признак или как любой признак их зависит от других не целевых признаков ваших объектов и когда на вход вашей модели подается объект новый объект для которого значение признака их неизвестно модель должна сказать почему действительности равно значение три знака или для нового объекта в этом заключается задача предсказания задачи предсказания делятся на два типа и причем отделения существенно потому что методы решения и методы проверки качества моделей машинного обучения для разных группа указанных на складе оно существенно различные первая группа задач предсказания эта задача регрессии если предсказывается целевой количественный признак или кто такая задача называется задачей агрессии если же предсказывается значение номинального или категориального признака или кто задачи предсказания называется задача классификации например для данных указанных на прошлом складе предсказания коэффициента интеллекта можно рассматривать как задачу регрессии а предсказания пола по другим признакам человека является задача классификации я думаю у некоторых собственных сложилось негативное отношение к слову регрессии но действительно это слово несет в себе негативный оттенок действительно регион серьги с деградации так далее так далее почему модель предсказания получили такое обидное прозвище кака задачи регрессии отдела чем это произошло случай когда видовое имя стало применяться к описанию всего рода предметов такие случаи происходили не единожды например есть от производителя детских товаров под названием памперсы но с другой стороны слово памперс часто используют для описания так сказать всего рода то есть детского товара как то кого то же самое произошло слово ксерокс ксерокс одной стороны это некий фиксированный бренды копировальной техники но часто словом ксерокс нашей стране обозначают так сказать сам так сказать вид копировального аппарата вот ааам для тех кто не живет в нашем богоспасаемой городе омске пояснил что слово человека в нашем городе употребляется как замена фразе лапша быстрого приготовления делов том что в начале девяностых когда лапша только начала появляться российском рынке первым брендом который появился в нашем городе был бренд вообще всё теперь слово часть очень часто употребляется для описания как бы сказать всей всего множества различных брендов лапши быстрого приготовления то то же самое случилось с регрессией делов том что исторически самая первая задача предсказания она обнаружила эффект некоторого регресса к среднему что это была задача это очень известный факт из истории математики и статистики ещё девятьсот потом веке в англии исследователи пытались предсказать рост сына по росту его отца для этого они составили огроменный выбор которой заносили рост отца и его взрослого ребенка для красоты жаль только мужчины отцы и сыновья и полученные данные были обработаны ещё тогда ручным способом то есть была проведена около соро колоссальная адская работа и что получилось оказалось что рост сына регрессировал к среднему знать линию роста мужчин по стране как то можно пристреливать помощью картинки она складе вы видите иллюстрации из за внешнего журнала девятнадцатого века вкачестве горизонтальной линией показана средний рост мужчин по всей стране но в данном случае по всей англии авто наклонных линий показано как зависит рост сына от роста отца что произойдет если роста отца больше чем средний рост по стране на рисунке эти люди у которых роста выше среднего показанных правой части мы видим что если роста отца выше среднего то как правило рост его ребенка будет меньше роста отца то есть он будет приближаться к среднему значению роста мужчин по стране аналогичная ситуация происходит когда рост отца ниже среднего в этом случае роста ребенка как правило наоборот выше роста отца но все равно это просто рот сына приближается к среднему значению по стране вот собственно регресс слово или фразу собственно из решения данной задачи какие выводы можно сделать из нашей лекции вывод номер раз задачи делятся на два типа эта задача классификации и задачи регрессии причем это деление существенно задачей классики если задача грехи требуют принципиально различных подходов к своему решению и измерению качество построенных моделей",
        "rating": 1,
        "url": "https://stepik.org/lesson/83203/step/2?unit=59839"
      }
    ],
    [
      {
        "lesson_name": "План решения задачи регрессии",
        "type": "video",
        "step_id": 588077,
        "lesson_id": 83204,
        "content": "а сейчас мы рассмотрим общий план решения задачи регрессии а план решения задачи классификации он будет рассмотрено анапа следующих лекциях но во многом аналогичен план решения задачи регрессии действитель как решать задачи предсказания на примере задачи регрессии итак у вас есть данные это таблице данном случае четырех объектов которые вы видите на складе информация людях что нужно сделать с этой таблицей эту таблицу нужно разбить на две выбор эти тренировочную и пятого что будет происходить дальше модель предсказания будет строиться по объектам из тренировочной выборки а качество моделей предсказания будет заменяться по объектам из тестовой из проверочный выборки итак что нужно делать дальше допустим как то мы вот по тренировочным выборки научились строить модели предсказания допустим как они строятся мы будет рассказана далее пока давайте предположим что мы как то умеет предсказывать целевой признак ик вот для таблицы которая приведена на складе целевым признаком игорь является айкью что нужно сделать дальше как это можно оценить качество предсказания для этого нужно взять с собой проверочную выборку для каждого объекта второй выборки известно точное значение целевого признака вы видите его на экране есть столбец где они пьют точный который нам как бы был известен есть таким который получен результате работы нашей модели модели предсказания как зная эти пары чисел можно заметить качество моделей предсказания модели регрессии первым показателем качества модели регрессии является мая средняя абсолютная ошибка она считается очень просто ну я думаю очевидно что качеством это предсказание напрямую зависит от величины от разницы между истинным и предсказанным значением так давайте возьмём эти разницы посчитаем для каждого объекта и прошу миром таким образом у нас возникнет сумма моделей разности между истинным и предсказанным лечением но и поделим на количество всех объектов тестовой выборке чтобы величину усреднить полученная в результате вычислений величина называется мая средняя абсолютная ошибка на складе приведем пример вычисления мая для объектов из наших тестовой выборке нашем примере мая получилась равной пятнадцать помимо мая можно вычислять марте средняя абсолютная ошибку процентов она вычисляется следующим образом снова вычисляется модуль разности между истинными предсказанным значением и делится она модуль истинного значение целевого признака но естественно я здесь оставляем за скобками случай когда истинное значение целевого признака равно нулю получается что мы делим на ноль но впринципе эту проблему можно обойти исключив из линия марты объекты для которых истинное значение признака равно нужен итак вы вычисляете модуля разности между истинным предсказанным значением делите на моду истинного значения признака суммируйте это все усредняет разделите на н на объём тестовой выборке ещё умножаете на сто процентов чтобы получить величину процентов пример вычисления массы для нашей тестовой выборки вы видите на нашем складе маты нашем случае казалось двадцать пять процентов ну чтож давайте теперь скажу очень страшная вещь оказывается модели регрессии не обязана давать точный ответ на объектах тренировочные выборки то есть на тех объектах которые использовались при ее построении опаньки а почему так происходит а вот смотрите итак у вас была тренировочная выборка из двух объектов на складе она показана вверху выполнении построили модель предсказания вы предсказываете ип по другим характеристикам человека и вот для построения модели вы решили приколоться и подать ей на вход объект который уже использовался при ее построении оказывается не нужно это понять что ответом модели не обязательно будет число которое было указано тренировочный выборке почему так происходит а дело вот в чем модель предсказания если это настоящая конечно модель предсказания она работает не так не запоминает все значения целевого признаков тренировочная таблице она не запоминает значения целевых признаках она пытается именно построить зависимость между нецелевыми признаками и целевым признакам и поэтому если найденная зависимость говорит о том что целевой признак для объекта должен быть немножко другим не таким каким он был тренировочный таблиц цветок модель вполне возможно для объекта из тренировочной про таблицы которые мы подаем ее вход может показать другое значение целевого признака это все это естественно все точки зрения ну чтож теперь давайте посмотрим как должно работать моделью действий точнее модель линейная регрессия который будет рассказана этой лекции на последующих итак у вас есть тренировочная выборка тренировочная выборка объектов это та по которой вы строите модель у нас каждый объект будет иметь а один нецелевой признак х и один целевой признак иг поскольку у каждого объекта есть два признака то эти объекты можно изобразить на плоскости по горизонтальной оси мы откладываем признак экспо вертикальный признак тогда каждый объект тренировочный выбор это точка или узел узлы на нашем складе изображение видел таких вот осенних ромбиков возникает вопросов каком виде мы будем искать зависимость целевого признака от нецелевого признаках а это можно сделать двумя очевидными способами можно соединить наши объекты с помощью сломанной синий линии а можно это сделать с помощью красной прямой которая проходит где то примерно посередке между всеми объектами какие минусы есть у ломаные линии что это за модель такая интересная получается то есть если мы берем качестве модели предсказания значения налоговой долг будет происходить предсказания признака и вот допустим у нас есть значение ломанный точки один точки два мы хотим предсказать значение признака для объектов которого экран полтора что мы должны это сделать мы должны взять точку полтора на горизонтальной оси или провести вертикальный перпендикуляр до нашей ломай его то значение которое будет наломанными будет значением предсказываемых значением признака или для нашего объекта какие минусы есть такого подхода во первых когда вы предсказываете значение числового признак с помощью вот такой ломаной то оказывается модель сама модель предсказания она имеет очень болит сложность сравнимую с объемом данных но действительно у вас получается а сколько отрезков ломаных сколько собственно и было данных тренировочный выборки то есть модель предсказания она не проще чем сами данные кроме того такую модель совершенно никак нельзя про интерпретировать она не имеет никакой естественно интерпретации а естественно нет никакой гарантии что когда вы будете замечать качество построенной модели подстройки там не возникнут слишком большие ошибки и четвертый недостаток этой модели он у меня на складе не поместился но заключается в следующем что построенная модель виде ломаной вы никак не можете экстраполировать на представьте себе если вам доход подадут объектом которого х равен семнадцать надо точка семнадцать никакой вам она не или что нужно выдавать качестве ответа совершенно непонятно таким образом построение модели предсказания с помощью грубой вот интерполяции то есть помощью соединения данных тренировочный выборки с помощью лома но это не есть удовлетворительно решения нужно строить прямую агрессию которая показана на складе с помощью прямой красного цвета возникает вопрос а почему красная прямая лучше чем синяя лома на нашем складе красная прямая почему то не про нет не через один узел не через один объект тренировочный выборки то есть каждый объект тренировочные выборки по которой эта красная прямая была построена он ей этой прямой не удовлетворяет почему красная прямая должна быть лучше чем ломанная но оказывается именно такой подход можно обосновать следующими соображениями мы живём в не идеальном мире естественно когда вы заверяете значение признаков различных объектов то неизбежно возникают погрешности удаление так далее так вот там одним из оптимальных подходов когда вы оперируете раз объектами несовершенного мира является их апраксии мации то есть вы проводите примую которое наибо это близко лежит ко всем вашим объектом а то что объекты не попадают на эту прямую вы просто интерпретируете это ошибками но и ссылается на то что ничего идеального нашем мире не бывает расхождение между положением прямое положение те кто это вот грубо говоря ошибка измерения случайно ошибка измерения так вот на следующих слайдах настоящий секс будет показано как нужно строить модель линейной регрессии которая оптимизирует ваши данные с помощью прямой итак модели регрессии называется линейный если значение предсказывая мова признака или как вычисляется как сумма нецелевых признаков хх которые берутся с некоторыми коэффициентами и ещё говорят леса согласно формуле которая видна на вашем складе получить предсказываемых значение игры для этого нужно произвести следующие вычисления нужно взять значение целевого признака фексадин домножить его на некоторые места дубль вы один то же самое сделать нецелевым признакам их два и так далее до см все все полученные числа суммируйте прибавить свободный член клуба ноль задача построение моделей агрессии заключается в нахождении оптимальных вне котором смысле значение лесов или коэффициентов дубль вы их возникает вопрос а как же самое искать так вот принцип основополагающий принцип построения линейной регрессии следующий нужно взять объект и тренировочный выборки и для них минимизировать отклонения от объектов до прямой которая будет строиться естественно такой подход является неустойчивым курсом поэтому прежде чем строить модель линейной регрессии нужно для ваших данных запустить один из алгоритмов поиска выбросов и все подозрительные объекты все аномальный объект из выборки естественно нужно заранее удалить как конкрет настроиться модель линейный речи будет показана следующая лекция пока можно сформулировать предварительные выводы итак при решении задач агрессии выборку нужно разбить на две части тренировочную и тестово смысл построения моделей линейной регрессии заключается зачем вам нужно минимизировать величину отклонений объектов до разделяющей прямой если объекты характеризуется не одним признаком под двумя и более то с геометрической точки зрения вам нужно минимизировать отклонение уже не до прямой адрес русскости или до такого общего объекта как гиперплоскость если число признаков у вас слишком велико",
        "rating": 1,
        "url": "https://stepik.org/lesson/83204/step/1?unit=59840"
      }
    ],
    [
      {
        "lesson_name": "Построение модели линейной регрессии",
        "type": "video",
        "step_id": 588076,
        "lesson_id": 83205,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83205/step/1?unit=59841"
      }
    ],
    [
      {
        "lesson_name": "Проблемы модели линейной регрессии",
        "type": "video",
        "step_id": 588078,
        "lesson_id": 83206,
        "content": "а сейчас поговорим о неприятном о проблемах которые возникают с моделями линейной регрессии на этой лекции мы разберемся какие проблемы во первых возникают и во вторых что очень важно какие проблемы может узнать решить только человек то есть какие проблемы можно отдать на откуп вычислительной техники а вот есть вопросы которые кроме человека никто решить на самом деле не сможет он давайте разбираться итак какие возникают проблемы построение моделей регрессии делов том что самым важным моментом построение такой модели является решение системы линейных уравнений на одном из шагов нашего алгоритма так вот когда вы строите системы и направление может оказаться что либо полу иная система не имеет вообще решений либо она может иметь более одного решения когда такие проблемы возникают а такая проблема возникает когда например между нецелевыми признаками существует очень сильная линейная зависимость ибо очень высокая корреляция то есть если вы почитаете допустим конкуренции между какими то необходимыми признаками то на реляции может оказаться по модулю очень близким к единице такой проблемы еще называют проблемой мультика и неровности то есть наличие очень сильной зависимости между нецелевыми признаками давайте рассмотрим на примере к чему приводит наличие такой очень сильной зависимости между нецелевыми признаками вот перед вами очень простенькая табличка между необходимым признаком с один и нецелевым признаком с две существует очень сильная зависимость а почему должен просто потому что они совпадают друг друга правильно так вот давайте на основании этой табличке по правилам которые были на прошлых лекциях построил модель линейной регрессии поскольку здесь необходимых признаков уже что то предсказывать значение эрика мы будем по формуле думали вы один из один плюс дубль в две х и плюс дубленой таким образом нам нужно найти неизвестное значение для трех весов думаю один двести рубль ноль х делается все аналогично примеру который был разобран на прошлой лекции а нужно посчитать сумму квадратов отклонений раскрыта все скобки посчитать частные производные приравнять их к нулю и далее решать системы линейных уравнений какая система ленина уравнение тут у нас получается но здесь возникает три уравнения потому что было три частных производных но оказывается что первое уравнение точности совпадает со вторым и таким образом фактически вы нашей системе не три уравнения а две почему это приводит это приводит к тому что неизвестных больше чем уравнение нашей системе и поэтому данная система уравнений имеет бесконечное число решения можно показать очень легко что любая точка вот такого вида а единица минуса и третья координата равна единице где а произвольно вещественное число удовлетворяет нашей системе уравнений таким образом вот как я уже сказал это система имеет бесконечное множество решений новые вопросы что там плохого имеет ни одно решение целых целых бесконечности решений с наоборот радоваться должна это то есть вы можете взять достаточно произвольное значение параметров дублевые их например ноль пять ноль пять единицу по этой тройки построить модель предсказания которые при цена в самом низу слайда или радоваться вот вы научились предсказывать значение признака ик на самом деле проблема есть и они красот связано с тем что решение этой системы бесконечно много отдела общем поскольку вы можете окончательно модель предсказ сами выбирать так сказать произвольно из бесконечного набор возможностей топ модель теряет свойства интерпретируем асти вы никак не сможете доказать заказчику что вы выбрали именно эту модель а не другую например то есть у вас никто не поймет почему то что мы выбрали лучше чем другие модели поскольку решение системы вас бесконечно то вы можете выбрать бесконечное множество моделей то есть например вы можете взять модель предсказания игра винкс один плюс один а можно взять и экран и две господин китая там одна ли она удовлетворяет системе уравнений из прошлых слайдов какая из этих моделей лучше сложно сказать более того эти модели они на самом деле утверждают совершенно противоположные вещи а именно первая модель утверждает что признак никак никак не зависит от признака с две а вторая модель утверждает наоборот что их никак не зависит от признаков х один мы кто прав из этих моделей непонятно да все это произошло из за того что у нас вот сижу тема имеет бесконечное множество решений и непонятно какую модель тогда выбирать кроме того из за бесконечности множество решений системы уравнений может возникнуть проблема связанная с некоторой такой ошибкой например представьте все что качестве коэффициентов значение лесов были выбраны очень очень большие по модулю число вкачестве дублевые один было выбрано десять шестой степени вкачестве дубль два была выбрана единица минус десять шесть пинов качестве были в зале было выбрано единиц что произошло а это произойдет к неустойчивости работы модели итак представь себе что у вас есть объект у которого на два равно нулям и экран тоже нужен для этого объекта будет предсказано значение равное единице действительно если вы поста проверьте номер вот та формула для вычисления или четырех приведенные на складе вынесено получить единицу а теперь давайте возьмём очень близкий объект у которого признаках два тоже равен нулю а признак один он правда не надо чуть чуть отличается а сточки зрения так сказать я думаю статистики здравого смысла эти объекты очень близки друг другу так вот для этого объекта если вы подставите из за очень больших коэффициентов моделей регрессии произойдет всплеска предсказываем означать непредсказуемая значение будет равно тысячи таким образом смотрите при переходе попова значение признака фексадин равно нулю крикс один равному одна сотая происходит очень резкое увеличение предсказываем обозначения игристых это я нормально и нужно как то с этим бороться вся проблема из за слишком больших весов которые мы можем выбрать возникает вопрос а что делать вопросом кто виноват мы уже разобрались это произошло из за того что система имеет бесконечное множество решений а вот что нужно делать ну во первых нужно исследователи уделить время такому важному процессу как отбора признаков из за множества необходимых признаков нужно удалять те признаки которые очень сильно зависит от других если у вас есть пара нецелевых признаков и значение коэффициента корреляции по модулю между ними очень близко единицы то из этой пары лучше один из признаков удалить кроме того существует второй подход можно настроить модель линейной регрессии и по слегка измененным форму и мы получим альтернативные подходы к построению регрессии далее коллекции рассмотрим два из них это регуляризация и метод ласок",
        "rating": 1,
        "url": "https://stepik.org/lesson/83206/step/1?unit=59842"
      },
      {
        "lesson_name": "Проблемы модели линейной регрессии",
        "type": "video",
        "step_id": 588079,
        "lesson_id": 83206,
        "content": "ну что же далее рассмотрим два обобщения моделей регрессии это реализация и класса начинается реализации на одном из предыдущих слайдов была вот эта табличка и модель всё очень большими значениями часов я говорил что эта модель очень плохая возникает неустойчивость и её работе для очень близких объектов она выдают очень разные значения предсказуемого признака так вот и я и говорил раньше что нужно стремиться к тому чтобы значения часов по модулю были как можно более маленькими как можно более небольшими нельзя выбирать качественный софт очень большие числа поэтому предлагается минимизировать не только выражение л которой равна сумме квадратов отклонений но также минимизировать величины весов дубль вы их как то делать возникает вопрос нужно минимизировать не только исходную выражение л а че че вот такую сумму нужно взять некую константу ция и доложите наскоков сколько будет стоять сумма квадратов наших лесов и далее мы будем минимизировать указанное выражение искать оптимальное значение весов были в их ну что ж давайте прогоним это все на примере для простоты возьмём значение константы равны единице и далее мы будем минимизировать выражение указана внизу нашего слайда как это делается давайте посмотрим как это работает модели регуляризации для того же самого примера где и признаками один и две имеет очень сильную зависимость доли другом более того они совпадают а вы по прежнему ищем уравнения регрессии вот в таком вот виде нам нужно найти неизвестное значение лесов болевой один дубль два и были но теперь мы будем минимизировать не выражение для л а выражение р она немножко отличается я отел там ещё добавляется сумма квадратов весов мы находим частные производные для выражения частной производной для выражения они имеют очень похожий вид мы видим возникает поражение скобках скобках показано выражение которое возьми цикла при дифференцировании эл привод дифференцировании старого выражения но поскольку дальше у нас идет сумма квадратов то возникает новая слагаемое для первой производной который указан на складе это слагаемое равно два дубля один вы другие производные добавляется прямо две дубль две дубль ноль после этого частные производные вы как обычно приравнивается к нулю но из за того что возникли вот такие вот добавочки каждом уравнении полученная система уравнений не совпадает с той которая была разобрана выше более того эта система уравнений она хорошая хорошая том смысле что она имеет единственное решение это решение приближённым она равно будто следующим значениям и таким образом вы уже можете качестве ответа выдать следующая модель линейный рейсы вы просто представляете вот найденное значение место думали в две и дубль вэ ноль и это таким образом вы уже знаете как нужно предсказывать значение признака или давайте посмотрим как общих чертах модель реализации зависит от выбора константа просто последнем примере констанция я выбрал так сказать произвольно я положила что она равна единице все на самом деле значение константы цен можно варьировать и от этого очень сильно может изменяется поведение модели ней регрессии на этом слайде показаны результаты некоторых вычислительного эксперимента что происходило мы варьировали значение константы т и замедлялась величина весов веса каждого признака на рисунке вы видите много цветных графиков каждый цветной график соответствует значению веса некоторого признака и что можно увидеть исходя из информации на этом складе а мы видим что если вы будете очень сильно увеличивать константу т то все признаки точнее весса всех признаков будут стремиться к нулю причем они могут стремиться к нулю так сказать непрерывно синтетически очень гладко стремятся к нулю и уменьшается модули но теперь осталось разобраться с константой т делов том что константа цен для логарифма реализации является неким входным параметром то есть человек заранее задает её значение и допускает алгоритм реализации естественно возникает вопрос а какое опт реальное значение для константы т нужно выбрать ну самый простенький совет заключается в следующем но давайте возьмём как можно больше различных значений для этой самой констан для каждого из этих значений мы строим модели регрессии и проверяем её качество на тренировочной выборки окончательное решение мы принимаем следующим образом для каждого значения константы т замеряли качество модели на тестовой выборке так вот давайте выберем то значение константы т на котором точность модели на тестовой выборке вам наилучший это что называется самый простой совет выбора параметра т естественно есть более зашоренные методы например кровь валидация который будет рассказана одно из следующих лекций давайте познакомимся со вторым обобщением модели линейный агрессии который называется ласок оно это самая ласок очень похоже на легализацию только там минимизируется немножко другое выражение давайте теперь будем минимизировать выражение которое очень похоже на выражение раздельный регуляризация но чуть чуть отличается а именно возникает та же самая добавка плюс т константа некоторые умноженное на скобку скобки будут стоять не сумма квадратов а сумма модулей весов и так как его я уже говорил это нам нужно для того чтобы врезультате была построена модель регрессии которое значение весов не слишком большие конечно здесь возникает функция модуля поэтому точку минимума выражение мы не можем таким простым способом то есть помощью взятия частных производных тем не менее суд математике высшей математике существует изощренные методы которые тем не менее позволяет найти точку минимума и в этом случае то есть эта задача решаемая и оптимальное значение лесов ром рано или поздно будут найдены а как зависит от значения весов от значение константы т делов том что каждый случай регуляризации конфронтация является входным параметром алгоритма и она определяется чело летом что будет если человек будет варьировать значение константы здесь на графике показаны все будет происходить свесами для признаков здесь как и на прошлом графике каждому весу соответствует линия своего цвета и что происходит если мы увеличивать значение константы т то оказывается значение признаков большинства признаков будет стремиться к нулю но в отличие от моделей регуляризации где это стремление к нулю такое плавное синтетическая здесь стремление к нулю а что такое более явное а именно привлечение константа т большинство признаков становятся равными нулю начиная с некоторого момента и далее они нолями и остаются то есть при применении метода ласок происходит замедление кафе центов при некотором количестве признаков и чем больше величина константы тем большее число признаков у вас забавляется но естественно так же как случае сигнализации возникает вопрос выбора оптимального значения константы совет здесь точно он такой же нужно взять много много различных значений для тех которые вы хотите протестировать для каждого из этих значений запустить модель голосов и в конечном итоге выбрать эту модель на которой достигается наилучшее качество на тестовой выборке ну и я снова напоминаю что более изощренные методы выбора оптимальных значений входных параметров наших моделей будет рассказана следующих лекциях например мы познакомимся с такой процедурой как раз валидации на этом графике просто показано как растет или наоборот уменьшается качество работы моделей результате формирования константы т что означает этот график по горизонтальной оси отложено значение константы и для каждого значения констант замедлялась качество работы моделей на тестовой выборке что мы видим мы видим что вначале привлечения константы т качество улучшается но потом она наоборот начинает возрастать и таким образом возникает точка минимума то есть то значение константы цены котором на тест второй выборки наблюдается наилучшее качество наименьшее ошибка и это значение константы цену для данной модели для данного графика но равно примерно где то три или две нужно взять качестве окончательного какие выводы можно сделать из нашей лекции о линейной регрессии есть свои проблемы эти проблемы как правило связаны с наличием зависимости или корреляции между нецелевыми признаками данные проблемы можно решить как с помощью отбора признаков то есть выкидывание лишних нецелевых признаков так и с помощью обобщения моделей регрессии которые называются регуляризации голосок",
        "rating": 1,
        "url": "https://stepik.org/lesson/83206/step/2?unit=59842"
      }
    ],
    [
      {
        "lesson_name": "Полиномиальная регрессия",
        "type": "video",
        "step_id": 588080,
        "lesson_id": 83207,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83207/step/1?unit=59843"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310275,
        "lesson_id": 83244,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83244/step/1?unit=59879"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310278,
        "lesson_id": 83244,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83244/step/2?unit=59879"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536204,
        "lesson_id": 83244,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83244/step/3?unit=59879"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536205,
        "lesson_id": 83244,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83244/step/4?unit=59879"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536206,
        "lesson_id": 83244,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83244/step/5?unit=59879"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 537007,
        "lesson_id": 190153,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190153/step/1?unit=164658"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "План решения задачи классификации",
        "type": "text",
        "step_id": 626866,
        "lesson_id": 83208,
        "content": "В прошлом разделе мы учились предсказывать количественные признаки, то есть решать задачу регрессии. А сейчас мы будем заниматься классификацией, то есть предсказывать значения номинальных признаков. Но станет ли нам от этого легче? \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/KJBJY",
        "rating": 1,
        "url": "https://stepik.org/lesson/83208/step/1?unit=59844"
      },
      {
        "lesson_name": "План решения задачи классификации",
        "type": "video",
        "step_id": 590324,
        "lesson_id": 83208,
        "content": "а сейчас мы начинаем цикл лекций посвященных решению задачи классификации она это и на последующих лекциях мы расскажем о том что такое задача классификации какими методами она решается и как можно измерять качество классификации кроме того мы на одной из следующих лекциях изучим такую универсальную процедуру как кросс валидация эта процедура позволяет находить оптимальные значения для параметров алгоритмов предсказания начинается с этой лекции мы прощаем сейчас задачами регионе потому что они такие плохие нам жалкое заниматься нет на самом деле когда мы изучим много алгоритмов классификации то мы поймем как с помощью классификации иногда возможно решать и задачи регрессии в данной лекции я расскажу о том как нужно общих чертах решать задачи классификации как нужно замерять качество построенных классификаторов итак давайте я напомню как общих чертах решается задача предсказания этот шаблон работал и случае задача регрессии оно первых я уже сначала лекцию потребил кучу раз слово весло и куча слово классификация давайте все таки сначала напомню чем они все таки различаются делал чем когда вы предсказываете количественные числовой признак то задача предсказали называется задачи регрессии а если вы предсказываете номинальные или категориальные говорят признак то задача предсказали называется задача классификации вот на этом слайде вы видите что у вас есть информация людях и давайте будем предсказывать их пол по росту и подвес поскольку пол это признак категориальной то мы будем заниматься задача классификации так вот какой общий шаблон решения задачи классификации он очень похож на подход связан с решением задачи версии нужно все ваши выборка объектов раскидать на две таблицы тренировочная выборка и тестовые выборка модель предсказания будет строиться по тренировочным выборки а качество классификации будет проверяться по тестовой выборке на самом деле может да и они сильно упорствовать и очень часто задача классификации можно свести задачу регрессии следующим образом отдавайте волевым решением представим что допустим для нашей задачи предсказания пола это будет предсказания числового при на которых мы будем предсказывать вещественное число так вот после этого когда вы решили что предсказываем признак имеет числовую природу вы начинаете автоматически решать задачи регрессии и представьте себе что вы как то построили модели регрессии у вас получилось предсказанные значения вот на нашем складе приведем пример решения такой задачи регрессии мы представили что пол является числовым признаком запустили одну из моделей регрессии и она выдала качестве предсказанные значения вещественное число ну вот мы видим например что она поиска бывает что для первого объекта пол равен ноль целых пятьдесят пять сотых для второго объекта пол равен ноль целых восемьдесят одна сотая то есть она выдает числовые значения поскольку это была задача россии как полученные числовые значения можно про интерпретировать и решив при этом задача классификации ну самый простой способ это полученные числа нужно округлить до нуля или до единицы и таким образом у вас получается уже метки классов то есть либо ноль либо единица таким образом вы ваши задачи классификации я только что рассказал как задачу классификации можно решить с помощью задачей агрессии на самом деле этот шаг очень экзотический большинстве случаев исследователи пытаются решать задачи классификации непосредственно не слабеет задачей агрессии как я уже сказал вам нужно построить модель классификации по тренировочной выборки и проверить его качество по тестовой выборки почему это так потому что тестовой выборке вам известно для каждого объекта значения у него признаков нота на складе вы можете видеть что у каждого объекта тестовой выборке известно истинное значение пола об скобочках указано то значение которое предсказала модель классификации так вот возникает вопрос а как можно удостовериться том что построена модель классификации работает хорошо или наоборот работает плохо ну давайте для простоты дальнейшем мы будем изучать бинарная классификация то есть когда предсказывается целевой бинарный признак то есть когда у вас возможное значение у него признаков состоят лишь из двух значений либо мальчик либо девочка например вы предсказываете пол человека который как правило действительно является бинарным признаком либо мужчина либо эта женщина мы далее вся последующая теперь будет касаться именно бинарный классификации если вы изучаете задача которой предсказывается неординарный признак то все понятия которые будут на дальнейших лекциях указаны допускает легкое обобщение на случай не бинарной классификации а как можно заметить качества модели классификации а для этого нужно сделать следующее у вас на тренировочной выборки построена модель классификации вы тестовую выборку прогоняется через эту модель и смотрите а какое значение какую метку класса предсказала вашему для каждого объекта тестовой выборке и что у вас получается а у вас должна получиться вот некая такая вот матрица ошибок чем смысл этой матрице это матрица содержит в себе информацию о том как часто модели ошибаются при своей классификации ну давайте обсудим как эту матрицу нужно заполнять это матрица имеет размеры две на две и каждый ячейка имеет собственное название например для краткости ячейки обозначенной следующими ну правда английскими словосочетании время ячейка которая находится вне левом верхнем углу называется то негатив то есть истина отрицательные объекты истинно отрицательные значения ячеек которые находятся вне нижнем правом углу называется истина положительные объекты истинно положительное значение очки которые расположены так сайте вне от главной диагонали они называются соответственно ложноположительный и ложноотрицательный ячейки ну что же как эту матрицу ошибок нужно заполнять он я тебе расскажу на примере давайте мы будем проектировать такое классификатор как эта рамка металлодетектора но это очень полезный и нужный и народном хозяйстве инструмент который позволяет например отличить от человека который проходит в аэропорта без запрещенных предметов а также поймать тех людей которые несут товара здание аэропорта какие либо запрещенные предметы итак вы когда проектируйте эту рамку металлодетектора вы естественно начинаете проводить некие испытания смотрите как адекватно таранка работает для этого выберите группу людей делятся на две части часть людей проходит через рамку металлодетектора не имея никаких запрещенных предметов а вторая группа людей проходит через рамку металлодетектора саморезами неко так запрещенных к проносу предметов что происходит давайте группу людей которые не имеют запрещенных предметов далее для краткости будем называть честными нормальными людьми а группа людей которые несут муляжи или действительно реально опасно приметы будем для краткости дальше называть террористами если человек принадлежит классу нормальных людей то будем считать что он имеет метку ноль если человек принадлежит классу туристов то далее будем считать что он принадлежит классу один естественно назначение таких меток нет никакой дискриминации лежит на нас могут обвинить в том что нормальные люди это как бы мы ничего не значит а тариста это как бы единица то есть как бы нас могут обвинить в том что мы как бы возвеличивает террористов но я должен сразу заметить что метки классов это конечно условное обозначение то есть можно было и людей обозначите наоборот ну просто исторически сложилось следующее соглашение что наиболее массовую группу обозначается помощью метки класса нулю а маленькую группу то есть группы состоящей из них типичных представителей очень часто обозначается помощью метки один поскольку нормальной ситуации большинство людей террористами не являются давайте будем обозначать помощь метки равной нулю от триста единиц нормальные люди карьеристы идут через рамку металлоискателя и вы начинаете заполнять матрицу ошибок как заполняется числа указанных матрица ошибок а вот например как вычислить число если оно отрицательное элементы матрицы это нужно смотрим это ячейка находится на пересечении следующего столбца следующей строки туда попадают люди у которых истинный класс равен нулю или рамках их детектировать как принадлежащие классу ноль то есть туда нужно написать количество людей которые действительно являются честными и рамка это распознала а вместо числа п нужно писать количество террористов которые распознала рам то есть количество людей которые действительно просили что то запрещенное рамка их распознала а вот вместо чисел ибп нужно писать на самом деле количество ошибок вашей рамки а именно вместо числа фэн нужно писать количество террористов которые рамка пропустила не сумела детектировать их как террористов а вместо числа вправо должны писать количество честных людей которых рамка наоборот детектировать как террористов и таким образом у вас возникает матрица две которые полный все четыре ячейки с помощью этих чисел интуитивно понятно что чем больше число стоящее на диагонали то есть темп тем лучше в идеале нам хотелось бы чтобы число ф н ф вообще были равно нулю но на практике такого достичь очень сложно делов том что борьба с одним из чисел нлп часто приводит к увеличению другого числа то есть например если мы хотим бороться с числа мэн то есть мы хотим чтобы мамка не пропускала через себя террористов принимая их за нормальных людей это очень действительно важная проблема то есть мы не хотим чтобы ранка пропускала именно вот плохих людей что мы должны этого делать технической точки зрения мы должны увеличивать чувствительность сумки мы должны стремиться к тому чтобы наша ранка могла реагировать на более и более изощренные предметы которые скрывают своей одежды террористы но но из за того что мы увеличиваем чувствительность устройства наша рамка начинает пищать буквально из каждого мы говорили стандартного предметы например она может начать печать от каждой ну металлическая пуговица например на одежде честного человека и таким образом увеличивая чувствительность нашей рамки террористам то есть боюсь числом см мы автоматически увеличивает число офп то есть поскольку у нас рамка становится очень чувствительной то она начинает пищать и на очень большом числе нормальных людей то есть допустим на тех людях которых пуговицы металлические надежде правильно его там мы видим что борьба с одним из чисел в данном случае это приводит к увеличению числа ф п",
        "rating": 1,
        "url": "https://stepik.org/lesson/83208/step/2?unit=59844"
      },
      {
        "lesson_name": "План решения задачи классификации",
        "type": "video",
        "step_id": 590323,
        "lesson_id": 83208,
        "content": "а я думаю вы поняли что все не так однозначно не все так просто но тем не менее нужно как то замерять с помощью умных форму и чисел качества классификации то есть нужно как то замерять много ли или мало чисел и вперед то есть тех случаев на которых наши модели ошибается какие для этого существует умный формулы но самая простая умная форма это общая точность и ей нужно почитать следующим образом вы должны взять сумму чисел на диагонали вашей матрице то есть темп то есть те случаи когда модель не ошиблась и поделить на сумму всех чисел вашей матрицы и у вас получится число равная доля правильных ответов выдавать самых вашим устройством однако надо понимать что само по себе высокое значение общей точности ещё ничего не говорит об адекватности работы алгоритма модели классификации а вот давайте посмотрим на следующий пример у вас есть некая модель классификации ну может представлять что эта рамка металлодетектора и она детектирует террорист человек или нет и у вас возникло а после прохождения группы людей через эту рамку вот следующая матрица ошибок а давайте посчитаем общая точность для этого мы берем сумму чисел на диагонали то есть это девять тысяч девятьсот девяносто плюс ноль который стоит внешнем углу и делим на сумму всех чисел этой матрицы и мы получаем очень на самом деле большое число ноль целых две сто девяносто девять тысячных но фактически фактически то что общая точность этого классификатора большая ничего не говорит об адекватности его работы делов том что этот классификатор он вообще говоря бесполезен почему а потому что он не находит нет ни одного объекта из класса один то есть он все объекты детектируют как объекта из класса мной применительно как рамки металлодетектора это означает что рамка металлодетектора всех людей считает честными и поля смешными естественно от такого классификатора толку нет особенно общая точность неадекватно описывает работу классификатора когда у вас существует сильный дисбаланс между классами объектов класса ноль сильно больше чем объектов класса один ну например как случай честных людей и террористов и как я уже сказал это рамка металлодетектора может работать неадекватно имеет тем не менее слишком высокую общую точность как я уже сказал что характери река называемое общее точностью может неадекватно описывать качество работы классификатора поэтому вводятся дополнительные характеристики отвечающие за точность классификации к наиболее популярным таким характеристикам относятся точность и полнота давайте посмотрим как они вычисляются чем их смысл точность вычисляется по формуле приведенной на складе для этого вы должны взять число т п то есть количество объектов класса один классифицируемых правильным и поделите на сумму постра я и вы получите число от нуля до единицы смысл которого следующий этот доля объектов которые лежат вклассе один среди объектов классифицированных как класс один применительно задачи ооо рамки металлодетектора этом доля террористов среди людей которые были идентифицированы как террористы следующая характеристика эта полнота она считается очень похожа на форму для точность но в отличие от точности набрать сумму по столбцу выберите число п дети на сумму чисел столбце которому принадлежит число топ смысл палаты следующий давайте сразу скажу этот смысл применительно к задаче о рамки металлодетектора так вот смысл пол золотые примирительных задачи оо рамки металлодетекторов заключается в следующем эта доля грубо говоря пойманных террористов это то есть эта доля туристов которых рамках ловит некоторых она пропускает но если вы правильно посчитайте полноту пойми отец какой процент террористов рамка все таки детектируется естественно желательно чтобы и точность и полнота были как можно ближе к единице этому что называется эта задача максимум на этом складе просто показывала примеру отчисления точности и полноты по формулам которые были проведены на предыдущем складе точность здесь вычисляется так нужно взять единиц этого число в нижнем правом углу и поделить на сумму по строке сумме по строке будет одиннадцать элементов таким образом мы зенита делим одиннадцать получаем примерно ноль целых девять сотых а полнота считается как это же самое число делить на сумму постояльцу и мы получаем что полнота равна единице возникает следующая проблема не слишком многовато характеристик отличающих заработал классификатора мы уже получили у вас есть общая точность давайте пока мы вообще оставим главное что у нас есть две важнейшие характеристики как точность престижем и полнота прикол эти два числа отвечают за качество фото классификатора а теперь возникает вопрос а можем ли мы имея две различных классификатора понять какой из них лучше дела чем может так случиться что у первого классификатора точность больше чем у второго но полнота у первого меньше чем у второго и возникает вопрос а какое из двух классификаторов лучше какой выбрать качестве окончательного для этого существует полная формула связано с вычислением это значение фльюь эта форма позволяет как бы купить значение точности и полноты воду формулы получить вот одно число которое является мерой качество вашего классификатора м значение вычисляется по формуле приведенной на нашем складе для классификатора который был на предыдущем флаги самом конце нашего сайта который вы видите проведено правила как нужно вычислять значения для этого классификатора давайте расскажу про смысл значения у вас есть полнота и точность и вы полной формой получаете некое число которое называется обозначением правило такое чем больше это значение тем лучше и это позволяет вам сравнивать несколько классификаторов друг с другом помните я начал с истории о том что возникают ситуации когда очень сложно сравнить два классификатора с разными точности и полноты нами но это значение позволяет сделать вы точность и полноту одному классификатора переводите значение точности пол второго секатор тоже переводите взначении просто сравнивать эти два числа у какого классификатора значение будет лучше тот и победил тот и является самым лучшим классификатором какие выводы можно сделать из нашей лекции на этой лекции мы познакомились с величинами которые отвечают за точность классификации то есть вы теперь можете вычислять эти характеристики понимать какое из классификаторов лучше другого естественно среди этих характеристик наиболее важными являются точность и полнота и нужно жениться к тому чтобы их значения были как можно ближе к единице",
        "rating": 1,
        "url": "https://stepik.org/lesson/83208/step/3?unit=59844"
      }
    ],
    [
      {
        "lesson_name": "Метод k ближайших соседей (kNN)",
        "type": "video",
        "step_id": 590325,
        "lesson_id": 83209,
        "content": "ну что ж а теперь настало время поговорить о каком нибудь конкретном алгоритмы классификации а именно мы сейчас поговорим об одном из простейших алгоритмов классификации комета и ближайших соседей какая основная суть алгоритма этот алгоритм метрический то есть он использует вычисления метрики между вашими объектами ну что ж давайте посмотрим как можно применить методику для задачи классификации во первых метрических методах я это неоднократно говорил на прошлых лекциях объекты представляются в виде точек пространстве а на рисунке вы видите представление объектов виде точек на плоскости это происходит тогда когда каждый объект характеризуется двумя нецелевыми признаками один признак вы откладываете по одной оси другой признак откладывайте по другой оси допустим что вы сумели как то представить объекты виде точек некотором пространстве что происходит дальше дальше вы должны считать метрику между ними и я уже несколько раз упоминал потом что прежде чем читать метку вы должны проанализировать все ваши признаки то есть привести их к одному масштаба задачи классификации нужно нормировать не абсолютно вот все признаки а все остальные признаки предсказываемых приз а ты их оставьте спокойно нормировать не надо теперь что нужно сделать дальше а можно выбрать значение параметра ка ка это входной параметры вашего алгоритма от него зависит вся дальнейшая работа что происходит дальше дальше происходит следующее ок у вас есть тренировочная выборка и теперь этой тренировочной выборки попадает новый объект а для этого нового объекта а находится как его ближайших соседей соседей по тренировочным выборки то есть вы использ некую метрику ну действительно считаете расстояние от объекта а до всех остальных объектов тренировочной выборки находите к самых ближайших объектов а теперь собственно правила классификации объект относится к тому класс который является наиболее распространенным среди его вот этих вот как ближайших соседей то есть если среди его соседей преобладали объекты из класса ноль три объекта будет отнесен классу ноль если среди соседей преобладали объекты из класса один три объекта тоже будет классифицирован как объект из класса один на этой картинке вы можете видеть результаты работы алгоритма к н к ближайших соседей на самом деле его работа существенно зависит от значения входного параметра к давайте посмотрим что здесь происходит у вас есть объекты двух типов здесь метки классов объектов обозначенными ноль один два виде цвета и формы у вас есть класс состоящий из синих квадратиков и клад состоящий из красных треугольников а теперь эту выборку попадает новый объект он здесь обозначен зеленым кружочками нужно понять от какому классу квадратиков или треугольников он принадлежит давайте запустим метод кайнан для к равного три для этого нужно найти трех ближайших соседа нашего зеленого дочка на рисунке вы видите что среди трех его ближайших соседей преобладают треугольнике и поэтому при к равном три зеленые кружочек будет классифицирован как треугольник однако если бы вы запустили бы алгоритм канин для карловна пт и нашли его пять ближайших соседей то увидите что среди пяти ближайших соседей преобладает уже квадратике и зеленые кружочек будет уже классифицирован как квадратик причем нужно иметь ввиду что если вы запускаете к нг отчетного как то возможна ситуация когда результат классификации будет не определен это возникнет в том случае когда среди его ближайших к соседей представителей первого и второго класса будет поровну в этом случае результат фикации может быть неопределенным а что фактически происходит с нашим пространством при работе метода карин оказывается что наше пространство на самом деле разбивается на зоны итак давайте посмотрим что мы видим на левом рисунке мы видим раз положение объектов тренировочный выборки принадлежащих красному и синему классу также зафиксировано число карту есть входной параметр алгоритм окаянным так вот оказывается после того как вы выбрали число как происходит изменение всего ваш вопрос франсуа озона на красную синюю зону что означает сезона зона означает следующее что если новый объект попадет в красную зону то он будет классифицирован как красный если он попадет все не зону то он будет классифицирован как синий объект если вы начнете планировать число как то расположение зон тоже может меняться естественно нужно иметь ввиду что существуют так называемые ну выбросы вашей выборки то есть возможно существование синих объектов тренировочного баки попавших красную зону и красных объектов тренировочный выборки попавших сезонов такие вот выбросы то есть объекты которые лежат не там где они должны лежать вы можете видеть на нашем складе давайте подумаем а какие лечение качестве как можно выбирать давайте возьмём самый так сказать экстремальные значения минимально возможное число как оно равно единице в этом случае вы ищете лишь самого ближайшего соседа и вот какой класс у самого самого ближайшего соседа кто то будет определен для нового объекта для которого вы предсказываете метку класса можно взять другое экстремальное значение оно равно объему тренировочной выборки и в том случае когда к вам приходит новый объект для классификации вы считаете расстояние до всех объектов тренировочный выборки и для всей вот этой вот выборки смотрите а какой класс ней преобладает и таким образом когда карла такого очень большому числу совпадающим с объем тренировочной выборки алгоритм камеру он становится некотором смысле параноидальным то есть он начинает для всех объектов для всех новых объектов выдавать одно и то же число то есть он выдает метку класса который вашей выборки обладает естественно слишком маленькое число значение карт или слишком большое значение для они не рациональными оптимальный очевидно наиболее оптимальное значение для колец где то между единицей и объёма все тренировочные как правильно настроить оптимальное значение для ка я скажу одно из следующих лекций посвященных кросс валидации я думаю наши слушатели смогут предложить разные модификации алгоритма к нам потому что он очень простой для понимания и каждый может его оптимизировать все больше и больше например вы можете поиграться с выбором метрики естественно выбирая различные метрики для вычисления расстояния вы будете получать различные результаты и поэтому исследователи может выбрать метку которая бы используется для вычисления расстояния между вашими объектами во вторых сейчас вы можете немножко подумать какие модификация такая вы можете предложить и второе предложение соседей можно самом деле взвесить пропорционально мере были то есть обратно пропорциональна расстоянию и таким образом решение будет приниматься на основании просто подсчета числа соседей из класса но эти числа соседних класс один а с помощью подсчета именно взвешенных шум и вот какая взвешенная сумма побери к тому классу и будет отнесен новый объект а естественного метода конечно есть проблемы и немалые во первых алгоритм каен он очень неустойчив к выбросам почему это происходит отделов том что результате вот алгоритм аккаунт пространство разбивается на зоны и если возникает выброс то есть объект лежащий очень далеко в зоне другого класса то вокруг него может образоваться область попадает которую новые объекты классифицироваться как объекты имеющие тот же класс три класс выбросов поэтому с выбросами конечно нужно бороться прежде чем запускать метод кайдзен вторая проблема алгоритм как она работает очень плохо когда объекты имеют очень очень много признаков этот эффект имеет объяснение от теории вероятности ну там можно сослаться мало используемые слова но закон больших чисел а также когда объект у вас имя слишком много признаков и вы применяете к ним метрический алгоритм например колдунство возникает такой интересный эффект как проклятие размерности про проклятие размерности я скажу на одной из своих последних лекций на самом деле алгоритм как она очень простой и не всякую задачу можно решить с помощью алгоритма каин дают как правило так и не вступают об этом канале тем не менее используют его используют для вычисления некоторого промежуточного ответа то есть как некий место алгоритм что значит место алгоритм итак представьте вам нужно решить очень очень сложную задачу если вы запустите алгоритм какая нам то качество работы вашей модели будет на самом деле низкий но можно сделать следующие можно запустить алгоритм карен и заполнить те ответы которые она вам выдает для ваших объектов столбец с этими ответами вы можете добавить качестве нового столбца вашу таблицу и уже расширенную таблицу подать на вход более изощренному методы например коньки нейронные сети и как правило врезультате вот промежуточного применения алгоритма кайнан качества классификации может уже пора прости какой вывод следует из нашей лекции вывод простой на этой лекции мы изучили работу алгоритма к иным",
        "rating": 1,
        "url": "https://stepik.org/lesson/83209/step/1?unit=59845"
      }
    ],
    [
      {
        "lesson_name": "Методы выбора оптимальных параметров алгоритма. Кросс-валидация",
        "type": "video",
        "step_id": 590326,
        "lesson_id": 83210,
        "content": "а сейчас самое время поговорить а такой важной процедуры как процедура выбора оптимальных значений параметров для алгоритмов предсказания и в частности поговорить а такое очень важной процедуры как кровь валидации я уже на многих предыдущих лекциях говорил что очень часто машинного обучения алгоритм предсказания имеет некий параметр который задается человеком этот параметр определяет работал горит например для алгоритма карен таким параметрам является число как то есть число соседей которые так сказать учитываются при классификации и возникает вопрос а как оптимальным способом настроить значение вот этого параметра как это сделать на одной из предыдущих лекциях я показывал ровной более простой и наиболее тупой способ выбора оптимального значения параметра а это было на лекции посвященной реализации и ласок чем заключался этот самый самый простой способ выбора оптимального значения параметров нужно было взять как можно больше различных значения этого параметра для каждого из этих значений нужно запустить модель предсказания и посчитать и е качество на тестовой выборке и окончательно выбрать то значение параметра на котором достигается максимум качества классификация регрессии естественно этот очень простой способ имеет недостатки например такой поскольку вы перед началом работы зафиксировали разбиение на тестовые тренировочные выборки то получается что выбор ваш параметра он не явно используют разбиения на тестовые тренировочные баку которой у вас возникла таким образом выбор вашего параметров действительно зависит от конкретного разбиения на выборке поэтому нужно как то избежать вот этого негативного и предложить более общую процедуру выбора оптимального значения параметров рассмотрим процедуру валидации ещё называют скользящем контролем что нужно для этого сделать нужно поступить очень хитро не разбивать просто на тестовый тренировочный выборки разбить на большее число частей итак на этом слайде вы видите что все ваши данные все ваши данные были разбиты на качестве для нашего рисунка к равно пяти итак вы развиваете ваши данные на карту обычно берут равных частей но к равных частей что происходит дальше как вы начинаете вычислять а дело вот чем когда вы стоите то модель она обучается к раз на разных вот под выборках исходной выбор а и е качество будет заменяться на оставшемся на оставшейся части выборки таким образом вы запускаете вашей модели несколько раз причем каждый раз используйте различную тренировочную и различную тестовой выборке при чем каждая модель может быть запущена как раз потому что каждый раз у вас тренировочный выбор как бы меняется она каждый раз тестовой выборка становится одним из кусков таким образом возникает к оценок качества моделей в теко оценок качеством да не можете соединить и усредненное значение вызов качестве окончательного показателя качества вашей модели таким образом можно сформулировать следующую схему выбора оптимального значения параметра при с помощью процедуры с валидации и так как раньше мы берем как можно больше различных значений нашего параметра п для каждого значение этого параметра п нужно построить к моделей потом запустить процедуру кросс валидации у вас получится а значений показателей качества вашей модели при фиксированных значений параметров а потом вы показатели качества усредняется по к моделям и окончательно нужно выбрать такое значение параметра при котором усредненная пока эти качества моделей будет наибольшим итак какой вывод следует из нашей лекции оказывается существует очень мощная процедура выбора оптимальных значений параметров для алгоритмов предсказания эта процедура называется процедурой росавиации",
        "rating": 1,
        "url": "https://stepik.org/lesson/83210/step/1?unit=59846"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310284,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/1?unit=59880"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310285,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/2?unit=59880"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536214,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/3?unit=59880"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536219,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/4?unit=59880"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536218,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/5?unit=59880"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536220,
        "lesson_id": 83245,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83245/step/6?unit=59880"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 537009,
        "lesson_id": 190154,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190154/step/1?unit=164659"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Основные понятия",
        "type": "text",
        "step_id": 626873,
        "lesson_id": 83212,
        "content": "В этом разделе мы изучим, пожалуй, один из самых наглядных методов классификации и регрессии - деревья. Я надеюсь, мне удастся убедить вас в том, насколько это универсальный и мощный метод МО. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/ZQZQX",
        "rating": 1,
        "url": "https://stepik.org/lesson/83212/step/1?unit=59848"
      },
      {
        "lesson_name": "Основные понятия",
        "type": "video",
        "step_id": 594677,
        "lesson_id": 83212,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83212/step/2?unit=59848"
      }
    ],
    [
      {
        "lesson_name": "Неопределенность Джини. Построение дерева с его помощью",
        "type": "video",
        "step_id": 594678,
        "lesson_id": 83213,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83213/step/1?unit=59849"
      }
    ],
    [
      {
        "lesson_name": "Поиск выбросов с помощью дерева (изолирующий лес)",
        "type": "video",
        "step_id": 594679,
        "lesson_id": 83215,
        "content": "она эта лекция я исполнил свое давнее обещание втеме выброса я говорила о том что с помощью деревьев можно эти самые выбросы искать и вот как раз вот сегодня мы и поговорим об этом как с помощью деревьев можно найти выбросы вашим множестве данных давайте для начала посмотрим что значит классификация с помощью дерева с геометрической точки зрения итак я очень часто рисую объекты с помощью точек на плоской действительно это можно сделать когда вас объект имеет два нецелевых признаках один признак мы откладываем по горизонтали другой по вертикали на пересечении ставим точку а вот класс объекта то есть целевой признак или как он соответствует цвету нашей точки вот на этом рисунке вы видите что часть объектов принадлежит красному классу другая часть объектов принадлежит желтому классу так вот что делает дерева с геометрической точки зрения а вот дерево на самом деле раз бывает пространство на зоны на красную зону и на желтую зону чем смысл сезон а смысл простой что когда к вам приходят новые абсолютно объекта для классификации то он попадает какую то из зон и ответом будет как раз для него свет этой зоны что самое тут существенно оказывается если вспомните определение дерева и немножко так сказать геометрически это все провернуть то можно понять что дерево разбивает пространство на зоны прямоугольного вида не на кружочки не на вальщики именно у вас должна получиться разделение на зоны именно вот такой вот прямоугольного вида то есть каждая зона состоит из нескольких прямоугольников вот это вы видите например на экране и что ещё нужно иметь ввиду что деле естественно пытается построить зоны так чтобы вк красную зону попало как можно больше красных объектов желтую зону попало как можно больше жилых объектов из тренировочной выборки естественно это не всегда удается вы можете видеть на рисунке что часть объектов красного класса из тренировочных выборки попало желтую зону точно не всегда удается вот точности отделить наши классы на тренировочный выборке но тем не менее модель наших пытается пытается вот именно построить оптимальное разделение на такие зоны а теперь представим противоположную ситуацию наше дерево не будет вообще ничего стараться она будет эти зоны рисовать что называется произвольным образом от балды грубо говоря то есть она наша вера представьте будет эти самые прямоугольнички проводить валютная произвольных вот местах то есть возникнет такой вот некий хаос теперь давайте введем такое определение мы будем говорить что объект изолироваться если модель построила прямоугольник который попал только этот объект он лежит в этом прямоугольнике других у него соседей по этому прямоугольнику нету так вот возникает вопрос какие объекты проще всего изолировать привод случайно вот рисования прямоугольников оказывается проще всего изолировать объекты которые лежат на периферии вдалеке от основной куча обьектов то есть проще всего изолировать выбросы и на этот раз основана идея поиска выбросов с помощью деревьев действительно давайте посмотрим на примере может смотри вот если то лежит где то на периферии далеко от общей кучи то их очень легко провести прямоугольник который это точка попадет и другие точки у нее не попадут а теперь наоборот если точка лежит смотрите самые будущее то действительно очень сложно вот провести прямоугольник тем более как я уже сказал прямоугольник проводится случайным образом что этот прямоугольник попадет только эта точка другие не попадут таким образом проще с помощью прямоугольников изолировать те объекты которые лежат вот на периферии то есть выбросы как вот на этом основан формального видом поиска выбросов с помощью деревьев он называется изолирующий с давайте я приведу его формальное описание итак нужно построить н деревьев то есть у нас будет ни одно дерево а несколько штук поэтому наш алгоритм называется изолирующий вес то есть лес это предмет который состоит из нескольких деревьев так вот вам нужно построить ни одно н деревьев как они строится как среди расстроиться до исчерпания выборки это означает что каждом дереве строится случайная образом вот этим самым прямоугольники все это вот развивается на прямоугольнике так далее так далее до тех пор пока каждый объект не изолируются каком то прямоугольники а после этого считается так сказать мера нормальности каждого объекта из вашей выборки а именно мера нормальности это среднее арифметическое от глубины естественно который этот объект изолировал я оказывается что логика этого алгоритма очень достаточно простая что вот при случайном способы построения дерева то есть при случайном вот построения этих прямоугольников выбросы изолируются гораздо раньше чем нормальные объекты то есть выбросы будут попадать вместе на ранних этапах на небольшой глубине нашего дерева то есть для изоляции выбросов требуется меньшее число усилий то есть вот на этом основана работа нашего алгорит ну что ж какие выводы следуют из нашей лекции ока называется с помощью деревьев можно искать выбросы этим занимаются такая модификация деревьев как изолирующий вес",
        "rating": 1,
        "url": "https://stepik.org/lesson/83215/step/1?unit=59851"
      }
    ],
    [
      {
        "lesson_name": "Случайный лес (Random forest)",
        "type": "video",
        "step_id": 594680,
        "lesson_id": 83216,
        "content": "а сейчас я расскажу об одном популярном алгоритм предсказания которые используют так сказать идею построения деревьев это случайно лес рядом форст ну давайте подумаем зачем вообще нужно создавать какой то лес оказывается лес то есть несколько деревьев усиливают предсказательные способности каждого из деревьев по отдельности и так а что нужно для этого сделать мы умеем строить вот одно дерево которое решает задачи классификации ну или версии а теперь давайте построим несколько деревьев и что произойдет дальше у нас есть несколько деревьев и мы хотим предсказать значение целевого признака для какого то нового объекта на этот новый объект пропускаем через все деревья допустим часть деревьев предскажет что этот объект с классом а а другая часть деревьев предскажет что этот объект из класса один а какой ответ взять качестве окончательного нужно просто посчитать количество деревьев которые решили что этот объект москва самолете которые решили что из класса один нове побеждает та группа которая деревьев оказалось больше здесь приведена схема работает случайного леса мы видим что объект для классификации пропускается через все деревья и каждое дерево дает свой ответ после этого происходит голосование по большинству как я уже сказал и результат голосования определяет метку класса которому конце концов будет отнесен наш объект но теперь возникает вопрос а как пол тренировочные выборки построить несколько деревьев делов том что на предыдущих лекциях я рассказывал способ как с помощью тренировок выборки построить одно дерево а тут видите ли нам нужно построить несколько деревьев и во первых мы должны друг друга отличаться и во вторых они должны работать более менее адекватно вот это первое второе требование они несколько противоречат друг другу но тем не менее нужно попытаться это построить построить эти самые деревья как это делается это осуществляется с помощью процедура рандомизации во первых при построении каждого из деревьев леса мы используем случайную под выборку нашей тренировочной выборки то есть разные деревья строится с помощью разных множество объектов итак мы одно дерево строить помощью вот одних объектов другой с помощью других и так далее и поэтому деревья возникают разные во вторых мы используем при построении каждого дерева не фиксированная множество признаков то есть не все на они целевые признаки которые если наличие а лишь случайная подмножество наших нецелевых признаков это дает дополнительную организацию поэтому деревья которые у нас получается они будут не похожи друг на друга итак таким вот образом работает модель под названием случайно лес эта модель действительно очень мощная она обладает наибольшей предсказывающие силой среди так сказать всех моделей машинного обучения за исключением от глубоких нейронных сетей она этой лекции мы научились как самый случайно лес можно построить как из единичных деревьев можно построить целый лес и как оказалось что точность предсказания леса оно существенно выше чем точность предсказания каждого из деревьев из которых этот лес состоит",
        "rating": 1,
        "url": "https://stepik.org/lesson/83216/step/1?unit=59852"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 310289,
        "lesson_id": 83246,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83246/step/1?unit=59881"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536221,
        "lesson_id": 83246,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83246/step/2?unit=59881"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536222,
        "lesson_id": 83246,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83246/step/3?unit=59881"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536224,
        "lesson_id": 83246,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83246/step/4?unit=59881"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536261,
        "lesson_id": 83246,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83246/step/5?unit=59881"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 537051,
        "lesson_id": 190155,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190155/step/1?unit=164660"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Принцип классификации. Общая схема построения ЛК",
        "type": "text",
        "step_id": 626874,
        "lesson_id": 83218,
        "content": "На одной из прошлых лекций мы изучали линейную модель для решения задачи регрессии. Однако линейные модели применяются и при решении задачи классификации. Более того, на основе линейных классификаторов строятся такие мощные модели МО как нейронные сети. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/VzGzk",
        "rating": 1,
        "url": "https://stepik.org/lesson/83218/step/1?unit=59854"
      },
      {
        "lesson_name": "Принцип классификации. Общая схема построения ЛК",
        "type": "video",
        "step_id": 602453,
        "lesson_id": 83218,
        "content": "а сейчас мы начинаем целый цикл лекций посвященных очень важному классу классификаторов так называемым линейным классификатором класс линейных классификаторов он во первых интересен сам по себе как математические объекты а во вторых на основании линейных классификаторов строится более изощренные модели машинного обучения например такие как популярной сейчас нейронные сети так вот давайте изучим что такое линейные классификаторы на самом деле в основе работы линейный классификаторов лежат геометрические идеи оказывается классифицировать объекты можно с помощью таких геометрических объектов как прямые плоскости гиперплоскость в общем случае ну что ж давайте начнём формального определения работы линейного классификатора на этом складе конечно никакой геометрии пока нет она появится потом формальные правила как линейный классификатор классифицируют объекты оно выглядит достаточно очень так страшно вот смотрите что тут у нас на складе написал итак если объект у которого есть числовые признаки это один из моих стен и выполняется вот такой вот неравенство то назначается значит нужно каждый признак х один мм до можете на вес то есть некие числа потом все просуммировать добавить свободный член клуба ноль если полученное число больше нуля то объекта будет отнесен классу один отв противном случае если получается отрицательное число то объекта будет отнесен классу минус один ну я думаю сейчас все перфекционисты если они есть среди наших слушателей конечно же завопят что делать если полученное число точности равна нулю но действительно нашем складе есть неравенство строго больше нуля есть минерально строго меньше нуля а что делать если получится восточной стеной ну что ж я должен наших слушателей успокоить очень редко так происходит что удача выбранный объект дает точное равенство нулю после во всех вычислений это будет происходить очень редко но если все равно вы чувств есть некое тревожность давайте я вас успокою окончательно если возникает равенство нулю то будем делать следующие подбрасывая монетку если выпадает орел то окончательно относим объект класса один а если выпадает резком то окон желательно относим объект такой класса минус один так во первых а почему я метки классов обозначают как единица и минус один на предыдущих лекциях я же отметки классов обозначал как единица и ноль но все применены классификаторов просто удобнее класса обозначать именно так единица и минус единице а почему так сделано это будет понятно из дальнейших слайдов это это нужно для того чтобы упростить некоторые формулы которые так у нас очень сложно так вот чтобы они не были слишком уж очень сложными теории линейной классификации класс обозначается как единица минус единице а теперь собственно геометрическая интерпретация на основе которой работает линейный классификатор смотрите линейный классификатор фактически он строит разделяющую плоскости или гиперплоскость между классами объектов вот например на нашем складе показан двумерные линейный классификатор то есть все объекты они характеризуются двумя признаками и представимо виде точек на плоскости а класс объект он показан цветом так вот линейный классификатор должен построить разделяющую примую между классами и все что будет лежать в этой прямой относятся к одному классу а все что выше относится уже другому классу если признаков не цель их признаков уже три штуки то мы переходим вторых мерное пространство и линейный классификатор фактически строит разделяющую плоскость и как происходит классификация все что лежит выше этой плоскости классифицируются как один класс все что лежит ниже этой плоскости классифицируются как то другой класс теперь нам нужно понять как эти самые плоскости гиперплоскость и и разделяющие прямые нужно строить соответствии с определением линейного классификатора которая была на а первом складе нашей лекции нам нужно найти оптимальное значение лесов болевой один дубль две и так далее дубль войной и вот как их нужно находить надо решить такую задачу практическую ну чтож давайте сначала подумал как линейный классификатор может ошибиться оказывается линейный классификатор ошибаются только в двух случаях каких случаях но для этого нужно вспомнить правила классификации оно на нашем складе выделено красным цветом ошибаются линейный классификаторы вдруг случаев первый случай это когда вот полученное число ну которая получается после умножения весов назначения признаков там все это суммируются если это число больше нуля но объект но объектов действительности из класса минус один и второй случай ошибки он не стал некотором смысле двойственных первом случае во втором случае классификатора может ошибиться если вот после всех вычислений получается число меньше нуля но объект вместительности принадлежит классу плюс один итак линейный классификатор могут ошибаться только вот двух указанных на складе случаях что из этого следует из этого следует что эти два случая когда линейный классификатор ошибается можно объединить вадим с помощью введения очень хитрый величины как под стук давайте обозначим через букву м следующее выражение метка класса умноженное на вот то самое выражение которое вычисляется при работе линейного классификатора и вот эта самая величину обозначим через эм это будет отступ объектов такого два типа ошибок которые были на прошлом складе можно записать более компактно а именно линейный классификатор ошибается при классификации объекта а если отступ объекта а отрицательный теперь понятно что нужно делать ошибками линейного классификатора нужно смотреть на каких объектах тренировочный выборки наблюдается отрицательная отступ и минимизировать число таких объя для этого давайте введем вспомогательные обозначение мы будем писать что в квадратных скобках отступа меньше нуля это такое выражение оно равно единице если отступ действительно меньше нуля и нулю если отступ больше нуля или словами выражение квадратных скобках будет равно единице если не классификатор ошибается на текущем объекте или равно нулю если он не ошибается поскольку мы хотим чтобы линейный классификатор ошибался как можно реже то мы должны сделать так чтобы все эти выражения квадратных скобках были бы равны нулю а именно мы должны стремиться к тому чтобы сумма отступов квадратных скобках которая указана ниже на складе была как можно меньше как нужно теперь построить оптимальный классификатор вы должны взять все объекты вашей тренировочной выборки почитать для них подступы и составить вот это выражение виде суммы которая указана внизу нашего слайда это выражение будет зависеть от неизвестных пока значение лесов были один большой и два и так далее что дальше нужно делать этим выражением это выражение нужно минимизировать то есть мы хотим же минимизировать количество ошибок нашего линейного классификатора на тренировочной выборки стало быть вот эту сумму нужно минимизировать но минимизировать это легко сказано возникают проблемы есть такая вот проблема что эта функция дифференцируема а я думаю нашим слушателям понимаю что когда речь идет о минимизации то по любому выглядит понятие производной ди сильно точки минимума как правило ищут с помощью ум явно или неявно участие производных а вот эта функция она не дифференцируема что делать найти точку минимума для нее очень очень сложно с помощью производных это не получится как поступать на практике на практике делают следующие берут дифференцируемой функцию ф дифференцируемой функции это значит она обладает уже производной эта функция должна быть но приближенно равна выражению отступ меньше нуля квадрат в скобках приближенно и также крайне желательно чтобы эта функция модерировал нашу функцию отступ меньше нуля зачем мы требуем модерирования делов том что мы будем функцию ф тоже минимизировать и если мы смогли мне делать функцию это то автоматически будет минимизировано и наши вот эта функция отступ меньше нуля потому что функция и е как бы моделирует она больше не во всех точках аргумента на этом слайде показаны графики нескольких функций которые можно выбрать качестве моделирующих что мы видим на этом графике у вас есть функция отступ меньше нуля квадратных скобках график этой функции на рисунке показан чем она зеленая линии вот вы видите она идет такой ступенькой сначала это ступенька на кстати один а потом это ступенькам спускается на высоту ноль так вот график этой функции можно моделировать сверху с помощью других уже дифференцируемых функций например таких функций указанных ниже нашего рисунка как видите таких функций очень очень много они очень такие разнообразные а некоторые из них очень даже страшно видите они содержат логарифмы экспоненты и так далее но тем не менее точки зрения математики эта задача решаема мы функцию наши отступали заменяем на моделирующий дифференцируемой функции а дальше применяя все наши сознание о поиске минимума для дифференцируемых функций то есть мы дальше ищем частные производные можем приравнять к нулю решаем систему уравнений находим оптимальные значения весов дублевые один дубль во дворе думали итак какая общая схема построения линейного классификатора во первых нужно выбрать модерирую функцию ну хотя бы одно из тех которые были на прошлой свадьбе во вторых нужно составить выражение для минимизации нужно во первых грамотно его составить правильно замене ведь некоторые буквы на число его масса должна получиться выражение котором присутствует только неизвестные весах дубль вэ один и так далее добавил ноль далее вы это выражение минимизировать ищи точку минимума и эта точка минимума дает оптим важное значение лесов дублевые их так вот можно говорить общие слова развивать общую теорию но все таки лучше принести один конкретный пример и вот следующей лекции как раз и будет посвящена решению такого примера но пока давайте сделаем выводы из прослушанных лекций итак мы познакомились правилам классификации линейного классификатора мы имели такое важное понятие как понятие отступа и оказалось что найти оптимальные места у линейного классификатора это задач вообще говоря нетривиальная делов том что нужно найти модерирую функцию которая моделирует функцию отступа и точку минимума искать уже для нее а это вообще говоря не всегда бывает простым занятием",
        "rating": 1,
        "url": "https://stepik.org/lesson/83218/step/2?unit=59854"
      }
    ],
    [
      {
        "lesson_name": "Пример построения линейного классификатора",
        "type": "video",
        "step_id": 602454,
        "lesson_id": 83219,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83219/step/1?unit=59855"
      }
    ],
    [
      {
        "lesson_name": "Нахождение минимума функции с помощью градиентного спуска",
        "type": "video",
        "step_id": 602455,
        "lesson_id": 83220,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83220/step/1?unit=59856"
      }
    ],
    [
      {
        "lesson_name": "Трюк с ядром",
        "type": "video",
        "step_id": 602456,
        "lesson_id": 83221,
        "content": "она этой лекции мы вспомним а самое главное проблеме линейных классификаторов самая главная проблема линейный классификаторов это неразделимо сть выборке действительно когда у вас тренировочный выборка устроено очень сложно и объекты разных классов очень перемешаны и они не разделяются прямой либо плоскостью в этом случае линейный классификатор будет давать очень большое число ошибок что нужно делать в этом случае в этом случае применяют канал три трёхе дом он заключается в преобразовании пространство объектов то есть у вас объекты представляются виде точек в некотором пространстве вы можете это пространство как то преобразовать то есть как то покруче дефор мира значит так далее и после этой деформации классы ваших объектов уже становится разделим с помощью той же самой плоскости и тогда линейный классификатор после указанного преобразования пространства объектов становится уже достаточно мощно эта штука он ошибается на гораздо меньшем числе объектов и так только с ядром он позволяет сделать следующие мы преобразованным пространство данных пространство наших объектов мы хотим сделать так чтобы после нашего образование объекты разных классов уже были бы разделимы с помощью прямой и плоскости итак давайте посмотрим на этот слайд здесь приведен пример выборки которые неразделимо линейным действительно вы видите что у нас здесь две классом зеленые и синие причем объекты зеленого класса они сосредоточены в центре нашей области поскольку это пространство двумерная то есть у нас все объекты расположенные на плоскости то линейный классификатор должен построить прямую со следующим свой так что по одну сторону этой прямой будут объекты синего класса по другую сторону прямой должен быть объекты только зеленого класса к сожалению для указанных на складе объектов такую прямую построить нельзя какой вы примую здесь не проводили она будет ошибаться на очень большом количестве объектов то есть строить линейный классификатор для выборки указанный на складе очень очень задача неблагодарная что нужно делать нужно применить преобразование пространства или иными словами сделать трюк со льдом чем заключается итак давайте наши объекты которые представляются виде точек двумерного пространства перенесем трехмерное пространство и трехмерном пространстве эти объекты станут уже линейный разделенными как вы думаете что нужно сделать с этими точками как перенести вторых мерное пространство так чтобы между ними уже можно было бы провести разделяющую плоскость ответ на таком весьма образом метафорическом языке мы идем на этом складе представьте себе что эти точки нанесены на некую на некую такое эластичную пленку пленка на вас лежит на полу и естественно все ваши объекты находятся в плоскости пола что вы делаете дальше вы считаете своими ногами центр этой пленки то есть на зеленые объекты и тянете всеми четырьмя своими руками за четыре конца вашей пленки после этого если у вас есть четыре руки объекты становятся линейный разделенными зеленые объекты они у вас остались под ногами никуда не делись но из за того что вытянули четыре конца пленки синий объекты они поднялись пространство и уже между синим и зеленым класса можно провести разделяющей гиперплоскость это гиперплоскость пройдет на уровне грубо говоря плинтус вашей комнаты в этом и заключается трёхе дом вы выполняете некое преобразования и после этого объекта становится линейный разделяемыми и вы можете построить модель линейного классификатора естественно такие понятия неформальные как пленка тянуть за четыре угла уровень плинтуса эти понятия не математические вы математике просто есть формальные умные формулы которые могут выполнять подобного вида преобразования эти формулы есть соответствующих справочниках итак какие выводы следуют из нашей лекции первая трёх ядром он позволяет преобразовать пространство объектов так что класса становится уже линейный разделяемыми и можно построить линейный классификатор и этот в классификатор будет обладать очень высокой точностью к сожалению нельзя сказать заранее какое из преобразований их для краткости ещё называют ядрами подойдет для вашей конкретной задачи вы должны с помощью перебора решить какое из преобра название то есть ядер для вас является наиболее оптимальным",
        "rating": 1,
        "url": "https://stepik.org/lesson/83221/step/1?unit=59857"
      }
    ],
    [
      {
        "lesson_name": "Метод опорных векторов (support vector machine, SVM)",
        "type": "video",
        "step_id": 602457,
        "lesson_id": 83222,
        "content": "она этой лекции я скажу пару фраз об одном из самых популярных линейных классификаторов этот классификатор называется метод опорных векторов или сокращенно см от английской аббревиатуры на название этого модели итак какой действительно линейный классификатор наиболее популярен давайте дадим определение см на аналитическом формальном языке как вы помните построения линейного классификации сразу включается на одном из этапов выборе моделирующей функции то есть той функции которая моделирует вам функцию отступов я приводил вот такую картинку на одной из ранних лекций у вас есть функция отступов она обозначена здесь чем на зеленом а ведь такой вот ступеньки так вот эту функцию можно моделировать разными способами давайте вкачестве моделирующей функции выберем функцию в она обозначена на нашем сайте с помощью графика синего цвета так вот когда вы функцию доступа моделируйте функции в и далее ищете точку минимума пространстве лесов то на самом деле это и есть метод см метод опорных векторов таково определение метода см на аналитическом языке есть на языке формул и функций как видите определение своём она аналитическом языке достаточно скучно мы взяли какую то функцию моделирующего и то что получится это обзывается сфф оказывается см на гиа металлическом языке имеет очень понятно и естественно интерпретацию чем она заключается и что делает своём так сказать на геометрическом языке итак представьте что у вас есть два класса для простоты будем считать что они линейно разделимы то есть между ними можно так сказать просунуть плоскость или примую которая их разделить на две части а если эти классы неразделимы что на самом деле между ними возникает ни одна разделяющая прямая а целая полоса от полоса максимально возможная ты между этими классами переведена с помощью пунктирных линий на нашем рисунке давайте сначала обычным эту полосу а потом у этой полосы вычислим середину так вот с м и как раз и находит середину разделяющий полосы об этом и заключается геометрический смысл работы алгоритма см метода опорных векторов как видите он очень логичен и понятен но естественно нужно пару слов сказать о том что делает своим когда класса неразделимо линейный но во первых перед запуском алгоритмах можно запустить люксе дома котором была рассказана предыдущей лекции во вторых можно вводить величины штрафов за каждый неправильный классифицированные объект и на основании суммы пап мы будем строить разделяющую плоскость наиболее оптимальным способом почему метод см метод опорных векторов является одним из наиболее распространенных методов линейной классификации на потому что у него есть куча достоинств первое достоинство это быстрота построения модели если почитать умные книги по математике то из них можно извлечь следующую информацию что градиентный спуск для метода своим никогда не зацикливаться и найдет глобальный минимум второе достоинство это хорошая устойчивость к выбросам дело о том что модели своему она строится исключительно по объектам которые лежат вблизи вот твой полосы которая была на предыдущих сайтах а выбросы которые то есть объекты которые где то далеко они фактически построении разделяющей плоскости участ все не принимают и поэтому см очень устойчива к выбросам и третья если опять открыть умные книги там можно заметить такой факт см как модель она не слабее любой двухслойное ровной сети как и выводы следуют из нашей лекции на нашей лекции мы изучили как в общих чертах конечно же работает метод опорных векторов и во вторых мы указали главное достоинство этого метода почему его так любят машинном обучении",
        "rating": 1,
        "url": "https://stepik.org/lesson/83222/step/1?unit=59858"
      }
    ],
    [
      {
        "lesson_name": "Нейронные сети (как композиция линейных классификаторов)",
        "type": "video",
        "step_id": 602458,
        "lesson_id": 83223,
        "content": "на этой лекции мы поговорим о нейронных сетях эти модели машинного обучения является сейчас наиболее популярными и они я думаю слышал любой человек живущий сейчас на земле конечно же в инъекции я не смогу рассказать в деталях как работает нервной сети я расскажу об общих принципах их работы а чтобы получить о нейронных сетях детальное представление для этого нужно конечно же прослушать специализированные курсы как строй пицца нейронная сеть она эта сеть может быть построена с помощью линейных классификаторов мы же не зря их изучали на предыдущих лекциях давайте посмотрим как это делается но сначала нужно понять а что такое искусственный нейрон потому что они ровно сети но жизнь ирана состоит если мы поймем что такое нерон разобраться что такое сеть будет уже гораздо легче что такое искусственная неровность то есть нейрон как математическая модель это такая штука у него есть несколько входных сигналов на нашем рисунке обозначены буквами экз один экз далее экс ан каждый сигнал он наживается на вес дубового одежду были два и так далее дубль в далее все полученные числа суммируются не прибавляется ещё свободный член дубленой и к полученному числу применяется некая линейная функция ф функция называется функции активации и после того как вычислить значение функции активации вы точнее не вы а искусство нерон это значение выдается на выход вот так работает искусственный нерон какие функции белков качестве функции активации наиболее популярная функция активации является седьмой да то есть выражение которое считается по формуле экспонента деленной экспонента плюс один как это седьмой нужно применять при работе с искусственным нероном у вас есть искусственный нейрон у него есть нтв ходов на каждый исхода подано число х эти эксперты дожидаются на соответствующий вид да мы были в питере все это суммируются прибавляется свободный член дубль вэ ноль получается некое число и вот это число нужно подставить всегда иду то есть подставить вместо буквы зад вот выражение на нашем складе и то что уже получилось в итоге является выходным сигналом нашего нейрона для чего нужна седьмой да и вообще для чего нужны эти самые функции активации они нужны для некоторого сглаживания величины сигнала делов том что седьмое она переводит все вещественные функции во врезок ноль один но действительно вы можете видеть на нашем экране график седьмой какое бы вы число вещественное нельзя ли значение седьмое будет в интервале от нуля до единицы таким образом седьмой да на некотором смысле разглаживает либо слишком высокие выходные сигналы неровно либо слишком очень маленькие как работает один искусственный нейрон а теперь давайте несколько искусственных нейронов объединив сеть таким образом что выходной сигнал один их нейронов будет считаться входным сигналом других нейронов и так далее естественно разные нейроны могут иметь разные функции активации на этом сайте указана пример нейронной сети мы видим что она состоит из трех слоев из трех слоев миронов и причем отходы одного слоя являются выходами другого слоя нейронов теперь нужно понимать что теории нейронных сетей делает человек а что настраивается так сказать помощью искусственного интеллекта человека определяет следующие во первых человек определяет топологию сети то есть он должен человек выбрать количество слоев нейронной сети сколько нейронов на каждом слове будет какие у них функции активации как они связаны друг с другом помощью входных выходных сигналов ты должен сделать человек алгоритм он лишь находит оптимальные места то есть те самые добрые и ты которые были приведены на схеме искусственного нейрона то есть эллисона которые дожидаются входные сигналы вот оптимальная дублевые находит алгоритм машинного обучения ну что ж давайте посмотрим на самом простейшем примере как настраивается нейронная сеть у нас есть данные которые приведены таблицы здесь один нецелевой признак х и целевая признак игорь как вы видите это на немецком класса это числовой признак то есть здесь будет решаться задачи регрессии и вся тренировочная выборка для решения этой задачи она вот у вас приведена в таблице на складе давайте эту задачу будем решать помощью нейронной сети которая состоит из трех искусственных нейронов как выглядит эта сеть давайте сделаем так два нейронов которые вы видите слева это первый слой и один нейрон это нейрон который образует второй слой нашей сети и ответом третьего неровные будет окончательный ответ нашей модель поскольку у нас только один нецелевой признаках а то давайте сделаем так каждое из неронов первого слоя будет иметь лишь одно входное значение равное вот этому самому иксу это входной знач женя х будет дождаться на неизвестные пока весах дубль в один один дубль в один две далее внутри каждого нейрона первого слоя будет происходить добавление свободного члена дублевые ноль один боевые ноль две далее каким оно будет применяться функции активации видеосигнал еды а теперь будет происходить следующие у нас есть два выходных сигнала из нейронов первого слоя эти два выходных сигнала будут объединяться встретим нейроне причем эти выходные сигналы будут умножаться на неизвестной пока весса третьего неровно дублевые один три дублевые две три и кроме того третьем сезоне будет предлагаться свободный член дублевые ноль три и уже окончательным ответом будет то что вот вылезет из третьего нейрона таким образом задача модели заключается в нахождении оптимальных значений весов были в их что для этого нужно сделать а для этого нужно так сказать составить вырос жене которая отвечает за выход сигнала из нейронной сети для этого собственно нужно воспользоваться определением искусственных нейронов для каждого из них так сказать выписать аналитическом виде значение выходного сигнала далее выхода сигнал первых двух нейронов подать на вход третьему и выписать то что получается на выходе из третьего нейрона то что получается на выходе из третьего нейрона оно обозначается с помощью буква ф большое это выражение котором нужно найти неизвестно значение лесов что нужно делать дальше нужно составить теперь квадрат ошибки а именно нужно выражение с подставить значение х для каждого из объектов и вычесть значение игры для этого объектов и все это возвести в квадрат например эта операция для объекта дек приводит к появлению вот это выражение которое указано на нашем складе поступает так для каждого объекта тренировочная выборки у вас возникает вот несколько выражений вы должны суммировать а после этого искать точку не ума нахождение точки минимума как раз вам не даст оптимальные значения ваших весов точку минимума для построенного выражение можно найти с помощью различных методов с помощью того же самого градиентного спуска делов том что с помощью час их производных приравниваем их к нулю эту задачу не решить потому что функция активации вы данном случае это седьмое она имеет достаточно сложную производную и решить систему уравнений которая получается когда вы приобрели частные производные здесь у вас уже не получится поэтому для нейронных сетей вычисления оптимальных лесов осуществляются помощью и традиционных процессов например с помощью того же градиентного спуска однако есть и более известные модификации этого самого градиентного спор например алгоритм обратного распространения ошибки алгоритм обратного распространения ошибки это так сказать тот же самый грубо говоря градиентный спуск но он уже так сказать заточен под топологию нейронных сетей на предыдущих слайдах я рассказывала хоть и нейронной сети которая решала задачу регрессии если вам нужно решать задачи классификации то нейронной сети использовать тоже можно например если вам нужно предсказывать метку класса то вы можете поступить следующим образом вы можете на а последнем слове нейронной сети разместить не один нейрон как это было вот примере про регрессию а несколько причем ответом и трава нейрона будет так сказать уверенность моделей том что объект принадлежит и тому классу то есть если вы занимаетесь бинарной классификации то ваш выходной слой должен состоять из двух нейронов и допустим выход первого нейрона будет говорить об уверенности в том что объект принадлежит классу ноль а выход второго мира оно будет говорить о вашей уверенности что объект принадлежит классу один как я уже говорил топологию сети то есть сколько нейронов сколько слоев как они связаны функции активации определяет человек взависимости так сказать код типа патологии возникают различные типы нейронных сетей например есть тип святочных линейных сетей то есть это те нейронные сети которые заточены на распознавания картинок на распознавание изображений они обладают достаточно специфической топологии скоро человечество так сказать подобрал от методом проб и ошибок вторая группа нейронных сетей это рекуррентные нейронные сети их в основном применяют для распознавания или для какой либо работать видео если на самом деле ещё куча других различных чтоб нейронных сетей их очень много и как я уже говорил это не входит в предмет нашего курса мы должны заканчивать на этом слайде наше знакомство с нейронными сетями и формулировать выводы которые следует нашей лекции и так но это не если в общих чертах так сказать на коленке было показано как выглядит искусственная нейронная сеть мы сказали что тренировка искусственной неровности заключается в оптимальном нахождении весов этой нейронной сети так вот эти самые леса верона сети можно найти с помощью алгоритма градиентного спуска или с помощью его модификации например с помощью метода обратного распространения ошибок которые как раз и был разработан специально для нейронных сетей",
        "rating": 1,
        "url": "https://stepik.org/lesson/83223/step/1?unit=59859"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536264,
        "lesson_id": 83972,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83972/step/1?unit=60544"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310299,
        "lesson_id": 83972,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83972/step/2?unit=60544"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536268,
        "lesson_id": 83972,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83972/step/3?unit=60544"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536271,
        "lesson_id": 83972,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83972/step/4?unit=60544"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536290,
        "lesson_id": 83972,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83972/step/5?unit=60544"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 602466,
        "lesson_id": 190156,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190156/step/1?unit=164661"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Что такое вероятностные алгоритмы?",
        "type": "text",
        "step_id": 626875,
        "lesson_id": 83224,
        "content": "Представьте, что вы получили результаты медицинского анализа, а там написано: “Вы страдаете неизлечимой болезнью с вероятностью 10%”. Что делать? Ответ прост: надо прослушать наши лекции, где будет рассказано, откуда берутся модели, выдающие вероятность, и объяснено,  как интерпретировать их результаты. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/RgrgD",
        "rating": 1,
        "url": "https://stepik.org/lesson/83224/step/1?unit=59860"
      },
      {
        "lesson_name": "Что такое вероятностные алгоритмы?",
        "type": "video",
        "step_id": 612267,
        "lesson_id": 83224,
        "content": "она этой лекции мы начинаем целую серию сообщений посвященных вероятностным алгоритм что такое вероятностной алгоритмов машинного обучения оказывается есть алгоритмы которые выдают не метку класса там один или снег а они именно выдают вероятность принадлежности каждом из классов так вот такие алгоритмы мы для краткости будем называть вероятностными вот как они работают вот перед вами табличка с данными мы видим что по двум нецелевым признакам пустое место мы предсказывали пол когда мы запустили вероятностной алгоритм то вместо точных методов классов единичка и нолик мужчина женщина он выдал вероятности принадлежности допустим класс у мужчин то есть например объект акция является нашим алгоритмом как принадлежащие классу мужчин цветностью ноль семьдесят пять или семьдесят пять процентов объект бы оценивается нашим алгоритмом как мужчина с вероятностью ноль один или с вероятностью десять процентов то есть объект скорее всего мужчина объект бы скорее всего женщина вот так работает вероятности алгоритма существует достаточно много алгоритмов такого типа например на следующих лекциях будет рассказана алгоритм наивного боец так называемый кроме того многие из ученые ранее алгоритмы вы можете очень легко переделать под вероятностной например очень легко переделать алгоритм каен чтобы он выдавал уже немецкий классов вероятности принадлежности классно ну давайте рассмотрим как это можно сделать как можно переделать алгоритм камеры вот перед вами рисуночек который демонстрирует работу алгоритма к иным что мы делаем сначала нужно выбрать это число как число соседей давайте выберем число к равной пяти теперь для нового объекта он у нас обозначен зеленым кружочкам мы ходим пять его ближайших соседей соответствии с вашим рисунком печь ближайшими соседями будет три объекта класса квадратик и три объекта класса треугольник классическом каноне окончательное решение окончательный выбор метки классовыми остался по большинству объектов такого класса больше среди ближайших соседей к этому классу относятся тестируемый объект ну данном случае классический камин вам выдаст что активируемый объект зеленые кружочек принадлежит классу квадратиков а теперь давайте немножко модифицированный алгоритм канал чтобы он выдавал вероятности принадлежности к двум нашим классом итак мы видим что среди пяти его ближайших соседей находятся три квадратика и два треугольника таким образом объектов одного класса объектов другого класс это они относятся друг другу как три двум поэтому можно сделать следующие можно сказать что объект зелёный кружочек с вероятностью три пятых принадлежит классу квадратиков с вероятностью две пятых принадлежит класс треугольников вот таким отца очень простеньким способом можно алгоритм который выдает метки классов переделать вероятностный алгоритм примерно так же можно переделать решающие деревья и так предыдущих лекциях решающие деревья яапью снял как алгоритм который выдает метки классов либо единичку либо ролик оказывается их тоже очень легко можно переделать вероятности алгоритм это делается следующим способом вы строите дерева и каждом листе у вас есть информация сколько объектов того или нового класса тренировочный выборки этот лист попало допустим у вас есть ли в этом дереве которое попало допустим пять объектов класса один из тренировочной выборки и три объекта класса ноль из тренировочной выборке естественно оптическом решающим дереве любой тестируемый объект попавшим этот лист он будет отнесен классу один потому что среди тренировочных образцов представителей класса один в этом месте их больше их пять против трёх однако с помощью этих чисел можно легко модифицировать наш алгоритм так чтобы он выдавал вероятности итак вниз нашего дерева попало пять объектов класса один три объекта класса ноль так вот оказывается можно сделать следующие когда ваш новый объект тестируемые попадает этот есть то сѣверо ясностью пять восьмых он будет отнесен классу один отз вероятностью три восьмых он будет отнесен классу ноль восьмерка тут появляется как сумма чисел пять три нам нужно чтобы вероятности принадлежности к классу один класса ноль были все равно единице поэтому мы делим на восемь итак я рассказал как можно классический алгоритм модифицировать так чтобы они выдавали уже вероятность принадлежности классом мы сейчас возникает вопрос а зачем это все нужно делать зачем нужно возиться и придумывать новые алгоритмы которые выдают вам вероятность принадлежности класса зачем это нужно на самом деле существует очень большая польза связанные с применением вероятностных алгоритмов например допустим ваши данные они сначала обрабатывается один алгоритм потом подается на вход другому алгоритму так вот предсказывать вероятность и иногда считается более оптимальным способом чем просто предсказать метку классов почему да потому что вероятности ещё зашита как бы сказать вашему верь ценность том что какой то объект принадлежит определенному классу действительно большая разница когда вы говорите что смеяться девяносто девять процентов объект принадлежит классу один и не совершенно другой случай когда вы говорите что объект принадлежит классу один вероятность уже допустим пятьдесят один процент от совершенно разные ситуации потому что понятие вероятность защита как бы ваша уверенность в правильности классификации вторая причина почему очень часто используют вероятностной алгоритма это возможность некоторым всмысле переложить ответственность на заказчика как это делается например очень часто делается различных медицинских анализов то есть пациенту выдают не конкретный ответ здоров или болен очень часто выдают именно вероятность что ему говорят вы больны какой страшная неизлечимая болезнь приятностью двадцать процентов и что теперь делать человеку тут все зависит от заказчиков данном случае процента как он это число двадцать процентов про интерпретирует если это число про интерпретирует как слишком стой а то он конечно побежит задавать дополнительные анализы оплате дополнительные процедуры и так далее так далее если он по интерпретирует число как слишком маленькая как несущественное средства на этом успокоиться и пойдет дальше это чай пить то есть вот это ещё один аргумент пользу вероятностных алгоритмов естественно есть и другие аргументы в пользу например давайте я приведу очень такой серьёзный аргумент который я услышал беседуя с математиками из одной восточной стороны в восточной стране где то двадцать лет назад были очень сильные религиозные настроения и оказалось что предсказания немецкий класса то есть предсказание наступления или ненаступления какого то события в будущем расценивался очень часто как богохульство почему потому что собственно наступление наступления событий эта приобрести во всевышнего а человек не должен так сказать это брать на себя эту ответственность но обвинение в богохульстве вас автоматически снимут когда вы будете предсказывать вероятность вы предсказываете всего лишь число из интервала ноль один а реализуется вероятность или не реализуется это действительно забота всевышнего и вы здесь никак не посягает на него обязанностей вот такой вот очень интересно нет пользу применения вероятностных алгоритмов ну что ж пойдем дальше здесь на этом складе приведём ещё один пример работы вероятностных алгоритмов это классическая задача о кредитном вскоре то есть о предсказании возврата имя не возврата кредита что делает алгоритм алгоритм для каждого клиента банка выдает вероятность с какой этот человек кредит не вернет после этого клиентов банка можно упорядочить по возрастанию или убыванию вот то вероятности невозврата на нашем складе клиент упорядочены по убыванию вероятности невозврата эти данные подаются управление банка и уже ответственные лица в банке принимают решения а где установить порог выдачи три то есть какую вероятно считать за критическую выше которой человеку кредит давайте совершенно нельзя действительно существуют разные подходы к установлению этого порога все зависит от политики банка например банк может ввести рискованную он может этот порог поднять но с условием что вот этим людям которых вероятность невозврата большая увеличить процент по кредиту например да и таким образом банк рискует тем не менее выдавая кредиты таким людям он может если ему повезет конечно а получить дополнительные деньги за счёт более высокого процента итак какие выводы следуют из нашей коллекции есть алгоритмы предсказания которые выдают немецкий классов вероятности принадлежности классом мы сегодня то деле зачем такие алгоритмы нужны и чем их преимущества",
        "rating": 1,
        "url": "https://stepik.org/lesson/83224/step/2?unit=59860"
      }
    ],
    [
      {
        "lesson_name": "Повторим школьную теорию вероятностей",
        "type": "video",
        "step_id": 612268,
        "lesson_id": 83225,
        "content": "как следует из названия вероятно знал говорит мы должны как то там внутри себя использовать такую науку как теория вероятности так вот давайте сейчас на этой лекции и повторим основные понятия и определения этой науке вот как хорошо вы знаете вероятность давайте начнем с самого простого начнём событий теория вероятности имеет дело событиями с вычислением их вероятности какие бывают события какие события рассматриваются теории вероятности это самое возможно например события моментов при подкидывали выпадет орлом кверху или например на игральном кубике выпадает число шесть то есть события могут быть совершенно разные а как считаете вероятность таких событий самом простейшем случае и мы ограничим для наших лекциях именно таким случаям вероятность события а считается по следующей формуле во первых вы должны написать все элементарные события вашего опыта например когда вы кидаете монетку элементарными событиями будет выпадения орла либо выпадения прежде всего их будет две штуки а когда вы хотите посчитать вероятность события а то вы должны сделать следующее вы должны взять число элементарных исходов при которых наблюдается события а и поделить на число всех элементарных исходов то есть произвести всего лишь одну небольшую операцию деления ну что ж давайте посмотрим как это все работает на примерах итак первый пример это мы кидаем кубик как известно у комика шесть граней поэтому может быть шесть элементарных исходов для краткости я эти исходы абаза начал вот натуральными числами один две три четыре так далее шесть давайте просто посчитаем проверим так сказать свои знания теории вероятности предпочитаем вероятно следующих событий во первых выпадет четное число это считается очень просто нужно посмотреть сколько элементарных выйти удовлетворяют рассматриваемую события а мы видим что событие а наблюдается для трех элементарных исходов это число две четыре шесть таким образом события а наблюдается для трех исходов из шести то есть его вероятность равна три шестым или одна вторая аналогичная считается вероятность и других событий указанных на складе например давайте посчитаем вероятность события что выпадет делители числа шесть как его нужно посчитать нужно во первых найти все эти числа шесть среди наших элементарных исходов это один две три и сама шестерка таким образом элементарных событий удовлетворяющих рассматриваемому событию а будет четыре штуки то есть четыре исхода из шести то есть вероятность события равна четыре шестым или сокращают у дорог мы получаем что её вероятность равна две трети естественно есть события сочи так сказать вынужденной вероятностью например события что выпадет летательные число его вероятно все равно вообще ну да потому что не существует элементарных исходов при которых это событие наблюдается то есть формально если мы хотим посчитать вероятность такого события мы должны ноль поделить на шесть и получить в итоге ноль естественно но есть и противоположные события вероятность которых равна единице например события чтобы полить число меньше десяти она происходит всегда и соответствуют нашей формулы для вычисления вероятности мы должны шесть из как поделить на шесть и получить единицу ну что ж давайте рассмотрим второй пример он немножко более сложный давайте заведем двоих детей естественно такой эксперимент может вам они добавят познания теории вероятностей ну по крайней мере материнский капитал вы заработаете это завален двоих детей какие существуют элементарные события здесь всего оказывается четыре исхода они связаны с полом детей например у вас может быть два мальчика может быть старший мальчика младшая девочка может быть наоборот старшая девочка младший мальчик может быть две девочки всего четыре элементарных исходов теперь соответствует нашими исходами вы можете считать вероятности более сложных событий например вероятность события что дети будут разного пола равна следующему число сколько элементарных слов не удовлетворяет ему удовлетворяют две элементарных исходов они вот центре написаны складе а всего исходов четыре таким образом вероятность этого события равна дня четвертых то есть одна вторая ну давайте я последующие события пропущен давайте посчитаем какова вероятность события что число девочек не равно числу мальчиков что это за элементарной исхода какие элементарные сходные удовлетворяют его претворяет тоже два исхода это когда двое мальчиков и девочек никогда две для девочек и мальчиков и таким образом вероятность этого события равна две четвертых то есть одна вторая",
        "rating": 1,
        "url": "https://stepik.org/lesson/83225/step/1?unit=59861"
      },
      {
        "lesson_name": "Повторим школьную теорию вероятностей",
        "type": "video",
        "step_id": 612269,
        "lesson_id": 83225,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83225/step/2?unit=59861"
      },
      {
        "lesson_name": "Повторим школьную теорию вероятностей",
        "type": "video",
        "step_id": 612270,
        "lesson_id": 83225,
        "content": "но теперь обратимся к более сложным заданием вот на этом складе приведен текст задача эта задача имеет подобные входит во все учебники по высшей математике и я думаю услышали которые эту самую высшую тематику допустим вы изучали я думаю сложится впечатление а причем тут анализ данных и алгоритма предсказания оказывается вот это задача которая у меня на сайте приведена имеет более чем прямое отношение к нашей теме вероятность алгоритм предсказали ну что давайте я озвучил эту задачку итак у вас есть выбор камней шестьдесят процентов женщин и соответственно сорок процентов мужчин известно что среди этих людей курит десять процентов женщин и семьдесят процентов мужчин дальше дальше есть некий человек анонимный н про него известно что он курит так вот какой вероятностью этот человек является женщиной и соответственно мужчины так давайте разбираться какое отношение это все имеет колоритом предсказания а давайте я эту задачу переформулирую используя термины анализа данных термины связанные задачи предсказания итак представьте себе что у вас есть таблица которой эти самые люди так сказать все указаны в этой таблице шестьдесят процентов женщин и сорок процентов мужчин и у каждого объекта из этой таблицы есть нецелевой признак связанные с курением то есть это бинарный признак нецелевой и в этом столбце стоят новики имени в зависимости от того курит человек или не курит оцениваем признаком считается признак пола и таким образом наша задача заключается предсказания пола по факту курения а теперь смотрите нам подается на вход новый объект как бы новая строка в нашей таблице у которого известно значение нецелевого признака связано воскурения этого человека известно что он курит а право у него неизвестен и нам как раз это и надо найти вероятность принадлежности этого человека же кому это мужскому полу он давайте посмотрим как эта задача решать и как его применять анализа данных что эта задача решается очень простыми способами которые излагаются учебник по высшей математике давайте я приведу способ три ранее такой задачи итак первым делом нужно ввести обозначение через букву м мы будем обозначать события что человек является мужчиной через букву ж что человек является женщиной через руках мы будем обозначать события что человек как курить им пользуюсь условия наша задача можно сразу же найти вероятности этих событий тем же а также условных вероятностей капли условия м и к при условии же как они считаются очень просто вероятность события мм что иное как грубо говоря концентрация мужчины нашей выборке мужчину на сорок процентов поэтому вероятность гола ноль четыре концентрация женщин шестьдесят процентов или долях единицы ноль шесть поэтому вероятность того что случайно выбранный человек женщина это но шесть вы наша задача присутствуют и вторая пара чисел семьдесят процентов десять процентов так вот это ничто иное как условной вероятности вероятность того что человек курит при условии что это мужчина это как раз вот те самые семьдесят процентов или ноль семь и вероятность того что человек курит при условии что эта женщина есть процентов или ноль один таким образом возникает вот четыре числа указанных на складе с помощью этих чисел можно оказывается посчитать вероятность того что наудачу выбранный человек будет курить для этого эти числа нужно просуммировать не можете указанным на складе порядке откуда берется эта формула эта формула называется вы учебниках формула полной вероятности но этой формулы я думаю мы можем додуматься сам вот представьте себе что если бы мужчины женщины нашей выборке было бы поровну то тогда чтобы получить вероятность курения на удачи выбранного человека мы должны были бы взять среднее значение между ноль семь и ноль один то есть между курением среди мужчин и курения среди женщин иными словами мы должны были взять число ноль семь ноль один домножить их на одну вторую изложить получить среднее а теперь рассмотрим общий ситуацию вы наши выборки мужчины и женщины распределить не поровну выборка нас перекошенным так вот стало быть мы вот эти самые число то есть вероятность курения каждом поле должны нажать теперь ни на одну вторую не пополам а собственно на концентрат свои мужчин и женщин в нашей выборке и соответствуют нашей формулы мы вероятность курения среди мужчин ноль семь умножаем на концентрацию мужчины нашей выборке точно ноль четыре авиастроения среди женщин умножаем на концентрацию женщин то есть ноль шесть все это раскладываем получаем число три четыре процента однако это ещё не конец нас интересуют не вероятность курение во всей популяции нас интересует общение немножко другие события а именно нас интересует вероятности событий указан внизу слайдов какова вероятность того что это мужчина при условии того что человек ходит на какова вероятность того что эта женщина при условии того что человек курит и возникает вопрос а как вот эти самые вероятности найти зная вероятности событий по счету их верхней части слайда как их найти ну на самом деле вопрос риторический все давно известна как искать есть формула байеса она приведена на нашем складе формула байеса как раз и связывает вычисления условных вероятностей с помощью величенко которые нам уже известно осталось просто взять и тупо подставить все известные числа и мы получаем что вероятность того что человек мужчина при условии того что он курит будет равна дроби двадцать восемь тридцать четвертых а вероятность того что эта женщина при условии что этот человек курит эта вероятность равна шесть тридцать четвертых таким образом эти числа они фактически являются оценками принадлежности к классу мужчин и классу женщин режим это знать очко мы фактически получили оценки принадлежности к классу мужчин и классу женщин для некоего курящего объекта субъекта а что с ними делать дальше то есть у нас снова был не курящий человек имеет для него выдаем вот так невероятности принадлежности его к одному или другому полу то есть у нас получается два числа принадлежит одному пол и принадлежит другому что с этими двумя числами делать дальше чтобы получить окончательный ответ можно сделать следующие например можно выбрать как у вас с наибольшей вероятностью из полученных поскольку вероятность принадлежности класс у мужчин она выше чем принадлежность классу женщин то можно для нашего объекта он так сказать волевым усилием записать его мужчины а второй способ можно провести случайно испытания с указанными вероятностями то есть генерировать случайные чем которая принимает одно значение вероятности двадцать восемь три четыре второе приятностью восемь три четыре и посмотреть результаты случайная генерация вот то что случилось и объявить окончательным так сказать меткой пола этого человека а можно поступить ещё более хитро а можно установить пороговое значение для нашей вероятности которой мы будем означает греческой буквой п смысл не такой если вероятность принадлежности к классу мужчин больше чем пить то объект классифицируются как мужчина а если ниже то женщина обычно вкачестве порога пиво логичнее взять ноль пять одну вторую она пошла лежать посерединке между нулем и единицы но не всегда выбор порога равного ноль пяти является оптимальным делов том что задача бывают разные и если вашей задаче ущерб код ошибки классификации различен для одного и для другого класса топ клубом порода нужно отнестись очень ответственно итак давайте рассмотрим аналогичная ситуация только мы теперь будем предсказывать не мужчин и женщин по факту курения будем рассказывать допустим честных людей и террористов по каким то другим признакам я думал поколению туристов точно не распознает поэтому будем считать что каким то неизвестным признаком мы научились как то предсказывать вероятность того что человек честный и того что он террорист теперь мы думаем дальше так вот представьте себе что для некоторого человека который нас обозначил буквой а были получены следующие вероятностные оценки на принадлежность его классу честных людей и классу террористов вероятность того что он террорист равна шесть тридцать твердый а вероятность того что он честный человек двадцать восемь три четвертых со мной стороны ну как бы все нормально вероятность того что он честный человек четыре с половиной раза выше чем вероятность того что он плохой человек и скорее всего наверно может его нужно будет отнести это классу честных людей конце концов правильно однако нужно понимать цена ошибки что тариста приняли за честного человека цена этой ошибки гораздо выше противоположные ошибки когда честного приняли террористом а вот теперь давайте внимательно смотреть на человека обозначил буквой а мы видим что вероятность принадлежности его классу террористов она уже не такая маленькая она равна примерно восемнадцать процентов это число заведомо лучше доли террористов во всем населении страны правильно и поэтому задача когда определяется честный человек или террорист лучше установить очень низкий порог например равные десяти процентов для людей у которых вероятность принадлежности к классу туристов больше этого низкого порога больше восемьдесят ноль процентов таких людей лучше конечно отправлять на допустим дополнительную проверку таким образом правильная интерпретация результатов работы вероятностного горит но эта штука вообще говоря очень очень нетривиальная нужно как то из вероятности получать окончательный ответ это можно делать либо с помощью простых методов типа округления да которые были на прошлых слайдов а можно устанавливать некие вот порог и вот величина пароль ну это дело достаточно очень и очень тонкая правильно восстановление пароля конечно же зависит от специфики вашей задачи итак какие выводы следуют из нашей лекции нашей лекции мы познакомились а кто то наоборот вспомнил основные понятия теории подожди мы соответственно научились вычислять вероятности событий ну и соответственно установили связь между задачей предсказания и теория вероятности",
        "rating": 1,
        "url": "https://stepik.org/lesson/83225/step/3?unit=59861"
      }
    ],
    [
      {
        "lesson_name": "Наивный Байес",
        "type": "video",
        "step_id": 612273,
        "lesson_id": 83226,
        "content": "она этой лекции как я обещал несколько инъекций ранее мы поговорим об одном из вероятностных алгоритмов то есть об алгоритме наивного бориса пожалуй самый простой вероятности алгоритм давайте самостоятельно придем идеи которая лежит в основе его работы итак давайте рассмотрим такой проблемы принципе как это следует из результатов прошлой лекции мы научились худо бедно предсказывать вероятность принадлежности к классу если у нас лишь один нецелевой это признак вспоминаете на прошлой лекции действительно не целевой признак курения был только один и по нему мы предсказывали целевой признак пол естественно реальных задачах это не так реальных задачах у нас можно быть очень много не целевых признак вот например на этом сайте приведена таблица где у каждого объекта очевидно это люди потому что он как правило люди умеют курить и любит или не любит кошку у каждого объекта у каждого человека здесь два нецелевых однако это курение и любовь кошка и на основании этой информации мы хотим предсказывать значение целевого признаков пола и соответственно возникает вопрос а как тут быть дело в том что если мы будем рассуждать примерно так же как на про ой лекция то есть пытаться читать вероятность каких то событий применять формулы байеса то мы управимся следующую проблему у нас вылезут вот такие вот вероятности вероятность того что человек не курит и не любит кошек при условии того что это мужчина или вероятность того что человек не курит и не любит кошек при условии того что эта женщина вот все упрется в эти вероятности почему именно эти а потому что как бы указана прошлом славена на классификацию сдается новый объект который и не курит не любит кошек нужно понять мужчина это или женщина и все упирается красок вычисления вероятности подобного типа ещё раз повторяю что с помощью звездочки с помощью умножения математике сервера если этот значок означает сокращение союза и возникает вопрос а как считать вероятность события которые так сказать состоит из двух других событий связанных союзом и к счастью есть формула называемая формулой произведения вероятности то есть вероятность появления события можно посчитать как произведение вероятностей этих событий но если плохая новость эта формула работает не всегда она работает только для так называемых независимых событий то есть для событий на давление или наступления одного никак не влияет на наступление и ненаступления другого что нужно сделать а нужно поступить очень просто нужно взять и волевым решением объявить что наши не целевые признаки вдруг ну да действительно не зависит то есть для нашей задачи мы волевым решением говорит курение человека никак не влияет на его любовь к кошкам что это нам дает это нам дает право применить формулу для умножения вероятностей и вероятности который нас появились на прошлом складе то есть например вероятность того что человек не курит не любит кошек при условии того что это мужчина можно расписать как произведение других более простых вероятно то же самое можно сделать аналогичным событиям при условии того что человек женщина а теперь можно собственно приступить к решению нашей задачи то есть получению вероятности принадлежности ввод нового объекта класса мужчины и классу женщин для этого конечно по нашей таблице нужно рассчитать различные впомогательное вероятности может нам по любому понадобится вероятность того что случайно выбранный человек эта женщина эта вероятность равна две пятых откуда это число берётся это число берется из таблицы вот внимательно посмотрите на таблицу вней пять людей то есть грубое опять элементарных исходов но в этой таблице всего лишь две женщины но напоминаю что женщины условно обозначает роликами для краткости мужчин для краткости единичными то есть в таблице две женщины поэтому вероятность того что случайно выбранный человек будет женщиной равна две пятым соответственно вероятность того что случайно выбранный человек будет мужчины это три теперь считаем условной вероятности вероятность того что человек курит при условии того что он женщина как посчитать это ну для этого нужно из таблицы выкинуть все строки которые противоречат события указанному справа от вертикальной черты справа от вертикальной черты у нас указано что человек является женщины поэтому из таблицы нужно мысленно выкинуть всех мужчин останутся только две строчки б и д среди этих строк мы находим долю курящих людей она равна одной второй потому что там остается одна женщина которая не курица одна женщина которая курит соответственно вероятно курение среди таких оставшихся женщин равна одной второй аналогично считается например реальность того что не курит человек при условии того что эта женщина как например посчитать вероятность того что человек курит при условии что он мужчина для этого из таблицы нужно выкинуть те строки которые противоречат событию справа от вертикальной черты справа от вертикальной черты нас написано что человек мужчина поэтому из таблицы нужно выкинуть наоборот женщин вот выкидывая женщин таблица остаются три строчки смотрим сколько срок из оставшихся курит курит мест роки из трёх поэтому вероятность курение при условии что это мужчина равна две трети аналогично считаются вероятности связано с любовью кошками они делаются с помощью таких операций связанных таблицах результатов извини я привожу на складе теперь что делать дальше а теперь нужно применять формулу байеса формула байеса заключается в вычислении следующих вероятностей нам нужно посчитать что вероятность того что эта женщина при условии что человек не курит и не любит кошек можете промотать немножко презентацию назад там действительно было объект который подавался нам на классификацию с неизвестной меткой класса и это будет действительно не курил и не любил кошек как нам нужно посчитать вероятность того что эта женщина и соответственно нужно посчитать вероятность того что это мужчина при условии что вот этот субъект не курит и не любит кошек применяя формулу байеса мы получаем выражение указанная на верхней части нашего слайдов и там как раз возникает это всего сложных событий то есть например там возникает вероятность события не курит и не любит кошек при условии того что эта женщина ну и соответственно мужчина так вот соответствовать нашим волевым усилием мы говорим что эти признаки курения любовь кошка никак друг от друга не зависит и это позволяет нам переписать формула байеса дальше то есть качестве множители для формулы байеса мы записываем теперь следующий вероятности которые указаны нижней части нашего сайта эти вероятности они считаются про нашей таблице они были вычислены на предыдущем складе осталось вообще говоря только подставить вместо вероятности число давайте подставим числа получаем следующее выражение почти все вероятности заменяются на подсчитанные нами число кроме вероятность того что наудачу выбранный человек не курит и не любит кошек на складе вы можете видеть что это вероятность неизвестная пока она лежит знаменательный как первое условная вероятность так и второй и что нужно с ней дело тебе же надо на что то заменить правильно оказывается это вероятность можно явно не вычислять потому что нам собственно не нужна нам нужно в качестве окончательного ответа посчитайте минут теме реальности которые указаны слева от знака равенства части нашего сайта поэтому вероятность котов так возникает у нас вы знаменателях считать не нужно нужно воспользоваться равенством каким а именно что вероятность того что эта женщина при условии что не курит и не любит кошек плюс вероятность того что это мужчина при условии что он не курит не любит кошек она должна быть равна единице потому что это взаимно противоположные события понравится дополнительная мне сумма вероятность равна единице она нам как раз позволяет решить подобную систему уравнений и найти что вероятность того что человек является женщиной пресс условия некурения любви кошкам равна одной трети и аналогично вероятность того что это мужчина при условии некурения или кошка брала две трети давайте разделы последний комментарий к этому слайду как это очень быстро получить ответ у к знаешь последней строке если посмотреть первые две строки с формулами то мы видим что вторая вероятность в два раза больше чем первая но не сильно там во второй строчке стоит двойка а впервые точки стоит единица все остальное не совпадает есть вторая вероятностью два раза должна быть больше чем первая а вот здесь на нашем складе ещё условии что их сумма равна единице так теперь эту задачу решить очень просто можно сказать даже вкусно можно решить у вас есть два числа одно из них два раза больше чем ой а их сумма равна единице ну тут дима очень легко понять что одно из них ровно две трети второе равно одной трети что собственно мы получаем вот внизу слайда ну и таким образом мы получаем что сильно вероятность принадлежности класса мужчина нас два раза выше чем вероятность принадлежности к классу женщин естественно рассмотренный пример был так сказать учебным у нас была небольшая табличка с очень маленьким числом объектов с очень маленьким числом они целевых признаков но когда у вас нецелевых признаков угрозы это больше чем два то все происходит естественно аналогично вы тоже делаете предположения об их независимости получаете ответ по аналогичным формам",
        "rating": 1,
        "url": "https://stepik.org/lesson/83226/step/1?unit=59862"
      },
      {
        "lesson_name": "Наивный Байес",
        "type": "video",
        "step_id": 612272,
        "lesson_id": 83226,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83226/step/2?unit=59862"
      }
    ],
    [
      {
        "lesson_name": "Показатели качества алгоритма, выдающего вероятности",
        "type": "video",
        "step_id": 612276,
        "lesson_id": 83227,
        "content": "она этой лекции мы поговорим о показателях качества алгоритма который выдает вероятности действительно как измерить качество работы такого алгоритма ведь до этого мы встречались алгоритмами которые предсказывают как это число это алгоритм агрессии или метку класс от алгоритмы классификации нового предсказания вероятность у нас пока не было вот как оценить качество работы такого алгоритма давайте думать самый простой способ заключается в сведении алгоритма выдающего вероят нафиг алгоритм который выдает метки класса что для этого нужно сделать у вас есть информация о вероятности который выдал ваш алгоритм для объектов тестовой выборке вы эту вероятность просто берете и округляете так как это сделано в нашей улица нашей таблицы очевидно предсказывался пол человека по росту и по весу и при чем предсказывалось именно вероятность принадлежности к классу мужчин класс мужчин у нас чисто условно кодируются единичными таким образом возник какие числа то есть для объекта от ноль семьдесят пять это вероятность того что он мужчина а для объектов для того чтобы мужчина это всего лишь десять процентов ноль один что мы переделаем дальше эти числа можно взять и округлите таким образом мы получаем что человек а это мужчина а человек б эта женщина таким образом его столбец вероятностями переводим столбец состоящий из новиков единичек этот столбец можно выдать за результаты работы нашего алгоритма и после этого применить к нему все те характеристики которые имеют место быть для алгоритмов классификации то есть можно выяснить характеристики при сниженной коэлью и так далее но вот этот способ округления вероятности и сведения вероятности алгоритма как обычно алгоритмы классификации не всегда является оптимальным это особенно неудобно когда например класса не сбалансированы то есть число объектов одного класса гораздо меньше чем число объектов другого класса или например вторая проблема это когда цена ошибок неправильный классификации существенно различны вспоминаете задачи про террористов и честных людей вней цена ошибки когда в тариста приза честно она гораздо выше противоположной ошибки когда вы честного приняли за террориста и поэтому от рук вот закруглениями вероятности он здесь работать не может и наконец третьих как я уже говорил на прошлых лекциях теории вероятностей классификаторов важно такое понятие как порог порог округления то есть то число выше которого все вероятности округляются до единицы а все что ниже является нуля и вот правильный выбор порога очень сильно зависит от специфики задачи и поэтому скорее всего показатели качества работы вероятно говорит мы должны как то учитывать и величины у порога ну что какие показатели качества используются для вероятностных алгоритмов один из популярных является функция локос и е формула очень страшное приведена здесь на складе какой у неё смысл я не буду говорить от она возникает какой функции правдоподобия так далее почему тут логарифмы нам не думаю неважно просто нужно понять что вот это вот страшное выражение оно адекватно на математическом языке описывает следующие требования а именно модель штрафуются сильнее тогда когда она сильно уверена в правильном ответе вот эту фразу на математическом языке моделируется красоты помощью выражение для функции локос однако логос это единственный показатель как что вы вероятно стал гриппом на следующих слайдах мы опишем не менее популярный показатели качества как рок аук ну что ж давайте пойдем издалека и так но вот допустим вас есть результаты работы алгоритма на тестовой выборке представим себе что он говорит он выдал вероятностной оценки для наших объектов которые указаны в таблице один то есть у нас есть один столбец это вероятностная оценка то есть принадлежность к классу один а рядом стоят истинные метки классов мы же алгоритм прогоняем объектов тестовой выборки поэтому для каждого такого объекта у нас известно его истинное место класса итак у нас есть вероятность приобрести класса один истинный метки класса давайте сделаем следующее упорядочим нашу таблицу по убыванию ответов алгоритма и получим таблицу две видите она получилась перемешаны объектов ней упорядочены по убыванию краснотой вероятностной оценки теперь нужно обратить свое внимание на последний столбец внем находятся истинные метки класса смотрите в идеале последний столбец после упорядочивания по второму столбцу должен принять следующий вид в идеале вначале должны эти единицы а потом идти все это будет такс идеальный результат работы нашего алгоритма однако такое на практике не всегда выполняется после упорядочение по вероятностным оценкам в последнем столбце у вас единички новики могут стоять абсолютно вперемешку так вот как рассмотрим число этих переменных число нарушений порядка таблице две последнем столбце и как раз и определяет качество работы алгоритма по таблице две можно построить так называемую рок кривую она выглядит примерно так как указано на этих словах то есть она вот как проходит начиная с точки ноль ноль и заканчивая точки скандинавами один один и построив рок кривую вычисляют также площадь под этой кривой показатель паук и соответственно чем больше аук тем лучше но остался последний вопрос который нужно разобраться как построить рок кривую ничего сложного там нет итак у вас есть теста выборка вы его прогнали через ваш вероятности алгоритм у вас получились вероятностной оценки вы как я уже говорил на прошлых слайдов упорядочили таблицу тестовых объектов по убыванию вероятности принадлежности к классу один и теперь вы смотрите на последний столбец вашей таблицы внем стоят истинные метки класс нужно сделать следующее давайте немножко порисуем итак допустим что нам это число нулей тестовой выборке то есть это число объектов из класса ноль у которых действительно метка класса равно нулю нашем случае таких объектов читать три штуки а м это число объектов тестовой выборки у которых метка класса равна единице вы нашем случае таких объектов ровно три теперь нужно взять единичный квадрат на координатной плоскости и разбить его на м равных частей горизонтальными линиями и нам равных частей вертикальными линиями получится вот такая вот сеточка которая указана на нашем складе а теперь по этой сеточки нужно рисовать рок любую что мы делаем мы просматриваем сроки нашей таблице сверху вниз напоминает что это порядочные таблице второй столбец должен быть упорядочены мы стартуем с первой строки и спускаемся вниз после просмотра каждой строки мы будем рисовать фрагмент рог кривой мы стартуем из точки ноль ноль из начало координат если значение метки класса просматриваемые статьи равно единице то мы делаем шаг вверх а если значение метки классов текущей строке равно нулю то мы двигаемся вправо давайте посмотрим почему у вас получилось именно такая кривая посмотрите мы начинаем сначала координаты точки ноль ноль и вначале мы двигаемся вверх а почему потому что первый объект нашей таблицы имеет действительно метку один выдвигается вверх второй объект нашей таблицы имеет метку ноль поэтому мы двигаемся вправо открытие объект снова имеет метку один и мы поэтому сдвигаемся вверх но тут возникает вопрос какого черта у нас возник здесь фрагмент кривой которая идет на искосок об этом как раз повествует следующие правила построения рог кривой оказывается что если у нескольких объектов значение меток равны то есть равны значениям этих вероятностей который выдал алгоритм то мы делаем шаг так сказать по диагонали а именно мы сразу делаем шаг точку которая на да блоков выше и наоборот блоков правее чем наша исходная позиция а число ибо они вычисляют следующим образом число это число единиц в группе объектов у которых одно вероятное значение и число это число объектов которых мягко класс равно вот второй группе которые получили одну и ту же вероятность вот смотрите вы нашей таблице мы видим что у нас есть два объектов которых вероятность который выдал вероятности алгоритм равно другое там есть два объекта сна там три и пять из них вряд ли все равно ноль две но среди этих объектов есть ровно один объект из класса ноль ровно один объект из класса один поэтому мы должны двинуться сразу по диагонали на одну ячейку справа и на одну ячейку вверх и поэтому тут возникает вот такая вот диагональ для чего строится кривая какой ней толк отделом общем по рог кривой можно найти оптимальное значение порога ещё раз напоминаю смысл порога такой порог он об выделяет правила округления вероятности то есть все вероятности которые больше порога не появляется до единицы все что меньше округляется нуля как найти наиболее оптимальные для ваших целей значение порога для этого нужно взять то курс рог кривой и порок соответствующие этой точки определяется как вероятность объекта который использовался при построении именно этого фрагмента кривой например на центральном рисунке мы видим точку круглишок на срок либо какой ему соответствует порог нужно вспомнить какой объект использовался при построении этого фрагмента урока его и посмотреть какова у него значение вероятности при построении этой точке использовался объекту которая оценка я танцевала ноль три и поэтому порог соответствующий год указанные точки на рок кривой равен ноль три а теперь возникает вопрос почему мы взяли именно эту точку срок кривой почему мы взяли именно этот порог почему это точка лучше чем другие точки нашей рог кривой потому что если мы возьмём другие точки срок кривой порог будет уже другим правильно а на самом деле не все так просто делов том что координаты точки срок кривой они имеют очень важный так сказать физический смысл чему равны координаты точки срок кривой значение по горизонтали это так называемые фол спользуйте фрейд то есть доля объектов класса ноль который неверно классифицированы нашем алгоритм значение по вертикальной оси она тоже имеет физический смысл это тру позитив рейк или иными словами это прикол полнота эта доля объектов класса один который верно классифицированы нашим алгоритмом а теперь все понятно так нужно выбрать оптимальное значение порога вам нужно понять вот вашей конкретной задачи какие допустимыми значениями могут обладать функции спользуйте фрейд и цру позитив рейк какие значения для них ещё допустимо когда вы с этим определитесь вы просто постройте точку сроки в которые красоты имеет соответствующие координаты по горизонтальной оси у нее будет координаты вот фолз пользователей которые вас устраивают а по вертикальной оси у нее будет тру позитив который вас устраивает пресечение вы находите точку срок кривой как было сказано прошлом складе точка срок любой определяет собственно и порог по которому округляются вероятно ну что какие выводы следуют из нашей лекции оказывается качество работы вероятно алгоритма можно секс помощью как функция волос так с помощью такой интересный метрики как рок паук и мы собственно научились эти характеристики вычислять",
        "rating": 1,
        "url": "https://stepik.org/lesson/83227/step/1?unit=59863"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310355,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/1?unit=60545"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310357,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/2?unit=60545"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310412,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/3?unit=60545"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310414,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/4?unit=60545"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536792,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/5?unit=60545"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536793,
        "lesson_id": 83973,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83973/step/6?unit=60545"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 537039,
        "lesson_id": 190157,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190157/step/1?unit=164662"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Голосование по большинству (комитет)",
        "type": "text",
        "step_id": 626876,
        "lesson_id": 83229,
        "content": "На прошлых лекциях было изучено много моделей МО. А можно ли, имея несколько моделей, собрать из них новую модель с большей точностью предсказания? Этот раздел будет как раз посвящен способам “сборки” нескольких моделей в одну мощную “супермодель”. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/0GqGp",
        "rating": 1,
        "url": "https://stepik.org/lesson/83229/step/1?unit=59865"
      },
      {
        "lesson_name": "Голосование по большинству (комитет)",
        "type": "video",
        "step_id": 623914,
        "lesson_id": 83229,
        "content": "на этой и на следующих лекциях мы будем решать следующую проблему как известно человек существо ленивое и когда у вас есть уже несколько моделей готовых которые решают одно и то же задач и анализа данных то хочется не придумывай ничего нового не придумали ничего сверх оригинального как то из существующих моделей состряпать новую но которая будет лучше всех предыдущих вот как это сделать потом будет рассказана на это и на следующих лекциях ну что ж вот перед нами такая проблема у вас уже есть несколько алгоритмов классификации или регрессией с одиннадцать двадцать ноль которые обладают точностью три один две три к соотве именно здесь по точности можно понимать любую из метрик качество которое вводится для задачи классификации и регрессией я думаю пока не важно так сказать конкретизировать что такое один две считаете что это некий показатель точности каждого из алгоритмов мне кажется вопрос можно ли из модели т одиннадцать двадцать ноль построить новую модель которая будет мощнее всех исходных моделей целях естественно есть общие подходы которые позволяют это делать среди основных таких подходов история более мощных моделей существующих можно выделить следующие три подхода первый подход это голосование по большинству или подход комитет второй подход это взвешенное голосование и третий подход это про цензура так называемому стинга когда каждая следующая модель улучшается за счёт предыдущий на этой лекции давайте рассмотрим что такое комитет составленные из моделей машинного обучения для простоты будем представлять что мы решаем задачи классификации для задачи игры все делается аналогично итак представьте себе что у вас есть несколько моделей классификации содержится двадцать ну допустим т один эта моделька на две это дерево решений центре допустим это линейный классификатор и так далее и все эти модели решают одну и ту же задачу как будет работать комитет составленные из указанных моделей а очень просто на вход комитету подается тестируемый объект и вы этот объект прогоняете через каждую модель особо комитета часть модели отнесет объект класса один а другая часть модели отличие от объекта от классу минус один возникает вопрос а чтобы дать так сказать какие ответа а итог работы комитета такой а класс который выбрала большинство моделей является окончательной по такой схеме работает он сам болел гриппом который называется комитетов схема комитетов вообще говоря встречалась их предыдущих наших лекциях по схеме комитета работа такая модель машинного обучения как рядом форест случайный вес вспоминаете там устроилась несколько деревьев ну отсюда и название лес лес состоит из нескольких деревьев и через каждое дерево этого леса прогулялся тестируемый объект а далее мы смотрели сколько деревьев сказала что этот объект из класса один сколько деревьев сказала что этот объект из другого класса и в конечном итоге побеждал тот класс за которые проголосовало большинство деревьев естественно возник это просто какой смысл вообще строить комитеты из моделей машинного обучения почему точность комитета может превосходить точности по моделей из которых он составлен что лежит в основе этого эффекта на самом деле в основе этого эффекта несложные упражнения из теории вероятности давайте рассмотрим следующую простенькую задачку итак представьте себе что комитет состоит из трех членов то есть у вас есть три модели которые решают одну и ту же задачу классификации допустим что известно точность этих моделей здесь качестве точности можете представлять что имеется ввиду общая точность эко россии так вот это общая точность для каждой модели равно следующем числам ноль шесть ноль семь ноль восемь окончательно решение когда вы прогоняете через комитет какой тестируемый объект принимается в соответствии с большинством голосов и давайте найдем вероятность ошибки комитета то есть какой вероятностью тестируемый объект будет классифицирован неправильно сделаем важное допущение что члены комитета голосуют независимо друг от друга то есть результаты работы одного из членов комитета никак не влияет на результаты работы других членов комитета итак как найти вероятность ошибки и комитета для этого нужно разобрать все случаи когда комитет ошибается поскольку у нас окончательное решение принимается виде процедуры голосования то комитет из трех членов ошибётся тогда когда ошибутся две или три но комитета таким образом у нас возникает следующая четыре случая ошибки комитета они перечислены на складе первый случай это когда первый член комитета классифицировать объект правильно ошиблись второй и третий второй случай когда ошибка первый и третий третий случай когда ошиблись первый второй члены комитета и наконец четвертый случай когда ошибаются все члены комитета а теперь найдем вероятности событий которые указаны в пунктах один две три четыре поскольку мы предположили что он члены комитета не зависят друг от друга то можно применить формулу произведение вероятностей которая была проведена объектах по наивному баевском классификатору вот как посчитать вероятность случаев из пункта один итак первый член комитета папа голосовал правильно мы знаем что по условию эта вероятность равна ноль шесть а вторая две члена комитета они ошиблись с какой вероятностью ошибается второй член комитета по условиям известно что его точно все равно ноль семь стало быть вероятность ошибки равно противоположному числу то есть ноль три а про третьего члена комитета известно что его точность ноль восемь стало быть вероятность ошибки это единица минус ноль восемь шесть ноль два теперь по форм после произведения вероятности можно посчитать вероятность указанные в пункте один мы перемножаем эти три числа и получаем число ноль целых тридцать шесть тысячных аналогично считаются вероятности следующих трех случаев но давайте я адель остановлюсь на последнем случае когда ошибаются все классификаторы мы знаем что их точности равны соответственно число ноль шесть ноль семь ноль восемь соответственно вероятности ошибки этих классификаторов ровно противоположным число ночь три ноль три ноль две поэтому чтобы посчитать вероятность с какой вот случается событие указанных пункте четыре нужно перемножить вероятности ошибок каждого из членов комитета что мы собственное дело итак мы для каждого случая посчитали его вероятность эти случаи полностью описывают все ситуации когда комитет ошибается чтобы получить итоговую вероятность ошибки итогам вероятность ошибки комитета полученное число нужно сложить и получить число ноль целых двести двенадцать тысячных то есть вот такой вероятностью она примерно равна двадцати одному процент ошибается комитет соответственно точность комитета это противоположная число это единица минус ноль целых двести двенадцать тысячных мы получаем что точность комитета равна примерно семьдесят девять процентов как видим точность комитета она кстати возросла по сравнению с точностью самого слабого члена комитета особенно эффективна точность комитета возрастает тогда когда он составлен из классификаторов которые примерно равны друг другу по своей точности например наши слушатели могут самостоятельно решить задачу когда комитет составлен из трех членов каждый член этого комитета имеет одно и то же точность ноль восемь и требуется найти точность всего комитета это упражнение делается с помощью аналогичных рассуждений которые были приведены на предыдущих сайтах чтобы замотивировать наших слуг если я приведу ответ к этой задачи точность комитета составленного из трех членов каждый из которых имеет точность ноль восемь она оказывается больше чем точность каждого из членов равна ноль целых восемьсот девяносто шесть тысяч то есть это тот случай когда точность комитета превосходит точность каждого из его членов ну естественно хороший комитет получается тогда когда у вас и базовые модели то есть члены комитета уже являются хорошими алгоритмами то есть имеет очень высокую точность отделов том что если члены комитета имеет низкую точность вот как например в комитете указанным на складе мы видим что комитет состоит из моделей точность которых всего лишь ноль пять пятьдесят процентов возникает вопрос а какова будет точность комитета в этом случае оказывается если вы будете решать эту задачу по той же схеме которая была на предыдущих стадиях вы получите следующий ответ точность комитета от кажется равной такому же числу ноль пять то есть комитета составлены из таких вот плохо этих классификаторов он дополнительную точность не привносит и поэтому комитет имеет смысл создавать тогда когда точность ваших классификаторов точность ввод исходных моделей машинного обучения сильно больше чем ноль пять",
        "rating": 1,
        "url": "https://stepik.org/lesson/83229/step/2?unit=59865"
      },
      {
        "lesson_name": "Голосование по большинству (комитет)",
        "type": "video",
        "step_id": 623915,
        "lesson_id": 83229,
        "content": "мы видели что когда у вас комитет составлен из адекватных моделей то есть из моделей которые имеют достаточно высокую точность комитет этот имеет точность ещё больше чем точность его членов естественно это все работает предположение что результаты работы одного члена комитета не зависит от результата работает другого члена комитета возникает вопрос а что делать и что произойдет если члены комитета друг от друга зависит вчастности давайте рассмотрим такую парадоксальную ситуацию давайте возьмём и составим комитет из копии одного и того же классификатора то есть у нас есть допускается модель которая натренировано наших данных допустим кайн какой нибудь так вот вк члены комитета мы возьмём и составим его из склонов этой самой модели что произойдет давайте решим вот такую задачку составили комитет значит из трех клонов одно и то же модели давайте чтобы немножко затуманить мозги нашим слушателям я скажу что точность этой модели равна вот такому очень длинному число так вот не надо поддаваться панике увидев такое сложное число делов том что эту задачку решить очень просто можно даже лишить вкусно как вы думаете точность комитета составленного одной и той же модели равно чему на самом деле точность комитета никак не поменяется и это можно доказать строгими рассуждениями когда у вас комитет составлен из клонов одной и той же модели то возникает только двести функции при его работе первая ситуация это когда все ваши модели классифицировали объект правильно и вторая ситуация когда все они ошиблись других ситуаций нет потому что работа клонов она друг от друга зависит клон они могут замечательно вопрос по разному и соответственно объекта тестируемый будет правильно классифицирован комитетом если происходит случай указанные на складе в пункте один а этот случай происходит такой вот вероятностью которая указана на нашем флаги внизу потому что каждый комитет каждый клан имеет вот такой вот точность здесь вероятности перемножать нельзя потому что события то есть результаты работы классификатор они у нас уже зависимы друг от друга итак мы получили следующий результат если комитет составлен из клонов одного и того же классификатора то точности комитета равна точности вот только модели которые он составлен точность плитняком составлении комитета никак увеличится не может потому что она прошлом я не обозначил проблему когда вашем комитете появляются зависимые классификаторы то точность комитета может не возрасти по сравнению с точностью базовых моделей и действительно на практике независимость результатов классификаторов действитель она может быть зависимы друг от друга но это не повод отказываться от комитета и нужно пробовать составлять комитет любом случае смотри какая у него будет точность позвольте вам напомнить результаты связанные с наивным борисовским классификатором когда мы изучим на эту тему я говорил что при работе на абонентская классификатора нужно потребовать чтобы не целевые признаки были независимы друг от друга однако на практике такое случается очень редко но тем не менее наивный боярский классификаторы может отработать очень хорошо даже когда есть зависимость между нецелевыми признаками случае комитета нужно пробовать и пытаться его запускать любом случае даже если между результатами работы ваших классификаторов есть некая зависимость если они зависят друг от друга как то своей работе тем не менее комитет может получиться более удачным то есть он может иметь более высокую точность по сравнению с точностью а базовых классификаторах естественно если друг мои проблемы связаны с построением комитета например никак нельзя заранее рассчитать оптимальный размер комитета потому что тут возникает некая дилемма теоретической точки зрения добавления новых членов комитетов повышает точность предсказания но приус лори что своей работе эти алгоритмы не зависят друг от друга однако когда вам требуется составить комитет из все большего числа членов очень сложно найти модели машинного обучения которые друг друга не зависит спросить точки зрения означает что нужно найти моделей машинного обучения которые работают принципиально не похожи друг на друга когда размер комитета россии относительно очень сложно найти вот модели которые друг на друга не похожи но возьмите в качестве первого члена комитета как в качестве второго дерево решение качестве третьего линейный классификатор а дальше что на самом деле не так много различных принципиально различных моделей машинного обучения и поэтому когда комитет очень большой действительно очень сложно его наполнить модели машин обучение которые работают независимо друг от друга ну что давайте я скажу пару фраз насчет рейса оказывается комитет можно строить для задачи регрессии то есть для задачи предсказания числового признака итак представьте себе что вы уже построили несколько моделей машинного обучения которые решают одну и ту же задачу лестница одиннадцать двадцать ноль итак когда вы будете прогонять тестируемый объект а через все эти модели то каждый из них выдаст предсказанные значения или питания поскольку назначили россия то никита это число вещественное число так вот что взять качестве окончательного ответа комитета наиболее логично качестве окончательного ответа брать среднее значение чисел их его про таким образом с помощью комитета решается задача регрессии итак какие выводы следуют из нашей лекции во первых мы познакомились одним из видов ансамбли он называется комитет мы вычислили точность комитета конечно при условии независимая работа членов этого комитета если комитет входят зависимые друг от друга модели то могут возникнуть проблемы точность комитета необязательно возрастет по сравнению с точностью его членов",
        "rating": 1,
        "url": "https://stepik.org/lesson/83229/step/3?unit=59865"
      }
    ],
    [
      {
        "lesson_name": "Взвешенное голосование AdaBoost",
        "type": "video",
        "step_id": 623917,
        "lesson_id": 83230,
        "content": "а теперь мы рассмотрим процедуру взвешенного голосования давайте прежде всего я расскажу об аналогиях человеческом мире которые связаны с процедурой взвешенного голосования и так на прошлой лекции я говорил о комитете то есть это такая штука которая составлена из моделей машинного обучения каждая из моделей грубо говоря голосуют за тот или иной класса тестируемого объекта и окончательное решение принимается по большинству голосов то есть тот класс за который проиграл рисовала большая часть членов комитета и будет выбран в качестве окончательного однако такая казалось бы естественно схема голосования она не единственная например акционеры какого либо общество голосуют не каждый так сказать сам за себя а каждый голосует пропорционально доле его но акций то есть грубо говоря сколько у тебя акций столько голосов ты имеешь или как ещё можно сказать какой у тебя вес в этой компании вот столько голосов у тебя есть принятие решения касающиеся работы твоей компании так вот она этой идеи основана работа алгоритмов взвешенного голосования итак давайте сделаем следующее у нас по прежнему будут комитеты составленные из моделей машинного обучения но теперь каждый член ведь это будет иметь некий вес и этот вес будет влиять на окончательное решение которое принимается комитетом когда вы наход комитету подаете тот или иной объект ну что ж давайте напишем это более формально представьте себе что у вас если классификаторы садится двадцать ноль и вы проверяете результат их работы на некотором объекте а каждая из классификаторов может выдать либо один либо минус один здесь метки классов обозначаются тоже единица минус единица как теории именно классификатор итак каждый классификатор может отнести его либо классу один либо классу минус один окончательное решение зависит от весов этих классификаторов а именно если указанные на складе выражение меньше нуля что объект будет отнесен класса минус один а если оно больше нуля то классу один то есть смотрите вес каждого классификатора существенно используется при вычисления результата то есть результаты работы самого классификатора нужно домножить на его место потом сложить результаты полученные от других классификаторов и только после этого принять решение естественно чем больше по модулю весу классификатора тем так сказать сильнее его участие в выработке окончательного решения выработки оконч классификация объекта а вот такая схема работы взвешенного голосования очень сильно похоже на алгоритма линейный классификации на линейные классификаторы и более того вся теория и общие методы работы с комитетами снова голосование имеет своим началом теория линейных классификаторах так же как теория линейных классификаторов вводится понятие отступа отступ здесь считается по очень похожей формы нужно взять истинную метку класса игры и давно на вот это выражение указанная скобках только в отличие от линейных классификаторов там стоит не сумма нецелевых признаков некоторыми лесами а там стоят результаты работы классификаторов с некоторыми далее что нам дает понятие о ступа так же как в случае с линейными классификаторами отступ будет отрицательной величиной если комитет на этом объекте ошибается и соответственно нужно минимизировать число отрицательных отступов на тренировочной выборки то есть нужно на самом деле минимизировать выражение указана центре слайды нужно минимизировать указанную сумму почему равно выражение квадратные скобки мм меньше нуля это выражение выводилась на лекции по линейным классификатором здесь нам его смысл не важно главное понять суть мы должны минимизировать число ошибок нашего комитета на тренировочный выборки но здесь возникает такая же бодяга как случае семейными классификаторами те же самые проблемы мы понимаем что вот это выражение которое центре слайдов оно не дифференциал программа не имеет производные и поэтому искать у него точку минимума это очень очень тяжкое занятие так же как и случаями классификаторами это выражение заменяется на модерирую функцию которая уже является дифференцируемой естественно качестве моделирующей функции можно выбрать различные выражения и на лекции по линейным классификаторами там даже рисовал красивую картинку которая говорит о том что моделирующий функцию можно выбирать самые разные однако если мы выберем какой то конкретную функцию например функцию экспонентов степени минус отступ то мы получим очень известный алгоритм который называется отгрузка или адаптивные бусин что это за алгоритм такой допуст прежде всего нужно понимать что этот при этом из рациона то есть он будет работать цикле постоянно постоянно улучшая себя на каких идеях основана допуст вводится понятие вес объекта которая позволяет свою очередь вычислить взвешенную ошибку классификатора на трениро обычный выборки а поскольку алгоритма допуст работает цикле как я уже сказал то на каждой операции этого цикла происходит следующее мы берем классификатор то есть член комитета у которого минимальная взвешенная ошибка то есть который данные центр работает лучше других меньше других ошибается берём такой классификатор и настраиваем вес при этом классификаторе настроить этот вес мы смотрим на таких объектах классификатор которые мы настраивали по прежнему ошибается так вот наследуя операции алгоритма автобус объекты на которых он ошибается приобретут больший вес таким образом на каждой операции алгоритма выбирается классификатор которые лучше других классифицируют трудные объекты то есть объекты с очень большим весом нато давайте введем несколько обозначений математических и так через уже будем обозначать вес жидкого объекта вначале все леса они одинаковы и равны единице на объем тренировочной выборке ошибка в же классификатора цепи того эта сумма весов объектов на которых он ошибается так вот теперь давайте приведем формальную схему работы алгоритма автобус на первом шаге нужно найти классификаторов минимальной ошибкой б далее нужно пересчитать вес при этом классификаторы вес пересчитывается по формуле указанной центре слайда далее нужно изменить пересчитать леса объектов по формуле указанной в пункте три а далее мне нужно программировать это делается для того чтобы сумма весов всех объектов была равна единице и более того шаги один две три нужно повторять пока не произойдет следующее ну во первых пока вам не надоест потому что на каждом шаге будет увеличиваться точность ее все таки вы не можете позволить чтобы алгоритм работы бесконечное время когда нужно прекратить таким образом цикл предыдущего слайда можно крутить до тех пор пока вам действительно не надоело во вторых точность на тестовой выборки на какой этаж ты может стабилизироваться и это ещё одна причина когда нужно так сказать кончайте крутить этот цикл когда вы закончили крутить этот цикл то вы получаете итоговое правила классификации а именно у вас есть настроенные леса есть есть секаторы акций так вот вы просто вычисляете выражения если сумма состоящая из веса классификатора умноженного на результат его работы меньше нуля то объект будет из класса минус один противном случае система любимый объекта будет из класса один кстати схема работает алгоритма допуст позволяет вообще говоря бороться и с выбросами а именно что такое выброс с точки зрения алгоритма автобус а выброс в этом случае это будут тема как ты которые постоянно приобретают очень большие леса то есть эти объекты очень сложно поддаются какое то разумное классификации и таким образом алгоритм адамс можно использовать и запускать когда вы боретесь выбросы ищите выбросы",
        "rating": 1,
        "url": "https://stepik.org/lesson/83230/step/1?unit=59866"
      },
      {
        "lesson_name": "Взвешенное голосование AdaBoost",
        "type": "video",
        "step_id": 623916,
        "lesson_id": 83230,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83230/step/2?unit=59866"
      }
    ],
    [
      {
        "lesson_name": "Градиентный бустинг",
        "type": "video",
        "step_id": 623918,
        "lesson_id": 83231,
        "content": "а сейчас мы поговорим про такую схему построения ансамбли алгоритма как бусинка чем смысл постинга до этого мы изучали комитет связаны с голосованием комитет связанные со взвешенным голосованием а что это такое комитетов как бы сказать формируется сразу то есть у вас есть уже куча готовых моделей машинного обучения вы их одновременно запихиваете комитет и далее он у вас либо голосует по большинству либо вы потом настраиваете леса для взвешенного основания схема процедуры мусинга она несколько отличается от схемы комитета делал чем схеме бу стинга у вас нету так сказать заранее заданного множества моделей из которых вы формируете комитет каждая след будущая модель строится по старой точнее она тренируется по ошибкам предыдущей модели вот в этом смысл процедуры мусинга итак иными словами бусины это способ улучшить алгоритм предсказания бхсп очень нового алгоритма предсказания а ну давайте сделаем небольшое лирическое отступление от типов данных которые нам сегодня понадобятся на этой лекции я говорила о деревьях которые решают задачи классификации однако есть деревья которые решают задачи регрессии так вот дальнейшем процедуру стинга я буду стрелять помощи таких вот регрессионных деревьев вот одно из таких деревьев я привожу здесь на складе это дерево уже появлялась у нас ранее я всего помощи демонстрировал работу моделей название которое считает средний балл студента за следующую сессию и здесь это регрессионное дерево как вы видите используют число баллов экзамена егэ по математике и как ни странно она ещё использует время которое требует студенту чтоб дойти из дома до университета вот такие вот деревья которые очень короткий которые состоят из более чем четырех листьев мы будем для краткости называть пеньками на сайте приведены один из таких синяков у него как вы видите только три листа итак пинки дальше для краткости это очень короткие деревни ну чтож возвращаемся к нашему селу прежде всего давайте разберемся как процедура бусинка выглядит для задачи регрессией а потом наконец мы будем говорить опусти ноги для задачи классификации почему именно такая последовательно потому что построить бусин для задачи регрессии гораздо проще чем для задачи классификации и более того бусин для классификации он фактически сводится к путин для регрессии итак что мы имеем у нас есть тренировочная выборка которая приведена в таблице у нас были не целевые признаки здесь нам они не важны поэтому неоцененные признаки обозначил многоточие у нас есть истинное значение признака или поскольку мы решаем задачи регрессии то или это число допустим мы построены так сказать алгоритма регрессией который предсказывает целевой признак игорь и его результаты приведены столбце а то есть а это алгоритм ему на вход подаются не целевые признаки и он вам предсказывает значение целевого признака их естественно алгоритм а может ошибаться и как мы видим в этой таблице он конечно же в основном угол ну значение признака или но он допускает ошибку на каждом объекте так вот давайте разность между истинным значением признака игорь и предсказанным добавим в качестве нового столбца нашу таблицу как вы видите в нашей таблице появился столбец плавность это разность между истинным значением и предсказанным и возникает вопрос а дальше что делать что делать с этими ну что ж давайте во первых зафиксируем множество алгоритмов среди которых будем искать алгоритм который улучшает работу алгоритма а это может быть например множество всех моделей линейной регрессии может быть множество всех регрессионных деревьев а можно взять например более узкие класс например множество всех педиков мы будем искать алгоритм который улучшает работу алгоритма а как мы будем искать на основании чего давайте будем искать алгоритм б которые улучшают работу алгоритма по какому принципу алгоритм б будет искать поправки к ответам алгоритма а то есть алгоритм б будет три палаццо на ошибках алгоритма а а что такое ошибка алгоритма а эта красота есть разность между истинным значением и значение который предсказал алгоритма а вот собственно для этого нам понадобился новый столбец который эти разности содержит таким образом мы подаем на ход вот такую таблицу в которой не оценивали признаки и разность указаны и на основании этой таблице строим новую модель предсказания которые эту самую разность как раз будет предсказывать новую что мы получаем давайте вернемся к исходной таблице у нас там были целевые признаки был целевая признак точнее его истинные значения мы построили во первых алгоритм а почитали разности построить алгоритм б и после этого можно например рассмотреть сумму ответов которые выдает долго дома и алгоритм б как вы видите последний столбец таблицы красный получается эта сумма числа указана столбце для дома и числа для алгоритма б таким образом мы получаем новую модель регрессии которая является суммой алгоритмах и алгоритма б как вы видите это новая модель обладает большей точностью по сравнению с исходными моделями регрессией а и б теперь смотрите что дальше если точность новой модели а плюс б вас устраивает то можно остановиться но если не устраивает то этот процесс можно продолжать дальше именно нужно посчитать разность между истинным ответом игорь и ответом модели а плюс б сформировать новый столбец с разностью и уже на этом столбце тренировать новую модель т и после этого сформировать новую модель а плюс б плюс т ну что ж а чем нужно иметь ввиду оказывается не всякую модели можно юлук с помощью постинга но давайте рассмотрим это очень поучительный пример итак у нас есть такая вот таблица нецелевой признак х предсказываем целевой признак и эта задача регрессии то есть признак это число давайте качестве алгоритма возьмём модель линейной регрессии этот пример кстати пришелся на лекций посвященных моделям линейной регрессии и мы знаем ответ оказывается качестве модели берётся следующее выражение это икс плюс один то есть чтобы предсказать признак клик с помощью модели линейный рейсы вы должны взять нецелевой признак текст и прибавить к нему единицу всю вашу информацию можно занести таблицу она от этого разбухнет то есть мы записали в таблице все значения которые выдает алгоритм линейной регрессии а вот они перед вами они как я уже сказал вычисляются по формуле как нужно прибавить единицу и кроме того нужно посчитать разность между истинным значением игры и тем значением которое выдало алгоритма а для этого мы формируем новый столбец который называется разность а что дальше а давайте пытаться улучшить работу алгоритма а но давайте посмотрим следующим образом мы результаты работы алгоритма который является моделью линейный рейсе будем улучшать с помощью новой модели линейной регрессии посмотрим что из этого выйдет итак какую таблицу мы должны подать на вход алгоритма чтобы была построена модель б мы должны подать таблицу сми целевым признаком пикс от тренировать мы его будем на столбце с разность как вы думаете какая модель ней регрессия будет построена когда ей подадут на ход вот такой вот таблицу чтобы нашим слушателям легче думалось проще эти данные изобразить виде точек на плоскости итак мы видим вот такие вот точки красных четыре штуки как вы думаете как пройдет при майя которая должна так сказать минимизировать величину отклонения точек до нее есть должна пойти как то посередке между ними правильно так вот оказывается что прямая которая будет построена то есть моделей регрессии б это сам неделе тождественные ноль после сильного онекс это не что иное как график функции равна нулю как раз и удовлетворяет условию что отклонения точек вот этих разностей до нее они минимальным то есть на втором шаге бусин будет построена модель которая выдает тождественный ноль что это следует из этого следует что алгоритм а улучшить не получится потому что мы же получаете алгоритм с помощью суммирования его с результатом работы алгоритма поиска только алгоритм выдает тождественной ноль то получается сумма от плюс да это слово и таким образом у вас алгоритм не меняется вам не удалось его никак улучшить почему это произошло потому что линейную модель нельзя улучшить с помощью линейной модели то есть нужно естественно понимать что можно с помощью чего улучшается что с помощью чего улучшить не получится давайте разберем пример который взят из указанного здесь источника и этот пример демонстрирует работу процедуры мусинга когда а вы решайте задачи регрессии с помощью пеньков то с помощью регрессионных деревьев очень мало и высоты итак давайте вспомним как выглядит пенёк это дерево которого не более чем четыре листа ну один из примеров таких денег но приведены справа на нашем складе у него четыре листа а если изображать его так сказать графический то на самом деле с каждым пиком можно связать функцию которая не более чем четыре ступеньки делов том что каждый не спала является некое значение которое выдает дерево так вот значение различных не более чем четыре поэтому если вы результаты работы дерево будете изображать на плоскости то получится такая вот ступенчатый график котором не более чем четыре ступеньки а теперь посмотри ты какие у нас данные эти данные изображены слева на нашем складе там видел черные точки это красоту нашей тренировочной выборка есть нецелевой признак секс и есть любой признак это очень похоже на зашумленных график фу опции косинуса но поскольку мы его будем приближаться помощью пеньков то есть помощью четырех ступенчатых функций то это вообще говоря задача нетривиальная график косинуса с помощью четырех ступенчатых функции приблизительно так то и просто вообще какая но посмотрим тем не менее что происходит на каждое из ситуации бусинка итак давайте посмотрим на рисунок один д на этом рисунке строится пенёк то есть четырех ступенчатая функция которому соответствует которые более менее как это приближает ваши данные видите вот эти четыре ступеньки они как бы проходит через середины ваших данных и стараются сильно ошибаться тем не менее этот пенёк он не может в идеале приблизить абсолютно все ваши данные из тренировочных выборки естественно возник знают ошибки что вы делаете нужно посчитать разность между истинным ответом и тем который предсказал пенёк для каждой черной точки для каждого объекта тренировочный выборки нужно взять его значение по оси их и вычесть из него то значение на котором расположен ступенька вашей функций а то значение которое выдает вашу книг и у вас получится разности эти разности приведены на рисунке две теперь следующий алгоритм должен тренироваться именно на этих разных то есть вам нужно снова построить пенёк то есть функцию из четырех ступеней которые более менее как то вот проходит посередке разностей с рисунком две и чтобы она не сильно как ты ошибалась итак построенный пенёк он представлен виде зеленая линия на рисунке две теперь что мы делаем мы пенёк с рисунка один д складываем спилком с рисунком две и получается уже более сложная функция с большим числом ступенек и она гораздо лучше приближает функцию косинуса но она приезжает эту функцию не сильно то и идеально пока поэтому можно снова посчитать разности изобразить в виде черных точек на рисунке три эти точки снова пытаться построить четыреста пятьдесят график то есть снова попытаться построить ревизионное дерева которая является пеньком меню построен он указан на рисунке три с помощью зеленой линии его нужно как сложить с графиком который видите на рисунке две и получится график с рисунка три это более сложная функция более многоступенчатая она ещё лучше приближается график косинуса вот таким вот образом и происходит работа алгоритма мусинга при решении задачи регрессии каждая новая модель тренируется на ошибках старой и прибавляется к старой модели и таким образом у вас возникают все более и более мощные модели которые все лучше и лучше приближает ваши данные",
        "rating": 1,
        "url": "https://stepik.org/lesson/83231/step/1?unit=59867"
      },
      {
        "lesson_name": "Градиентный бустинг",
        "type": "video",
        "step_id": 623919,
        "lesson_id": 83231,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83231/step/2?unit=59867"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 310415,
        "lesson_id": 83974,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83974/step/1?unit=60546"
      },
      {
        "lesson_name": "Тест",
        "type": "number",
        "step_id": 536813,
        "lesson_id": 83974,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83974/step/2?unit=60546"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536815,
        "lesson_id": 83974,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83974/step/3?unit=60546"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536816,
        "lesson_id": 83974,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83974/step/4?unit=60546"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 537011,
        "lesson_id": 83974,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83974/step/5?unit=60546"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "fill-blanks",
        "step_id": 537041,
        "lesson_id": 190158,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190158/step/1?unit=164663"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Мотивация и простые способы отбора признаков",
        "type": "text",
        "step_id": 307163,
        "lesson_id": 83232,
        "content": "Как известно, каждые объекты в МО имеют набор признаков. \n\n Когда признаков слишком мало, то это плохо - моделям МО трудно найти закономерность между ними. Но оказывается, что слишком много признаков тоже плохо! \n\n Почему так происходит и как нужно удалять лишние признаки - на эти вопросы ответит этот раздел. \n\n   \n\n Лекция в формате .pdf доступна по ссылке https://ispri.ng/72J2K",
        "rating": 1,
        "url": "https://stepik.org/lesson/83232/step/1?unit=59868"
      },
      {
        "lesson_name": "Мотивация и простые способы отбора признаков",
        "type": "video",
        "step_id": 631318,
        "lesson_id": 83232,
        "content": "она этой лекции на последующие мы займемся весьма кровожадным делом мы будем удалять лишние признаки из наших таблиц возникает вопрос а зачем признаки нужно удалять версию с одной стороны когда у вас очень мало нецелевых признаков по ним действительно очень сложно предсказать целевой признак то есть малое число признаков это плохо но почему плохо когда признаков очень и очень многое почему лишнее признаки нужно удалять оказывается есть несколько соображений польза удаления избыточного числа признаков давайте их перечислим во первых когда вы удаляете внешние признаки у вас экономится время и память во вторых когда у вас слишком много признаков то легкая двоякая проблема сложнее найти закономерность смотрите когда признаком очень мало то закономерность тоже очень сложно найти но когда признаков очень много и все они говорят о чем то вразнобой то закономерность предсказать целевой признак по этим вашим если вы признаком тоже очень сложно между нецелевыми признаками могут существовать очень сильной зависимости и многие методы машинного обучения например наивный борис линейной регрессии и другие при существовании зависимых друг от друга признаков работают очень очень плохо и наконец если признаков очень много то возникает очень такой специфический эффект как проклятие размерности при проклятие размерности метрические линейные методы могут работать очень очень плохо нужно давайте теперь разбираться итак у нас есть не целевые признаки н и мы пытаемся решать задачи предсказания то есть пытаемся по признакам эксперты предсказывают значение целевого признака их какие из признаков икс все кандидаты на удаление вот как вы думаете какой из признаков их вам не нравятся какие должны быть свойства чтобы они попали подан наш нож и мы их удаления но самые простейшие изображения как найти таких кандидатов на удаление они привели на нашем складе ну во первых если признак содержит слишком много пропусков или слишком много костяных данных некорректных значений то конечно такие признаки лучше удалять во вторых если ваш признак характеризуется очень маленький ты отклонением вчастности если этот признак просто состоит из одних и тех же значений такой признак является неинформативным его тоже нужно удалить хотя конечно тут нужно очень тонко понимает что значит очень маленькое отклонение главное не удалить все больше чем надо признаков но тем не менее признак который состоит из одних и тех же константных значения если такое у вас есть он конечно бесполезно его можно удалять смело втретьих если между вашими признаками существует очень высокой корреляции секс корреляцию нужно считать с помощью коэффициента корреляции так вот если как инголяции между какими то признаками очень сильно близок по модулю единицы то это говорит о том что признаки друг друга очень сильно зависит и поэтому впринципе из этой пары как можно оставить только один а второй который зависит очень сильно от первого можно удалить и об этом вообще никак не живите а чтобы понять какие признаки очень сильно долго другом коррелирует для этого обычно составляет вот такой вот матрицу корреляцией как строится эта матрица по вертикали по горизонтали вы откладываете ваши признаки она пересечение вы оставите коэффициент корреляции но здесь приведена матрицы которые не числа указаны а коэффициент корреляции как бы обозначены каждый своим цветом и чем более светлый цвет тем выше корреляции в частности на диагонали стоят вообще единицы потому что на диагонали находится ячейки где стоит оценка реляции признака самим собой естественно признак собой он сильно коррелирует он естественно зависит сама себя и поэтому там стоят единицы но когда вы будете искать признаки которые вы хотите удалить вы должны естественно искать ячейки вне диагонали нов которых стоят очень высокий по модулю значение коэффициента корреляции какие можно советы ждать когда вы собираетесь удалять ненужные целевые признаки можно удалить лицевой признак текст если его корреляция с целевым признакам и близка к нулю хотя тут конечно надо быть очень очень внимательным к бы не удалить больше чем нужно делов том что может сложиться такая ситуация каждый из нецелевых признаков имеет очень маленькую очень очень маленькую корреляцию с целевым признаком игры но как бы в сумме они дают кумулятивный эффект и в целом все целевые признаки признаки имени определяют очень хорошо поэтому здесь конечно надо быть очень осторожным не удалить лишнего наконец ещё один совет для признаков можно читать различные характеристики их информативности вчастности неопределенность джинни посчитав неопределенность жизни для каждого из нецелевых признаков вы поймете какие из признаков наиболее информативным наконец самый такой мощный совет можно не мелочиться а сразу для всего множества признаков запустить модель предсказания делов том что некоторые модели предсказания и об этом говорил на прошлых лекциях помимо своей основной работы могут понять какие из признаков наиболее информативным вашей задачи таким моделям относятся например модель линейной регрессии и ласок делов том что когда вы строите модели линейной регрессии или ласок то у вас возникают значение весов перед каждым нецелевым признакам так вот те признаки у которых значение веса очень маленькая по модулю скорее всего будут не информативными итак какие выводы следуют из нашей лекции во первых мы выяснили точнее пришли к такому интересному факту что большое количество признаков чрезмерное число признаков нецелевых не всегда дает высокую точность алгоритмов машинного обучения и во вторых мы изучили самые самые простейшие методы отбора признаков то есть методы удаления лишних признаков",
        "rating": 1,
        "url": "https://stepik.org/lesson/83232/step/2?unit=59868"
      }
    ],
    [
      {
        "lesson_name": "Отбор признаков в несколько итераций",
        "type": "video",
        "step_id": 631319,
        "lesson_id": 83233,
        "content": "а сейчас рассмотрим сложные методы отбора признаков то есть те методы которые осуществляют отбор самых лучших признаков несколько этапов и так будем решать такую задачу нам надо найти оптимальное подмножеств на наших признаков лишнее признаки которые это оптимальная подмножество не войдут их просто возьмём и удали итак как найти оптимальное подмножество признаков ну первый способ самый простой это метод полного перебора вы начинаете перебирать все возможные подмножество ваших нецелевых признаков для каждого подмножество строить модель предсказания замеряйте е качества но проблема в том что эта задача очень трудоемкая вам нужно перебрать две степени м подмножеств ваши признаков где эм это число вот всех ваших нецелевых признаков даже для небольших м задачи перебора вот такого числа подмножеств она на самом деле очень и очень сложная даже для современных компьютеров поэтому чтобы не перебирать слишк большое число различных подмножеств признаков используют различные алгоритмы и эвристики например используют жадные алгоритмы и генетические алгоритмы вот как раз вот эти два типа алгоритмов мы и рассмотрим ну что ж давайте рассмотрим как работает жадный алгоритм мы начинаем со следующего давайте зафиксируем небольшое число н и выберем самый оптимальный набор признаков среди всех наборов признаков состоящих из стран признаков а теперь будем поступать следующим образом мы по одному признаку будем добавлять в наш исходный набор и смотреть после какого добавления качество улучшилось больше всего и поступая таким образом то есть каждый раз добавляя по одному новому признаку мы найдем рано или поздно набор который даст нам более менее высокую точность предсказания естественно добавлять новые признаки мы будем до тех пор пока мы не управимся либо максимально возможное число признаков либо точность модели не перестанет значимости но на этой картинке показано работа такого жаднов алгоритма всего начинается пожар на алгоритм мы стартуем с подмножеств признаков которые состоят из одного элемента итак у нас всего четыре нецелевых признаках кб цены давайте построим модели предсказания и посчитаем их точности случае когда целевой признак предсказывается лишь по одному не целевому признаку такие модели их точности вы видите внешнему виду нашего славно итак самый левый квадратик содержит информацию о модели предсказания где предсказания строиться лишь по нецелевому признаку а и точность такого предсказания она приведена тут же это ноль пять аналогично мы строим модели предсказать саня для других нецелевых признаков для б т д и для каждого из них считаем точность что мы видим мы видим что не целевой признак б дал модель самый большой точностью из всех которые мы имеем что мы теперь делаем признак бы как самый так сказать перспективный он у нас теперь зафиксирован и больше мы его никуда не трогаем наоборот мы были знакомы будем добавлять новые целевые признаки смотреть как сильно возрастет точность предсказаний итак у нас есть признак признак б можно добавить как признак а так признак цель или признак до давайте сделаем три модели предсказания зависимости от того какой признак добавляем и для этих трех моделей давайте посчитаем их точность эти модели предсказания на нашем сайте указанная во втором ряду что мы видим мы видим что при добавлении признаку б нового признака д мы получаем самую лучшую модель предсказания точность которой равна ноль один вот опять теперь что мы делаем дальше мы видим что признаки б далее нам так сказать самую лучшую модель предсказания на данный момент таким образом признаки б мы оставляем и пытаемся добавить к нему новый признак кто нибудь либо а либо т ну что давай попробуем у нас возникают новые модели предсказания первая модель предсказания строится с помощью признаков а б д а вторая модель предсказания строится с помощью признаков б центр мы заверяем у них точности предсказать даня и оказывается что точность не возросла по сравнению с набором из признаков б таким образом набор признаков б нашей задачей будет считаться оптимальным для дальнейшей работы мы оставляем это признаки б а остальные признаки можно удалить это был первый жадный алгоритм оно можно построить как бы двойственны алгоритм который не добавляет признаки по одному а наоборот по одному эти признаки исключает что происходит в этом магазины мы начинаем с полного пространства признаков то есть берем все все все наши не целевые признаки замеряем точность предсказания и начинаем признаки выкидывать по одному и всякий рассмотрим а где получается наибольшая точность давайте посмотрим это все на картиночки эту картинку нужно читать сверху вниз мы начинаем набор признаков который содержит все наши местные признаки опс вы эту модель видите в самом верхнем ряду мы замеряем точно мы смотрели предсказания нас получилась ноль восемь ну что ж давайте посмотрим лучше оценили качество моделей предсказания если мы какое то из признаков удален поскольку у нас всего четыре нецелевых признака то мы можем удалить каждый из этих четырех признаков и получить четыре новых моделей предсказания мы их получаем они у нас указаны во втором ряду на нашем складе теперь у каждой такой модели предсказания мы замеряем и ее точность и мы видим что наиболее так сказать классная модель предсказания она состоит из признаков б т д что мы делаем дальше мы теперь фокусируем свое внимание только вот на этой модели но моделей которые используются признаки б и продолжаем выкидывать признаки по одному из модель и состоящий из признаков да можно выкинуть и один из трех признаков и получить три различных моделей предсказания они у нас указанные следующем ряду на нашем складе что мы видим мы видим что когда мы выкинули признак б то остались признаки т д и в этой модели предсказания точность улучшилась по сравнению с предыдущей модели что мы делаем дальше итак у нас самая лучшая модель состоит из признаков т д так вот давайте попытаемся ещё выкинуть один признак либо признак тех либо признак да и у нас получится две модели состоящие из одного нецелевого признака для таких моделей мы снова считаем точность их предсказания но оказывается она падает и таким образом самая лучшая комбинация признаков которые мы нашли вот в результате нашей работы это вот как раз вот комбинация из двух признаков т д и на них вот как вы видите наблюдалось максимальная точность предсказания таким образом мы нашей модели в нашем исследовании оставляем только признаки т д остальные не целевые признаки можно удалить",
        "rating": 1,
        "url": "https://stepik.org/lesson/83233/step/1?unit=59869"
      },
      {
        "lesson_name": "Отбор признаков в несколько итераций",
        "type": "video",
        "step_id": 631320,
        "lesson_id": 83233,
        "content": "теперь давайте поговорим о генетических алгоритмов на какой идее они основаны ну во первых во первых нужно научиться кодировать набор признаков помощью векторов или единичек итак представьте себе что у нас есть нам не целевых признаков любой результат отбора признаков можно закодировать виде вектора из нулей и единичек вот например вектор который состоит из следующих циферок один один ноль один этот вектор говорит что врезультате работы некоторые алгоритм предсказания были оставлены следующие признаки с номером один с номером две с номером пять а признаки номерами три четыре были удалены потому что она соответствуй экспозиция стоят люди теперь для каждого вектора можно посчитать точность модели предсказания мы еще будем обозначать помощью ф от вектора в эта модель предсказания заключается предсказание целевого признака по тем признакам которые просто оставленным этом векторе то есть по тем признакам которые стоят единицы теперь что делаем дальше возникает биологической интерпретации отсюда собственно и название генетический алгоритм то есть имеющие отношения биологии оказывается вот эти наборы век это можно отождествить собственными некоторые популяции а функцию фатвы можно рассматривать как функцию приспособленности этой особи условиям окружающей среды а вот эти числа ноль единички которые это наш набор это как бы сказать гены нашей особи итак как работает генетический алгоритм во первых нужно случайным образом сгенерировать к особи то есть как векторов или единичек и для каждого из них вычислить функцию приспособ власти по определению функции приспособленности вы грубо говоря должны посчитать точность модели предсказания которая строится вот по тем нецелевым признакам которые есть наборе а далее нужно следующие действия крутить цикле что нужно делать но во первых нужно выбрать некоторые вероятность распределения пару особей для размножения всоответствии с какими правилами выбирая способ для размножения а вот вероятность так сказать попасть пару для размножения больше особей которые выше функция приспособленность далее когда выбрана пара для размножения происходит собственно этот процесс продолжение рода для этого генотипы родительских особей перемешиваются и возникают две новых особей и далее на каждой операции алгоритма некоторые правда небольшой вероятностью может возникнуть мутация то есть какой то особи может случайно позиции поменяться ген то есть нуля на единичку и далее что нужно иметь ввиду что у нас популяциями размножалась бесконтрольно после каждого так сказать прохода по циклу нужно удалить две самых наименее приспособленных особей ну что ж давайте посмотрим как это все делается итак как я уже сказал выбор особенно для размножения он пропорциональным их функции приспособленности теперь как происходит собственно положение точки зрения математики с точки зрения нического алгоритма как смешиваются генотипы родителей итак представь себе что у нас есть генотип первого родителя генотип второго родителя случайным образом генерируется множество индексов может своим рецептом я случайно сгенерированных оно показано ниже с помощью красного цвета а теперь происходит собственно создание двух дочерних особей как формируется генотип первого потока генотип первого потока формируется следующим образом так сказать черные гены первого родителя их ним добавляются красный гены второго родителя а потом ока номер два формируется аналогично черный гены второго родителя с красными белыми первого родителя но ампутация как я уже сказал это означает случайно изменения каком то из генов какой то осами итак вот этот цикл нужно крутить крутить на достаточно продолжительное время и все вычисления которые будут происходить будут моделировать жизнь популяции окружающей среде после того как вам либо надоело крутить это цикл либо функции приспособленности как то стабилизировать это цикл нужно остановить и посмотреть какая осака обладает наибольшей приспособленностью ов чем смысл смысл в том что когда такую о собственной большей приспособленностью найдем то мы так сказать по определению можем восстановить найти оптимальное подмножество признаков действительно находим это осок смотрим на нее генотип генотип кодируются роликами единичными но мы то знаем что многие днище кодирует множество признаков которые мы оставляем какие мы удаляем итак находим самые лучшие особи смотрим на генотип и удаляем текст знаки у которых генотипе самый лучший способ постоянно и оставляем только те признаки которые генотипе кодируется единичными и вот таким вот образом формируется оптимальная множество признаков какие выводы следуют из вот этой вот лекции мы на этой лекции изучили пожарные генетические алгоритмы формирования оптимального множество признаков",
        "rating": 1,
        "url": "https://stepik.org/lesson/83233/step/2?unit=59869"
      }
    ],
    [
      {
        "lesson_name": "Синтез новых признаков. Метод главных компонент",
        "type": "video",
        "step_id": 631321,
        "lesson_id": 83234,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83234/step/1?unit=59870"
      }
    ],
    [
      {
        "lesson_name": "Синтез новых объектов",
        "type": "video",
        "step_id": 631322,
        "lesson_id": 83235,
        "content": "она этой лекции мы признаки оставить в покое и поговорим об объектах возникает вопрос нужно ли синтезировать новые объекты и удалять старые вот что можно сделать с объектами чтобы улучшить качество задачи предсказания на самом деле самая сложная проблема здесь эта проблема не сбалансированности выборке итак представьте себе что тренировочная выбор конец балансировано то есть доля объектов одного класса она существенно больше чем доля эффектов другого класса и когда вы будете обрабатывать такую выборку решать для нее задачи предсказания например то могут возникнуть проблемы например такая алгоритм предсказания может просто забыть про меньшей класс это действительно может произойти тогда когда у вас объектов одного класса но во много раз больше чем объектов другого класса и действительно алгоритм предсказания он же основана таких статистических идеях и он может посчитать что множество объектов меньшего класса но статистически не значимы это фактически какие то выбросы и может просто игнорировать и соответственно никакого хорошего предсказания тут не получится что нужно с этим делать как бороться с несбалансированными выборками во первых можно удалять выбросы они действительно во первых улучшают качество работы только предсказания вторых могут помочь как то сбалансировать ваши класс во вторых есть такой метод андерсом пинг это удаление объектов большего класса это можно сделать разными способами например можно просто взять случайном порядке удалить некоторую часть те кто больше го класса до тех пор пока выборка объектов одного класса не сбалансируется выборка объектов другого класса можно применить кластеризацию например осуществить кластеризацию объектов большего класса и каждом классе оставить только эталонные объект третий подход это образ санкт вы можете взять и начать клонировать объекты меньшего класса до тех пор пока множество объектов одного класса не станет таким же по своей мощности как и объекты второго класса но и последний способ вы может если как бы создавать синтетические объекты то есть те объекты которые в реальности не существует но они как бы синтезированы из реально существующих как это делается как построить синтетические объект по пары имеющихся то есть как то нужно синтезировать искусственное объект которого не было природе итак у вас есть два объекта одного класса так вот как сделать из них некий такой синтез провести а нужно взять их линейную комбинацию давайте сгенерировать случайное число а отрезка ноль один и вычислим такой вот линейную комбинацию а умноженное на признаки объекта а плюс число единиц минусом можно признаки объекта это что означает это означает что синтетическом объекте каждый из его признаков будет вычисляться вот по такой формуле нужно взять признак объекта а умножить его на вот это число а маленькая и прибавить к нему выражение единица минус а умноженное на значение признака б а пример построения синтетического объектов приведено на вашем складе у вас есть две объекта аябэ вы генерируете случайное число в данном случае оно равно ноль целых одна десятая получаем вот такой вот синтетические объект действительно вы можете проверить что признаки этого объекта объект они вычислялись по формуле которая указана на нашем сайте но правда то указан пример где метки классов объектов альбиони различные поэтому возникла проблема когда метка класса синтетического объекта становится не целым числом и что с этим делать эта проблема характерна не только для меток класса вообще для категориальных или номинальных признаков синтетического объекте категориальные или номинальные признаки могут стать не совсем корректные значения могут принять например они могут стать не цель числами итак какие возможные пути решения вот претензии объекта целых вот как вы видели возникла проблема его пол это не целое число у нас же пол кодируются либо меткой ноль либо метка единица тут возникает какое то число ноль один что с этим делать но во первых можно его округлить до ближайшего целого значения во вторых можно провести случайно испытания с указанной вероятности то есть вероятностью ноль один мы его отправляем наоборот до единицы освещается семь ноль девять округляем до нуля при этих признаках можно сделать числовым то есть мы как бы забываем про то что это действительно пол что это имеет какую то аналогию живой природе мы просто берем и говорим пол это вещественное число со всеми вытекающими отсюда последствиями то есть мы можем как бы своей задачей разрешить что субъекты могут иметь например пол виде какого числа для решения какой либо задачи это впринципе может и сильно нам и не навредить что нужно иметь ввиду если вы использовали различные методы балансировки банки они оказались неэффективными либо вам этого делать не хочется и выборка тем не менее осталось не сбалансирована и вы начинаете решать задачу предсказания то что нужно иметь ввиду если вы только не сбалансирована тем не менее то что нужно сделать следующее нужно чтобы объекты одного и второго класса делились в одинаковой пропорции между тренировочной и тестовой выборкой частности прекрас валидации пропорция должна тоже сохранятся для каждого из солдат то есть для каждого из этих отпусков данных то есть если например у вас объекты класса ноль и объекты класса один в исходное во всей выборки делились между собой как тридцать процентов класса ноль семьдесят процентов класс один то вот каждом вот это вот кирпичики каждом столбе пропорция должна быть точно такой же что ещё можно порекомендовать очень часто во многих задачах возникает следующая проблема различные объекты имеют различную историю например вы занимаетесь слезам поведение пользователей некоторые онлайн игре и там возникает такая вот градация пользователя часть пользователей играют очень мало у них очень маленькая история их поведение в игре есть некоторая часть пользователей у которых вот небольшая история про них много можете сказать но философская проблема заключается в том что с точки зрения анализа данных и такой игрок с маленькой истории из большой истории они равноправны потому что и тот и тот объект должны кодироваться одной строкой таблице есть объект маленькая история дает одну строку таблицы объект богатой истории дает тоже одну строку таблицы вне которых задачах это несправедливо хочется придать больший вес объектом которых история очень длинная как это можно сделать но одним из способов является расщепление объекта на несколько новых синтетических объектов представьте себе что у вас есть ну то же самое игрок из онлайн игры которые играл очень очень долго у него есть очень длинная история здесь на складе это схематично показано что он как бы играл начиная от первого января и кончая девятое января здесь указаны просто значения числовых признаков связаны с длительностью игры что вы можете сделать этого игрока нужно купить на несколько новых объектов давайте сделаем так всю его так сказать жизни в игре мы разобьём на несколько меньших отрезков например вкачестве первого отрезка возьмём интервал с первого января по пятое января и это будет объект а то есть это физический объект который грубо говоря вырезаны из нашего игрока и содержит информацию об истории что происходило с этим игроком начиная от первого января до пятого теперь мы как бы сдвигает наш отрезок наша кошечка и формируем новости физический объект который состоит так сказать из нашего игрока но туда идет история от второго января до шестого января и так далее следующий объект формируется из истории от третьего до седьмого января и так далее таким образом один игрок большой истории дает целых аж пять новых объектов такая операция дает больший вес объектом у которых очень богатая история ну чтож какие выводы следуют из нашей лекции мы познакомились со способами ты вот собственность не сбалансированной выборкой",
        "rating": 1,
        "url": "https://stepik.org/lesson/83235/step/1?unit=59871"
      }
    ],
    [
      {
        "lesson_name": "Проклятие размерности",
        "type": "video",
        "step_id": 631323,
        "lesson_id": 83236,
        "content": "и в заключение поговорим о таком страшном понятие как проклятие размерности кого это касается и в чем заключается так сказать суть этого эффекта оказывается было замечено следующее что если признаков очень много то возник считает падение качества предсказания с увеличением числа вот этих самых признаков то есть мы видим что когда признаком очень мало точность очень маленькая но потом с добавлением каких то новых признаков точность начинает расти но потом она почему это неуклонно снижается когда признаков чересчур много то вот точно почему то начинает уменьшаться причем этот эффект возникает даже и для независимых признаков есть когда вы будете добавлять новые признаки которые не зависят от старых тем не менее такое секс может возникнуть чем же дело во первых когда это самое проклятие размерности проявляется этому проклятье размерности восновном подвержены метрические алгоритма возникает эта проблема смерти когда у вас признаков слишком очень очень много то есть возникает эта проблема вот именно с распределением расстояние между объектами в многомерном пространстве вот давайте разбираться чем тут проблема и как её можно объяснить на самом деле эта проблема очень многогранно и очень сложно и на самом деле существует несколько объяснений которые объясняют проклятие размерности с помощью различных вот аргументов давайте рассмотрим первое хотя существует очень много я рассмотрю только лишь два объяснения первое объяснение здесь на складе многомерном пространстве очень сложно сформировать репрезентативную тренировочную выборку что такое репрезентативность почему тренировочная выборка должна быть репрезентативной итак представьте себе что допустим вы хотите а заняться предсказанием допустим коэффициента интеллекта по допустим ну не знаю там длиннее носа у человека будут переходить решить такую абсурдную задачу естественно чтобы выявить зависимость интеллекта от длины носа мы должны сформировать тренировочного баку причем этот тренировочный выборку вы должны добавить как можно больше различных людей то есть как можно больше людей там зависимости от формы носа их там возраста социального происхождения там и так далее профессии то есть вы должны добавить как можно больше различных объектов туда это означает что выборка называется репрезентативной то есть она вот адекватно так сказать представляет всех объектов из генеральной совокупности ну что ж давайте представим что если тренировочная выборка репрезентативна то вы зависимость которую вы ищете вы находите причем давайте допустим что зависимость обнаруживается если тренировочная выборка состоит из не более двадцати процентов от множество всех возможных объектов то есть тренировочно выборка это не менее чем двадцать процентов от всей вот генеральной совокупности то есть иными словами если вы занимаетесь людьми то вы должны брать не менее чем двадцать процентов от всего населения земли теперь что возникает когда у вас начинает расти пространства признаков когда вас увеличивается количество нецелевых признаков отдела общем когда у вас увеличивается количество нецелевых признаков вам труднее сформировать репрезентативную выборку потому что и е объем начинай расти экспоненциально то есть добавление каждого нового признака вынуждает вас очень сильно увеличивать объем тренировочной выборки и когда у вас признаком очень очень много а выборка наоборот очень маленькая то она получается уже не буду репрезентативные поэтому зависимость целевого признака отметили у вас уже обнаружить не удастся и таким образом любая классификация в этом случае она теряет точность она становится уже не точно и неадекватный ну что ж какой второй объяснение проклятье размерности оказывается многомерном пространстве объекты в основном располагаются по периферии как это можно так сказать доказать а вот давайте будем считать следующие женщину мы будем считать долю пространство находящ спасибо максимальной сферы центром в начале координат это область для размерности две три указано на наших рисунках посмотреть на левом рисунке вслучае размерности две указана максимальная сфера которая находится в центре да вашего поста паства а то что фирма не попало оно нарисовано зеленым цветом это как бы по периферии когда вы приходите пространстве размерности три из новостройке сферу центре вашего пространства то у вас вот объем периферии стал больше а соответственно объем вот красный сферы наоборот становится меньше то есть это означает что с ростом размерности увеличивается доля пространство которое располагается вот как бы по краям по периферии от максимальной сферы что из этого следует из этого следует что многомерном пространстве почти все объекты располагаются на периферии но действительно если мы центр нашего пространства помещаем какой то новый объект то оказывается что при достаточно большой размерности остальные все тексты будут расположены за пределами этой сферы то есть возникают ситуации когда один новый объект находится в центре процента а все остальные расположены равномерно вот так вот на периферии и считать расстояние от нового объекта доставить бессмысленно потому что оно это расстояние будет примерно одинакова для всех ввод объектов такие выводы следуют из нашей лекции оказывается при достаточно большом числе признаков наблюдается такой эффект как проклятие размерности с этим проклятием но бороться естественно уменьшая число признаков производя операцию отбора признаков",
        "rating": 1,
        "url": "https://stepik.org/lesson/83236/step/1?unit=59872"
      }
    ],
    [
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 537049,
        "lesson_id": 83975,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83975/step/1?unit=60547"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536823,
        "lesson_id": 83975,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83975/step/2?unit=60547"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 536827,
        "lesson_id": 83975,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83975/step/3?unit=60547"
      },
      {
        "lesson_name": "Тест",
        "type": "choice",
        "step_id": 536832,
        "lesson_id": 83975,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83975/step/4?unit=60547"
      },
      {
        "lesson_name": "Тест",
        "type": "fill-blanks",
        "step_id": 537012,
        "lesson_id": 83975,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/83975/step/5?unit=60547"
      }
    ],
    [
      {
        "lesson_name": "Практическое задание",
        "type": "choice",
        "step_id": 537047,
        "lesson_id": 190159,
        "content": null,
        "rating": 1,
        "url": "https://stepik.org/lesson/190159/step/1?unit=164664"
      }
    ]
  ],
  [
    [
      {
        "lesson_name": "Конспекты лекций",
        "type": "text",
        "step_id": 1300297,
        "lesson_id": 353555,
        "content": "https://stepik.org/media/attachments/lesson/353555/1_%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2_%D0%9C%D0%9E_%D0%B8_%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D1%8B_%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.pdf \n\n   \n\n https://stepik.org/media/attachments/lesson/353555/2_%D0%92%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf \n\n https://stepik.org/media/attachments/lesson/353555/3_%D0%92%D1%8B%D0%B1%D1%80%D0%BE%D1%81%D1%8B.pdf \n\n https://stepik.org/media/attachments/lesson/353555/4_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.pdf \n\n https://stepik.org/media/attachments/lesson/353555/5_%D0%9B%D0%B8%D0%BD_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F.pdf \n\n https://stepik.org/media/attachments/lesson/353555/6_%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F__kNN.pdf \n\n https://stepik.org/media/attachments/lesson/353555/7_%D0%94%D0%B5%D0%B2%D0%B5%D0%B2%D1%8C%D1%8F_%D0%B2_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8.pdf \n\n https://stepik.org/media/attachments/lesson/353555/8_%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80%D1%8B.pdf \n\n https://stepik.org/media/attachments/lesson/353555/9_%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D1%8B%D0%B5_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B.pdf \n\n https://stepik.org/media/attachments/lesson/353555/10_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%BE%D0%B2.pdf \n\n https://stepik.org/media/attachments/lesson/353555/11_%D0%9E%D1%82%D0%B1%D0%BE%D1%80_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2.pdf",
        "rating": 1,
        "url": "https://stepik.org/lesson/353555/step/1?unit=337532"
      }
    ]
  ]
]